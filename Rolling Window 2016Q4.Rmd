---
title: "RW 2016 Q4"
output: html_document
date: "2025-04-22"
---
# Package Installation and Library Loading
```{r}
packages <- c("readxl", "janitor", "dplyr", "bsts", "forecast", "writexl", "broom", "openxlsx", "coda")
install.packages(setdiff(packages, rownames(installed.packages())))
```

```{r}
library(readxl)
library(janitor)
library(dplyr)
library(bsts)
library(forecast)
library(writexl)
library(broom)
library(openxlsx)
library(coda)
library(ggplot2)
```


# Data Import and Cleaning
```{r}
filepath <- file.choose()
mydata <- read_excel(
  path = filepath,
  sheet = "Consolidation Sheet",
  .name_repair = "unique"
) %>%
  clean_names() # for getting the whole dataset 

```

# Variable Definition
```{r}
dep_var_sets <- list(
  net_interest_income = list(
    dep_var = "net_interest_income",
    ivs = c("efficiency_ratio", "liquid_assets_deposits_short_term_borrowing",
            "net_interest_income_after_llp_operating_revenue", "npl_total_equity",
            "pretax_operating_roa", "total_loans_logged",
            "aggregate_corporate_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_percentage",
            "aggregate_financial_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_in_billions_logged",
            "allowance_for_loan_reserves_loans_and_leases_large_domestically_chartered_commercial_banks_percentage",
            "borrowings_large_domestically_chartered_commercial_banks_in_billions_logged",
            "consumer_credit_quarterly_cumulative_change_non_annualized_seasonally_adjusted_in_billions",
            "employment_population_ratio_quarterly",
            "federal_funds_effective_rate_quarterly_average_percentage",
            "federal_receipts_in_billions_logged",
            "gz_spread", 
            "ice_bof_a_us_high_yield_index_total_return",
            "labor_force_total_15_64_years_old_logged",
            "m2_money_supply_to_real_gdp_2017_ratio_percentage",
            "net_interest_income_all_us_banks_in_billions_logged",
            "net_operating_income_margin_all_us_banks_percentage",
            "percentage_of_profitable_financial_institutions_with_yo_y_earnings_growth",
            "real_disposable_income_per_capita_2017_logged",
            "real_government_consumption_expenditures_and_gross_investment_2017_in_billions_logged",
            "real_gross_private_investment_2017_to_real_gdp_2007_percentage",
            "real_personal_consumption_expenditures_on_services_2017_to_real_personal_consumption_expenditures_2017_percentage",
            "s_p_core_logic_case_shiller_u_s_national_home_price_index",
            "sahm_rule_recession_indicator",
            "securities_to_loans_and_leases_after_allowance_for_loan_losses_large_domestically_chartered_commercial_banks_percentage",
            "smoothed_u_s_recession_probabilities_percent",
            "total_deposits_quarterly_percentage_change_seasonally_adjusted_percentage",
            "treasury_constant_maturity_tcm_10yr_2yr",
            "velocity_of_m2_money_stock_ratio")
  ),
  non_interest_income = list(
    dep_var = "non_interest_income",
    ivs = c("efficiency_ratio", "market_cap_ltm_ebt_excl_unusual_items",
            "liquid_assets_deposits_short_term_borrowing",
            "non_interest_operating_income_operating_revenue", 
            "npl_total_equity",
            "pretax_operating_roa", 
            "securities_total_assets",
            "aggregate_corporate_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_percentage",
            "aggregate_financial_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_in_billions_logged",
            "coincident_economic_activity_quarterly_index",
            "consumer_price_quarterly_index_for_all_urban_consumers",
            "employment_population_ratio_quarterly", 
            "equity_volatility_overall_quarterly",
            "federal_receipts_in_billions_logged", 
            "gz_spread",
            "ice_bof_a_us_high_yield_index_total_return",
            "labor_force_total_15_64_years_old_logged",
            "m2_money_supply_to_real_gdp_2017_ratio_percentage",
            "net_operating_income_margin_all_us_banks_percentage",
            "percentage_of_profitable_financial_institutions_with_yo_y_earnings_growth",
            "personal_savings_rate_percentage",
            "real_disposable_income_per_capita_2017_logged",
            "real_government_consumption_expenditures_and_gross_investment_2017_in_billions_logged",
            "real_gross_private_investment_2017_to_real_gdp_2007_percentage",
            "real_personal_consumption_expenditures_on_services_2017_to_real_personal_consumption_expenditures_2017_percentage",
            "reserve_balances_with_federal_reserve_banks_in_billions_logged",
            "s_p_core_logic_case_shiller_u_s_national_home_price_index",
            "sahm_rule_recession_indicator",
            "securities_to_loans_and_leases_after_allowance_for_loan_losses_large_domestically_chartered_commercial_banks_percentage",
            "total_deposits_quarterly_percentage_change_seasonally_adjusted_percentage",
            "total_non_interest_income_all_us_banks_in_billions_logged",
            "velocity_of_m2_money_stock_ratio")
  ),
  provision_credit_loss = list(
    dep_var = "provision_for_credit_losses",
    ivs = c("net_interest_income_after_llp_operating_revenue", 
            "npl_total_equity",
            "securities_total_equity", 
            "total_loans_logged",
            "aggregate_corporate_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_percentage",
            "allowance_for_loan_reserves_loans_and_leases_large_domestically_chartered_commercial_banks_percentage",
            "coincident_economic_activity_quarterly_index",
            "consumer_credit_quarterly_cumulative_change_non_annualized_seasonally_adjusted_in_billions",
            "consumer_price_quarterly_index_for_all_urban_consumers",
            "delinquency_rates_on_loans_for_100_largest_us_banks_by_assets_percentage",
            "employment_population_ratio_quarterly",
            "federal_funds_effective_rate_quarterly_average_percentage",
            "federal_receipts_in_billions_logged", 
            "gz_spread",
            "ice_bof_a_us_high_yield_index_total_return",
            "net_charge_off_rate_of_total_loans_and_leases_percentage",
            "net_operating_income_margin_all_us_banks_percentage",
            "real_disposable_income_per_capita_2017_logged",
            "reserve_balances_with_federal_reserve_banks_in_billions_logged",
            "sahm_rule_recession_indicator",
            "treasury_constant_maturity_tcm_10yr_2yr",
            "treasury_constant_maturity_tcm_10yr_3mo",
            "unemployment_rate_percentage",
            "velocity_of_m2_money_stock_ratio")
  ),
  non_interest_expense = list(
    dep_var = "non_interest_expense",
    ivs = c("efficiency_ratio", 
            "gross_property_plant_equipment",
            "net_interest_income_after_llp_operating_revenue",
            "non_interest_operating_income_operating_revenue",
            "npl_total_equity", 
            "securities_total_equity",
            "total_loans_logged", 
            "total_deposits_logged",
            "consumer_price_quarterly_index_for_all_urban_consumers",
            "employment_population_ratio_quarterly",
            "labor_force_total_15_64_years_old_logged",
            "loans_and_leases_to_deposit_ratio_large_domestically_chartered_commercial_banks_percentage",
            "m2_money_supply_to_real_gdp_2017_ratio_percentage",
            "net_operating_income_margin_all_us_banks_percentage",
            "percentage_of_profitable_financial_institutions_with_yo_y_earnings_growth",
            "real_gross_private_investment_2017_to_real_gdp_2007_percentage",
            "real_personal_consumption_expenditures_on_services_2017_to_real_personal_consumption_expenditures_2017_percentage",
            "sahm_rule_recession_indicator",
            "total_noninterest_expense_all_us_banks_in_billions_logged",
            "treasury_constant_maturity_tcm_10yr_2yr",
            "treasury_constant_maturity_tcm_10yr_3mo")
  )
)
```


```{r}
# Create datasets for each dependent variable
dep_var_datasets <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Store in list
  dep_var_datasets[[dep_var_name]] <- dataset
}


# Take lag of the missing value of 2024 Q4
dep_var_datasets_modified <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Repeat the first column (dep_var) and add it before the first column
  dataset <- cbind(dataset[, 1, drop = FALSE], dataset)  # Add first column as the first column again
  
  # Loop over columns starting from the second column
  for(col in 2:ncol(dataset)) {
    # Check if the last row value is missing
    if(is.na(dataset[nrow(dataset), col])) {
      # Apply lag: take the value from the previous row for all rows of the column
      dataset[, col] <- lag(dataset[, col], 1, default = NA)
      
      # Modify column name to indicate lag if it was modified
      new_col_name <- paste0(colnames(dataset)[col], "_lag1")
      colnames(dataset)[col] <- new_col_name
    }
  }
  
  # Store in list
  dep_var_datasets_modified[[dep_var_name]] <- dataset
}

# Delete 1996Q4
dep_var_datasets_modified <- lapply(dep_var_datasets_modified, function(x) {
  x[-(1:8), ]
})

# Extract last row 2025Q4
last_row <- lapply(dep_var_datasets_modified, function(x) {
  x[nrow(x), ]
})

# Remove last row from each dataset
dep_var_datasets <- lapply(dep_var_datasets_modified, function(x) {
  x[-nrow(x), ]
})

# Verify dimensions
cat("Number of rows in main datasets:\n")
sapply(dep_var_datasets, nrow)
cat("\nNumber of columns in main datasets:\n")
sapply(dep_var_datasets, ncol)
print(dep_var_datasets[[1]][nrow(dep_var_datasets[[1]]), ])  # For the first dataset
```


```{r}
create_quarter_four_from_last_row <- function(dep_var_datasets, dep_var_sets) {
  quarter_four <- list()
  
  for (dep_var_name in names(dep_var_sets)) {
    # Get the original dep_var column name
    original_dep_var_name <- dep_var_sets[[dep_var_name]]$dep_var
    
    # Extract last row from dataset
    last_row_data <- dep_var_datasets[[dep_var_name]][nrow(dep_var_datasets[[dep_var_name]]), , drop = FALSE]
    
    # Separate and rename dep_var column
    dep_var_column <- last_row_data[, original_dep_var_name, drop = FALSE]
    colnames(dep_var_column) <- "dep_var"
    
    # Get the independent variables
    indep_vars_data <- last_row_data[, setdiff(colnames(last_row_data), original_dep_var_name), drop = FALSE]
    
    # Combine into final row format
    quarter_four[[dep_var_name]] <- cbind(dep_var_column, indep_vars_data)
    
    # Remove the last row from the dataset
    dep_var_datasets[[dep_var_name]] <- dep_var_datasets[[dep_var_name]][-nrow(dep_var_datasets[[dep_var_name]]), ]
  }
  
  return(list(quarter_four = quarter_four, dep_var_datasets = dep_var_datasets))
}

# Usage:
result <- create_quarter_four_from_last_row(dep_var_datasets, dep_var_sets)
quarter_four <- result$quarter_four
dep_var_datasets <- result$dep_var_datasets
```

# Extract the hold-out period 
```{r}
# Extracting rows 80-86 from each dataset in dep_var_datasets
hold_out_dataset <- lapply(dep_var_datasets, function(df) {
  df[80:86, ]#period
})

# Create a list to store the updated datasets with quarter_four at the end
holdout_with_last_row <- list()

# Iterate through each dep_var_name and append quarter_four at the end
for(dep_var_name in names(hold_out_dataset)) {
  
  # Ensure column names match between quarter_four and the data frame in hold_out_dataset
  if (!all(names(quarter_four[[dep_var_name]]) == names(hold_out_dataset[[dep_var_name]]))) {
    # Manually adjust the column names of quarter_four to match hold_out_dataset
    names(quarter_four[[dep_var_name]]) <- names(hold_out_dataset[[dep_var_name]])
  }
  
  # Combine the selected holdout (80-87 rows) and the corresponding last row from quarter_four
  holdout_with_last_row[[dep_var_name]] <- rbind(
    hold_out_dataset[[dep_var_name]],
    quarter_four[[dep_var_name]]
  )
}

# Update hold_out_dataset with the modified datasets
hold_out_dataset <- holdout_with_last_row
```

```{r}
# Define the hold-out period (rows 80-86)
hold_out_period <- 80:86#period

# Function to exclude the hold-out period from the dataset
exclude_hold_out_1 <- function(df) {
  # Exclude the rows that are in the hold-out period (80-86)
  df_no_hold <- df[!rownames(df) %in% hold_out_period, ]
  return(df_no_hold)
}

#--------------------------------------------------------------
# This is the dataset without the hold out period 
Indice_testing_dataset <- lapply(dep_var_datasets, function(df) {
  # Exclude rows 80-87 from the dataset and return the modified dataset
  exclude_hold_out_1(df)
})

# Verify the result
Indice_testing_dataset
```

# Rolling window
```{r}
# Create rolling windows with consistent sizes (4-5 windows)
rolling_splits <- list()
for(dep_var_name in names(dep_var_datasets)) {
  current_data <- dep_var_datasets[[dep_var_name]]
  dataset_windows <- list()
  
  # Get total number of rows in the dataset
  total_rows <- nrow(current_data)
  
  # Fixed parameters
  hold_out_period <- 80:86#period
  train_size <- 90  # Fixed training window size
  window_step <- ceiling((total_rows - length(hold_out_period) - train_size) / 4)  # Step size to create 4-5 windows
  
  # Calculate number of windows possible
  max_windows <- 1 + floor((total_rows - length(hold_out_period) - train_size) / window_step)
  max_windows <- min(max_windows, 5)  # Cap at 5 windows
  
  # Available indices (excluding hold-out period)
  available_indices <- setdiff(1:total_rows, hold_out_period)
  
  # Create windows
  for(window_index in 1:max_windows) {
    # Calculate start position for this window
    start_pos <- 1 + (window_index - 1) * window_step
    
    # Get train indices (first train_size available indices from start_pos)
    # Make sure we don't go beyond available indices
    potential_train_indices <- available_indices[available_indices >= start_pos]
    if(length(potential_train_indices) < train_size) {
      cat("Not enough data for window", window_index, "in", dep_var_name, "\n")
      break
    }
    
    train_indices <- head(potential_train_indices, train_size)
    
    # All remaining indices are test indices
    test_indices <- setdiff(available_indices, train_indices)
    
    # Extract the data
    train_data <- current_data[train_indices, ]
    test_data <- current_data[test_indices, ]
    
    # Check for NA values in training data
    train_na_rows <- which(apply(train_data, 1, function(x) any(is.na(x))))
    if(length(train_na_rows) > 0) {
      cat("Removing", length(train_na_rows), "rows with NA values from", 
          dep_var_name, "window", window_index, "training data\n")
      
      # Remove rows with NA
      train_data <- train_data[-train_na_rows, ]
      train_indices <- train_indices[-train_na_rows]
    }
    
    # Check for NA values in test data
    test_na_rows <- which(apply(test_data, 1, function(x) any(is.na(x))))
    if(length(test_na_rows) > 0) {
      cat("Removing", length(test_na_rows), "rows with NA values from", 
          dep_var_name, "window", window_index, "test data\n")
      
      # Remove rows with NA
      test_data <- test_data[-test_na_rows, ]
      test_indices <- test_indices[-test_na_rows]
    }
    
    # Store the window
    dataset_windows[[window_index]] <- list(
      train = train_data, 
      test = test_data,
      train_indices = train_indices,
      test_indices = test_indices
    )
  }
  
  # Now ensure all windows have the same train and test size
  # Find the minimum sizes across all windows
  train_sizes <- sapply(dataset_windows, function(w) nrow(w$train))
  test_sizes <- sapply(dataset_windows, function(w) nrow(w$test))
  min_train_size <- min(train_sizes)
  min_test_size <- min(test_sizes)
  
  # Trim data in each window to match the minimum sizes
  for(i in 1:length(dataset_windows)) {
    # Trim training data if needed
    if(nrow(dataset_windows[[i]]$train) > min_train_size) {
      cat("Trimming window", i, "training data from", nrow(dataset_windows[[i]]$train), 
          "to", min_train_size, "rows\n")
      
      # Take the first min_train_size rows for consistency
      keep_indices <- head(1:nrow(dataset_windows[[i]]$train), min_train_size)
      dataset_windows[[i]]$train <- dataset_windows[[i]]$train[keep_indices, ]
      dataset_windows[[i]]$train_indices <- dataset_windows[[i]]$train_indices[keep_indices]
    }
    
    # Trim test data if needed
    if(nrow(dataset_windows[[i]]$test) > min_test_size) {
      cat("Trimming window", i, "test data from", nrow(dataset_windows[[i]]$test), 
          "to", min_test_size, "rows\n")
      
      # Take the first min_test_size rows for consistency
      keep_indices <- head(1:nrow(dataset_windows[[i]]$test), min_test_size)
      dataset_windows[[i]]$test <- dataset_windows[[i]]$test[keep_indices, ]
      dataset_windows[[i]]$test_indices <- dataset_windows[[i]]$test_indices[keep_indices]
    }
  }
  
  rolling_splits[[dep_var_name]] <- dataset_windows
}

# Function to summarize window information
summarize_windows <- function(windows_list) {
  cat("\n======= Window Summary =======\n")
  
  for (dep_var_name in names(windows_list)) {
    cat("\nSummary for", dep_var_name, ":\n")
    windows <- windows_list[[dep_var_name]]
    
    # Create a table for sizes
    test_sizes <- sapply(windows, function(w) nrow(w$test))
    train_sizes <- sapply(windows, function(w) nrow(w$train))
    
    size_table <- data.frame(
      Window = 1:length(windows),
      Train_Size = train_sizes,
      Test_Size = test_sizes
    )
    
    print(size_table)
    
    # Check if all sizes are the same
    if(length(unique(train_sizes)) == 1) {
      cat("\nAll windows have uniform train size of", train_sizes[1], "rows\n")
    } else {
      cat("\nWarning: Train sizes vary across windows:", paste(train_sizes, collapse=", "), "\n")
    }
    
    if(length(unique(test_sizes)) == 1) {
      cat("All windows have uniform test size of", test_sizes[1], "rows\n")
    } else {
      cat("Warning: Test sizes vary across windows:", paste(test_sizes, collapse=", "), "\n")
    }
    
    # Function to find consecutive sequences for readability
    find_sequences <- function(indices) {
      if(length(indices) == 0) return(character(0))
      
      indices <- sort(indices)
      gaps <- diff(indices) > 1
      group_starts <- c(TRUE, gaps)
      group_ends <- c(gaps, TRUE)
      
      sequences <- character(0)
      start_idx <- 1
      
      for(i in 1:length(indices)) {
        if(group_starts[i]) start_idx <- i
        if(group_ends[i]) {
          end_idx <- i
          if(start_idx == end_idx) {
            sequences <- c(sequences, as.character(indices[start_idx]))
          } else {
            sequences <- c(sequences, paste(indices[start_idx], "-", indices[end_idx]))
          }
        }
      }
      
      return(sequences)
    }
    
    # Show detailed information for each window
    for (i in 1:length(windows)) {
      window <- windows[[i]]
      
      cat("\nWindow", i, ":\n")
      
      # Print train ranges in desired format
      train_sequences <- find_sequences(window$train_indices)
      train_display <- paste(train_sequences, collapse=", ")
      cat("  Training indices:", train_display, "\n")
      cat("  Training size:", nrow(window$train), "rows\n")
      
      # Print test ranges in desired format
      test_sequences <- find_sequences(window$test_indices)
      if(length(test_sequences) > 1) {
        test_display <- paste(test_sequences, collapse=" & ")
      } else {
        test_display <- test_sequences
      }
      cat("  Testing indices:", test_display, "\n")
      cat("  Testing size:", nrow(window$test), "rows\n")
      
      # Check hold-out period exclusion
      if(any(window$train_indices %in% hold_out_period)) {
        cat("  WARNING: Hold-out period found in training indices\n")
      }
      if(any(window$test_indices %in% hold_out_period)) {
        cat("  WARNING: Hold-out period found in test indices\n")
      }
    }
  }
}

# Run the summary function
summarize_windows(rolling_splits)

# Check for any remaining NA values in the final dataset
check_for_remaining_nas <- function(windows_list) {
  cat("\n======= Checking for remaining NA values =======\n")
  
  for(dep_var_name in names(windows_list)) {
    has_na <- FALSE
    
    for(i in 1:length(windows_list[[dep_var_name]])) {
      window <- windows_list[[dep_var_name]][[i]]
      
      # Check training data
      if(any(is.na(window$train))) {
        has_na <- TRUE
        cat("\nFound NA values in", dep_var_name, "window", i, "training data\n")
      }
      
      # Check test data
      if(any(is.na(window$test))) {
        has_na <- TRUE
        cat("\nFound NA values in", dep_var_name, "window", i, "test data\n")
      }
    }
    
    if(!has_na) {
      cat("\n", dep_var_name, ": No remaining NA values in any window\n")
    }
  }
}

# Check for NA values
check_for_remaining_nas(rolling_splits)
```
# BSTS model 
Apply the simliar BSTS model (base model structure code from rolling)
```{r}
set.seed(1234)
fit_bsts_model <- function(train_data) {
  set.seed(1234)
  # (first column): dep
  y <- train_data[, 1]
  # (all columns except first): IVs
  X <- train_data[, -1]
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y)
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Model
  model <- bsts(formula,
                state.specification = ss,
                niter = 10000,
                data = as.data.frame(cbind(y = y, X)))
  
  return(model)
}
```

```{r}
fit_all_windows <- function(dep_var_splits) {
  models <- list()
  for(i in 1:4) {
    models[[i]] <- fit_bsts_model(dep_var_splits[[i]]$train)
  }
  return(models)
}
```

```{r}
all_models_rwall <- list()
########### 4 dep cars 
# Net Interest Income
all_models_rwall$net_interest_income <- fit_all_windows(rolling_splits$net_interest_income)

# Non-Interest Income
all_models_rwall$non_interest_income <- fit_all_windows(rolling_splits$non_interest_income)

# Provision Credit Loss
all_models_rwall$provision_credit_loss <- fit_all_windows(rolling_splits$provision_credit_loss)

# Non-Interest Expense
all_models_rwall$non_interest_expense <- fit_all_windows(rolling_splits$non_interest_expense)
```




# Function to calculate error metrics 
```{r}
# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Define the hold out period
hold_out_period <- 80:86#period

# Function to get test indices based on the window number
get_test_indices <- function(window_num) {
  total_rows <- 111  # Total number of rows in the dataset
  train_size <- 90   # Fixed training window size
  fixed_test_start <- 91  # Fixed test portion starts at row 71
  
  # Calculate training start and end based on window number
  train_start <- 1 + (window_num - 1) * 10
  train_end <- train_start + train_size - 1
  
  # Test indices consist of:
  # 1. Any rows before the training window (if training doesn't start at row 1)
  # 2. The fixed test portion (rows 71-111)
  
  # Part 1: Rows before training window (if any)
  before_train_indices <- if(train_start > 1) 1:(train_start-1) else integer(0)
  
  # Part 2: Fixed test portion (rows 71-111)
  fixed_test_indices <- fixed_test_start:total_rows
  
  # Combine both parts for complete test indices
  test_indices <- c(before_train_indices, fixed_test_indices)
  
  # Remove any indices that would overlap with training window
  test_indices <- test_indices[!test_indices %in% train_start:train_end]
  
  return(sort(test_indices))
}

# Improved evaluate_predictions function with NA handling
evaluate_predictions <- function(model, full_data, window_num) {
  # Get the test indices based on the window number
  test_indices <- get_test_indices(window_num)
  
  # Check if test indices are within bounds
  valid_indices <- test_indices[test_indices <= nrow(full_data)]
  if(length(valid_indices) < length(test_indices)) {
    cat("Warning: Some test indices for window", window_num, "exceed data dimensions.\n")
    cat("Removing", length(test_indices) - length(valid_indices), "out-of-bounds indices.\n")
    test_indices <- valid_indices
  }
  
  # Check if there are any indices left
  if(length(test_indices) == 0) {
    cat("Error: No valid test indices for window", window_num, "\n")
    return(list(
      actual = numeric(0),
      predicted_mean = numeric(0),
      predicted_median = numeric(0),
      errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
      errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
      test_indices = integer(0)
    ))
  }
  
  # Extract actual values for testing
  actual_combined <- full_data[test_indices, 1]
  
  # Prepare test data for prediction
  pred_data <- as.data.frame(full_data[test_indices, -1, drop = FALSE])
  
  # Check for and handle NA values in test data
  rows_with_na <- which(apply(pred_data, 1, function(x) any(is.na(x))))
  if(length(rows_with_na) > 0) {
    cat("Warning:", length(rows_with_na), "rows with NA values found in test data for window", window_num, "\n")
    cat("These rows will be removed from evaluation\n")
    
    # Remove rows with NA
    pred_data <- pred_data[-rows_with_na, , drop = FALSE]
    actual_combined <- actual_combined[-rows_with_na]
    test_indices <- test_indices[-rows_with_na]
    
    # Check if we have any data left
    if(nrow(pred_data) == 0) {
      cat("Error: No data left after removing NA values for window", window_num, "\n")
      return(list(
        actual = numeric(0),
        predicted_mean = numeric(0),
        predicted_median = numeric(0),
        errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
        errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
        test_indices = integer(0)
      ))
    }
  }
  
  # Make predictions - using tryCatch to handle errors
  tryCatch({
    pred <- predict(model, newdata = pred_data, burn = 100)
    mean_predictions <- colMeans(pred$distribution)
    median_predictions <- apply(pred$distribution, 2, median)
    
    # Check if predictions length matches actual values length
    if(length(mean_predictions) != length(actual_combined)) {
      cat("Warning: Mismatch between predictions length (", length(mean_predictions), 
          ") and actual values length (", length(actual_combined), ") for window", window_num, "\n")
      
      # Adjust to the shorter length
      min_length <- min(length(mean_predictions), length(actual_combined))
      mean_predictions <- mean_predictions[1:min_length]
      median_predictions <- median_predictions[1:min_length]
      actual_combined <- actual_combined[1:min_length]
    }
    
    # Calculate errors
    errors_mean <- calculate_errors(actual_combined, mean_predictions)
    errors_median <- calculate_errors(actual_combined, median_predictions)
    
    return(list(
      actual = actual_combined,
      predicted_mean = mean_predictions,
      predicted_median = median_predictions,
      errors_mean = errors_mean,
      errors_median = errors_median,
      test_indices = test_indices
    ))
  }, error = function(e) {
    cat("Error in prediction for window", window_num, ":", e$message, "\n")
    return(list(
      actual = actual_combined,
      predicted_mean = NA,
      predicted_median = NA,
      errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
      errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
      test_indices = test_indices
    ))
  })
}

# Function to evaluate all windows
evaluate_all_windows <- function(models, full_data) {
  # Determine number of windows based on models list
  num_windows <- length(models)
  
  results <- list()
  for(i in 1:num_windows) {
    cat("\nEvaluating window", i, "\n")
    # Evaluate predictions with the rolling window structure
    results[[i]] <- evaluate_predictions(
      models[[i]], 
      full_data,
      i
    )
  }
  
  return(results)
}

# Helper function to summarize indices (group consecutive sequences)
summarize_indices <- function(indices) {
  if(length(indices) == 0) return(character(0))
  
  gaps <- diff(indices) > 1
  group_starts <- c(TRUE, gaps)
  group_ends <- c(gaps, TRUE)
  
  sequences <- character(0)
  start_idx <- 1
  
  for(i in 1:length(indices)) {
    if(group_starts[i]) start_idx <- i
    if(group_ends[i]) {
      end_idx <- i
      if(start_idx == end_idx) {
        sequences <- c(sequences, as.character(indices[start_idx]))
      } else {
        sequences <- c(sequences, paste(indices[start_idx], "-", indices[end_idx], sep=""))
      }
    }
  }
  
  return(sequences)
}

# Improved function to print summary tables that handles NA results
print_summary_tables <- function(all_evaluations) {
  for(var_name in names(all_evaluations)) {
    cat("\nResults for", var_name, "\n")
    
    # Determine number of windows
    num_windows <- length(all_evaluations[[var_name]])
    
    # Mean-based metrics
    cat("\nMean-based metrics:\n")
    window_results_mean <- data.frame()
    for(i in 1:num_windows) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      
      # Check if window_errors contains valid data
      if(all(is.na(window_errors))) {
        cat("Window", i, "has no valid mean-based metrics\n")
        window_errors <- data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
      }
      
      window_errors$Window <- i
      window_results_mean <- rbind(window_results_mean, window_errors)
    }
    if(nrow(window_results_mean) > 0) {
      print(round(window_results_mean[, c("Window", "MSE", "MAE", "MAPE", "SMAPE", "MASE", "OWA")], 4))
    } else {
      cat("No valid mean-based metrics found\n")
    }
    
    # Median-based metrics
    cat("\nMedian-based metrics:\n")
    window_results_median <- data.frame()
    for(i in 1:num_windows) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      
      # Check if window_errors contains valid data
      if(all(is.na(window_errors))) {
        cat("Window", i, "has no valid median-based metrics\n")
        window_errors <- data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
      }
      
      window_errors$Window <- i
      window_results_median <- rbind(window_results_median, window_errors)
    }
    if(nrow(window_results_median) > 0) {
      print(round(window_results_median[, c("Window", "MSE", "MAE", "MAPE", "SMAPE", "MASE", "OWA")], 4))
    } else {
      cat("No valid median-based metrics found\n")
    }
    
    # Print test indices for each window
    cat("\nTest indices for each window:\n")
    for(i in 1:num_windows) {
      test_indices <- all_evaluations[[var_name]][[i]]$test_indices
      if(length(test_indices) > 0) {
        cat("Window", i, ": ", 
            paste(summarize_indices(test_indices), collapse=", "), 
            " (", length(test_indices), " rows)\n", sep="")
      } else {
        cat("Window", i, ": No valid test indices\n")
      }
    }
  }
}

# Example of how to use these functions with your data:
 all_evaluations_rwall <- list()
 for(dep_var_name in names(dep_var_datasets)) {
   cat("\nEvaluating", dep_var_name, "\n")
   all_evaluations_rwall[[dep_var_name]] <- evaluate_all_windows(all_models_rwall[[dep_var_name]], dep_var_datasets[[dep_var_name]])
 }
 print_summary_tables(all_evaluations_rwall)
```

## Calculate model weights based on test&train SMAPE
```{r}
# Improved function to calculate model weights with NA handling
calculate_model_weights <- function(all_evaluations) {
  weights_list <- list()
  
  for(var_name in names(all_evaluations)) {
    cat("\nCalculating weights for", var_name, "\n")
    
    # Extract SMAPE for both mean and median
    smape_values_mean <- sapply(all_evaluations[[var_name]], function(x) {
      if(is.null(x$errors_mean) || all(is.na(x$errors_mean))) {
        return(NA)
      } else {
        return(x$errors_mean$SMAPE)
      }
    })
    
    smape_values_median <- sapply(all_evaluations[[var_name]], function(x) {
      if(is.null(x$errors_median) || all(is.na(x$errors_median))) {
        return(NA)
      } else {
        return(x$errors_median$SMAPE)
      }
    })
    
    # Check if we have any valid SMAPE values
    if(all(is.na(smape_values_mean)) && all(is.na(smape_values_median))) {
      cat("Warning: No valid SMAPE values found for", var_name, "\n")
      weights_list[[var_name]] <- list(
        mean = data.frame(Window = integer(0), SMAPE = numeric(0), Weight = numeric(0), Method = character(0)),
        median = data.frame(Window = integer(0), SMAPE = numeric(0), Weight = numeric(0), Method = character(0))
      )
      next
    }
    
    # Small value to avoid division by zero
    epsilon <- 1e-10
    
    # Calculate weights for mean-based SMAPE
    if(!all(is.na(smape_values_mean))) {
      # Remove NA values for calculation
      valid_mean_indices <- which(!is.na(smape_values_mean))
      valid_mean_values <- smape_values_mean[valid_mean_indices]
      
      # Calculate weights only for valid values
      inverse_weights_mean <- 1 / pmax(valid_mean_values, epsilon)
      normalized_weights_mean <- inverse_weights_mean / sum(inverse_weights_mean)
      
      # Create a dataframe with weights only for valid values
      weights_df_mean <- data.frame(
        Window = valid_mean_indices,
        SMAPE = valid_mean_values,
        Weight = normalized_weights_mean,
        Method = "Mean"
      )
      
      # Sort by SMAPE
      weights_df_mean <- weights_df_mean[order(weights_df_mean$SMAPE), ]
    } else {
      cat("Warning: No valid mean-based SMAPE values found for", var_name, "\n")
      weights_df_mean <- data.frame(
        Window = integer(0),
        SMAPE = numeric(0),
        Weight = numeric(0),
        Method = character(0)
      )
    }
    
    # Calculate weights for median-based SMAPE
    if(!all(is.na(smape_values_median))) {
      # Remove NA values for calculation
      valid_median_indices <- which(!is.na(smape_values_median))
      valid_median_values <- smape_values_median[valid_median_indices]
      
      # Calculate weights only for valid values
      inverse_weights_median <- 1 / pmax(valid_median_values, epsilon)
      normalized_weights_median <- inverse_weights_median / sum(inverse_weights_median)
      
      # Create a dataframe with weights only for valid values
      weights_df_median <- data.frame(
        Window = valid_median_indices,
        SMAPE = valid_median_values,
        Weight = normalized_weights_median,
        Method = "Median"
      )
      
      # Sort by SMAPE
      weights_df_median <- weights_df_median[order(weights_df_median$SMAPE), ]
    } else {
      cat("Warning: No valid median-based SMAPE values found for", var_name, "\n")
      weights_df_median <- data.frame(
        Window = integer(0),
        SMAPE = numeric(0),
        Weight = numeric(0),
        Method = character(0)
      )
    }
    
    weights_list[[var_name]] <- list(
      mean = weights_df_mean,
      median = weights_df_median
    )
  }
  
  # Print results
  for(var_name in names(weights_list)) {
    if(nrow(weights_list[[var_name]]$mean) > 0) {
      cat("\nWeights for", var_name, "(Mean):\n")
      print(round(weights_list[[var_name]]$mean[, -4], 4))
    } else {
      cat("\nNo valid weights for", var_name, "(Mean)\n")
    }
    
    if(nrow(weights_list[[var_name]]$median) > 0) {
      cat("\nWeights for", var_name, "(Median):\n")
      print(round(weights_list[[var_name]]$median[, -4], 4))
    } else {
      cat("\nNo valid weights for", var_name, "(Median)\n")
    }
  }
  
  return(weights_list)
}

# Example usage:
# model_weights <- calculate_model_weights(all_evaluations_rwall)

# Function to create weighted ensemble predictions
create_weighted_ensemble <- function(all_evaluations, all_models, holdout_data, weights_list) {
  ensemble_results <- list()
  
  for(var_name in names(all_evaluations)) {
    cat("\nCreating ensemble for", var_name, "\n")
    
    # Skip if no weights are available
    if(nrow(weights_list[[var_name]]$mean) == 0 && 
       nrow(weights_list[[var_name]]$median) == 0) {
      cat("No valid weights available for", var_name, ", skipping ensemble creation\n")
      ensemble_results[[var_name]] <- list(
        mean_ensemble = NA,
        median_ensemble = NA,
        actual = NA
      )
      next
    }
    
    # Get holdout data
    holdout_x <- as.data.frame(holdout_data[[var_name]][, -1, drop = FALSE])
    holdout_y <- holdout_data[[var_name]][, 1]
    
    # Prepare containers for predictions
    mean_predictions <- list()
    median_predictions <- list()
    
    # Get predictions from all valid models
    valid_windows <- unique(c(
      weights_list[[var_name]]$mean$Window,
      weights_list[[var_name]]$median$Window
    ))
    
    for(i in valid_windows) {
      if(i > length(all_models[[var_name]])) {
        cat("Warning: Window", i, "model not available for", var_name, "\n")
        next
      }
      
      # Check for NA in holdout data
      if(any(is.na(holdout_x))) {
        cat("Warning: NA values found in holdout predictors for", var_name, "\n")
        # Impute NA values with column means
        for(col in 1:ncol(holdout_x)) {
          if(any(is.na(holdout_x[, col]))) {
            holdout_x[is.na(holdout_x[, col]), col] <- mean(holdout_x[, col], na.rm = TRUE)
          }
        }
      }
      
      # Make predictions
      tryCatch({
        model <- all_models[[var_name]][[i]]
        pred <- predict(model, newdata = holdout_x, burn = 100)
        
        # Store mean predictions
        mean_pred <- colMeans(pred$distribution)
        mean_predictions[[as.character(i)]] <- mean_pred
        
        # Store median predictions
        median_pred <- apply(pred$distribution, 2, median)
        median_predictions[[as.character(i)]] <- median_pred
      }, error = function(e) {
        cat("Error predicting with window", i, "model for", var_name, ":", e$message, "\n")
      })
    }
    
    # Check if we have any valid predictions
    if(length(mean_predictions) == 0 && length(median_predictions) == 0) {
      cat("No valid predictions available for", var_name, "\n")
      ensemble_results[[var_name]] <- list(
        mean_ensemble = NA,
        median_ensemble = NA,
        actual = holdout_y
      )
      next
    }
    
    # Create mean-based ensemble
    if(nrow(weights_list[[var_name]]$mean) > 0) {
      # Initialize ensemble predictions
      mean_ensemble <- numeric(length(holdout_y))
      
      # Apply weights
      for(i in 1:nrow(weights_list[[var_name]]$mean)) {
        window <- weights_list[[var_name]]$mean$Window[i]
        weight <- weights_list[[var_name]]$mean$Weight[i]
        
        # Check if we have predictions for this window
        if(!is.null(mean_predictions[[as.character(window)]])) {
          # Add weighted prediction
          mean_ensemble <- mean_ensemble + weight * mean_predictions[[as.character(window)]]
        }
      }
    } else {
      mean_ensemble <- NA
    }
    
    # Create median-based ensemble
    if(nrow(weights_list[[var_name]]$median) > 0) {
      # Initialize ensemble predictions
      median_ensemble <- numeric(length(holdout_y))
      
      # Apply weights
      for(i in 1:nrow(weights_list[[var_name]]$median)) {
        window <- weights_list[[var_name]]$median$Window[i]
        weight <- weights_list[[var_name]]$median$Weight[i]
        
        # Check if we have predictions for this window
        if(!is.null(median_predictions[[as.character(window)]])) {
          # Add weighted prediction
          median_ensemble <- median_ensemble + weight * median_predictions[[as.character(window)]]
        }
      }
    } else {
      median_ensemble <- NA
    }
    
    # Store results
    ensemble_results[[var_name]] <- list(
      mean_ensemble = mean_ensemble,
      median_ensemble = median_ensemble,
      actual = holdout_y
    )
  }
  
  return(ensemble_results)
}
```

```{r}
# Calculate model weights
model_weights_rwall <- calculate_model_weights(all_evaluations_rwall)
```


```{r}
# Function to generate prediction tables and SMAPE for holdout data for all rolling windows and variables
predict_all_rolling_windows_holdout <- function(all_models_rolling, hold_out_dataset) {
  # Initialize results storage
  holdout_smape_results <- list()
  
  # Loop through each rolling window
  for(window_num in 1:5) {  # 5 rolling windows
    cat("\n================================================================\n")
    cat("ROLLING WINDOW", window_num, "HOLD-OUT PREDICTIONS\n")
    cat("================================================================\n")
    
    # Loop through each variable
    for(var_name in names(all_models_rolling)) {
      # Skip if this model doesn't exist
      if(is.null(all_models_rolling[[var_name]]) || 
         length(all_models_rolling[[var_name]]) < window_num || 
         is.null(all_models_rolling[[var_name]][[window_num]])) {
        cat("\nSkipping", var_name, "for window", window_num, "- model not available\n")
        next
      }
      
      # Get the model for this window and variable
      model <- all_models_rolling[[var_name]][[window_num]]
      
      # Get the hold-out data for this variable
      holdout_data <- hold_out_dataset[[var_name]]
      
      # Get number of rows in the holdout data
      num_rows <- nrow(holdout_data)
      
      # Create proper row identifiers
      # First 8 rows are 80-87, and if there's a 9th row, it's row 112
      if(num_rows <= 7) {
        row_ids <- 80:86#period
      } else {
        row_ids <- c(80:86, 112)#period
      }
      
      # Get actual values and predictors from holdout data
      holdout_actual_values <- holdout_data[, 1]
      holdout_pred_data <- as.data.frame(holdout_data[, -1])
      
      # Make predictions on hold-out period using current window's model
      holdout_pred <- predict(model, newdata = holdout_pred_data, burn = 100)
      holdout_mean_predictions <- colMeans(holdout_pred$distribution)
      holdout_median_predictions <- apply(holdout_pred$distribution, 2, median)
      
      # Calculate errors
      holdout_mean_errors <- holdout_mean_predictions - holdout_actual_values
      holdout_median_errors <- holdout_median_predictions - holdout_actual_values
      
      # Calculate SMAPE for mean predictions on holdout
      holdout_mean_smape <- mean(2 * abs(holdout_actual_values - holdout_mean_predictions) / 
                                 (abs(holdout_actual_values) + abs(holdout_mean_predictions))) * 100
      
      # Calculate SMAPE for median predictions on holdout
      holdout_median_smape <- mean(2 * abs(holdout_actual_values - holdout_median_predictions) / 
                                   (abs(holdout_actual_values) + abs(holdout_median_predictions))) * 100
      
      # Store holdout SMAPE results
      if (is.null(holdout_smape_results[[var_name]])) {
        holdout_smape_results[[var_name]] <- data.frame(
          Window = numeric(),
          Mean_SMAPE = numeric(),
          Median_SMAPE = numeric()
        )
      }
      
      holdout_smape_results[[var_name]] <- rbind(holdout_smape_results[[var_name]], 
                                                 data.frame(
                                                   Window = window_num,
                                                   Mean_SMAPE = holdout_mean_smape,
                                                   Median_SMAPE = holdout_median_smape
                                                 ))
      
      # Display SMAPE for holdout data
      cat("\n----------------------------------------------------------------\n")
      cat("SMAPE FOR ROLLING WINDOW", window_num, "HOLD-OUT PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      cat("Mean SMAPE:", round(holdout_mean_smape, 4), "%\n")
      cat("Median SMAPE:", round(holdout_median_smape, 4), "%\n")
      
      # Display mean holdout results
      cat("\n----------------------------------------------------------------\n")
      cat("TABLE: ROLLING WINDOW", window_num, "MEAN HOLD-OUT PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      
      holdout_mean_results <- data.frame(
        Row = row_ids,
        Actual = holdout_actual_values,
        Predicted = holdout_mean_predictions,
        Error = holdout_mean_errors
      )
      
      # Round numeric values before printing
      holdout_mean_results_rounded <- holdout_mean_results
      holdout_mean_results_rounded$Actual <- round(holdout_mean_results_rounded$Actual, 4)
      holdout_mean_results_rounded$Predicted <- round(holdout_mean_results_rounded$Predicted, 4)
      holdout_mean_results_rounded$Error <- round(holdout_mean_results_rounded$Error, 4)
      
      print(holdout_mean_results_rounded)
      
      # Label the quarter_four row
      if(num_rows > 7) {
        cat("Note: Row 112 is from quarter_four data\n")
      }
      
      # Display median holdout results
      cat("\n----------------------------------------------------------------\n")
      cat("TABLE: ROLLING WINDOW", window_num, "MEDIAN HOLD-OUT PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      
      holdout_median_results <- data.frame(
        Row = row_ids,
        Actual = holdout_actual_values,
        Predicted = holdout_median_predictions,
        Error = holdout_median_errors
      )
      
      # Round numeric values before printing
      holdout_median_results_rounded <- holdout_median_results
      holdout_median_results_rounded$Actual <- round(holdout_median_results_rounded$Actual, 4)
      holdout_median_results_rounded$Predicted <- round(holdout_median_results_rounded$Predicted, 4)
      holdout_median_results_rounded$Error <- round(holdout_median_results_rounded$Error, 4)
      
      print(holdout_median_results_rounded)
      
      # Label the quarter_four row
      if(num_rows > 7) {
        cat("Note: Row 112 is from quarter_four data\n")
      }
    }
  }
  
  # Display summary of SMAPE results for all windows and variables (hold-out data)
  cat("\n================================================================\n")
  cat("SUMMARY OF SMAPE RESULTS FOR ALL ROLLING WINDOWS AND VARIABLES (HOLD-OUT DATA)\n")
  cat("================================================================\n")
  
  for(var_name in names(holdout_smape_results)) {
    cat("\n----------------------------------------------------------------\n")
    cat("HOLD-OUT SMAPE SUMMARY FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    print(round(holdout_smape_results[[var_name]], 4))
    
    # Calculate and display average SMAPE across all windows
    avg_mean_smape <- mean(holdout_smape_results[[var_name]]$Mean_SMAPE)
    avg_median_smape <- mean(holdout_smape_results[[var_name]]$Median_SMAPE)
    
    cat("\nAverage Hold-out Mean SMAPE across all windows:", round(avg_mean_smape, 4), "%\n")
    cat("Average Hold-out Median SMAPE across all windows:", round(avg_median_smape, 4), "%\n")
  }
  
  # Return the hold-out results
  return(holdout_smape_results)
}

# Example of how to run the function with your models and data
smape_results_rolling_holdout <- predict_all_rolling_windows_holdout(all_models_rwall, hold_out_dataset)
```


```{r}
# Function to aggregate hold-out period predictions for all windows and calculate SMAPE for each dep var
aggregate_and_calculate_holdout_smape <- function(smape_results_holdout, all_models_rwall, hold_out_dataset) {
  # Initialize results storage for aggregated SMAPE
  aggregate_smape_results <- list()
  
  # Loop through each dependent variable
  for (var_name in names(all_models_rwall)) {
    cat("\n================================================================\n")
    cat("AGGREGATED HOLD-OUT PREDICTIONS AND SMAPE FOR", toupper(var_name), "\n")
    cat("================================================================\n")
    
    # Initialize vectors to store actual, mean predictions, and median predictions
    all_actuals <- numeric()
    all_mean_preds <- numeric()
    all_median_preds <- numeric()
    
    # Get the hold-out data for this variable
    holdout_data <- hold_out_dataset[[var_name]]
    actual_values <- holdout_data[, 1]
    pred_data <- as.data.frame(holdout_data[, -1])
    
    # Loop through each window to collect predictions
    for(window_num in 1:4) {
      # Get the model for this window and variable
      model <- all_models_rwall[[var_name]][[window_num]]
      
      # Skip if model is missing
      if(is.null(model)) {
        cat("\nModel not available for", var_name, "window", window_num, ". Skipping.\n")
        next
      }
      
      # Make predictions on hold-out period
      pred <- predict(model, newdata = pred_data, burn = 100)
      mean_predictions <- colMeans(pred$distribution)
      median_predictions <- apply(pred$distribution, 2, median)
      
      # Add to the vectors
      all_actuals <- c(all_actuals, actual_values)
      all_mean_preds <- c(all_mean_preds, mean_predictions)
      all_median_preds <- c(all_median_preds, median_predictions)
    }
    
    # Calculate SMAPE for the aggregated predictions using mean method
    mean_smape <- mean(2 * abs(all_actuals - all_mean_preds) / 
                         (abs(all_actuals) + abs(all_mean_preds))) * 100
    
    # Calculate SMAPE for the aggregated predictions using median method
    median_smape <- mean(2 * abs(all_actuals - all_median_preds) / 
                           (abs(all_actuals) + abs(all_median_preds))) * 100
    
    # Store aggregated SMAPE results
    aggregate_smape_results[[var_name]] <- data.frame(
      Mean_SMAPE = mean_smape,
      Median_SMAPE = median_smape
    )
    
    # Display aggregated SMAPE results for this variable
    cat("\n----------------------------------------------------------------\n")
    cat("AGGREGATED HOLD-OUT SMAPE FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    cat("Mean SMAPE:", round(mean_smape, 4), "%\n")
    cat("Median SMAPE:", round(median_smape, 4), "%\n")
  }
  
  # Return the aggregated SMAPE results for all variables
  return(aggregate_smape_results)
}

# Run the function to calculate aggregated SMAPE for all dep vars
aggregate_smape_results_holdout <- aggregate_and_calculate_holdout_smape(smape_results_rolling_holdout, all_models_rwall, hold_out_dataset)

# Display aggregated results
cat("\n================================================================\n")
cat("SUMMARY OF AGGREGATED HOLD-OUT SMAPE RESULTS FOR ALL VARIABLES\n")
cat("================================================================\n")
for (var_name in names(aggregate_smape_results_holdout)) {
  cat("\n----------------------------------------------------------------\n")
  cat("AGGREGATED HOLD-OUT SMAPE SUMMARY FOR", toupper(var_name), "\n")
  cat("----------------------------------------------------------------\n")
  print(round(aggregate_smape_results_holdout[[var_name]], 4))
}
```

# Summary stat
```{r}
get_model_diagnostics <- function(model, window_data) {
  # Numerically stable log-mean-exp function
  log_mean_exp <- function(x) {
    m <- max(x)
    m + log(mean(exp(x - m)))
  }
  
  window_stats <- lapply(seq_along(model), function(i) {
    ll <- model[[i]]$log.likelihood   # log-likelihood from the model
    n_train <- nrow(window_data[[i]]$train)
    
    # --- DIC Calculation ---
    # We always compute DIC on the full set of draws.
    # If ll is a matrix, use all elements; if it's a vector, use it directly.
    if (!is.null(dim(ll))) {
      # ll is already a matrix (n_iter x n_train)
      ll_vec <- as.vector(ll)
    } else {
      ll_vec <- ll
    }
    
    D_bar <- -2 * mean(ll_vec)
    D_theta_bar <- -2 * max(ll_vec)  # Using the maximum (posterior mode) as a plug‐in estimate
    p_D <- D_bar - D_theta_bar
    DIC <- (D_bar + p_D) / n_train  # reporting per-observation
    
    # --- WAIC Calculation ---
    # Check if we can reshape ll into a point-wise matrix.
    if (is.null(dim(ll)) && (length(ll) %% n_train != 0)) {
      # The log likelihood is aggregated (one value per iteration)
      # Compute overall lppd and p_waic, then scale per observation.
      lppd <- log_mean_exp(ll)
      p_waic <- var(ll)
      WAIC <- (-2 * (lppd - p_waic)) / n_train
    } else {
      # If ll is already a matrix, or if its length is divisible by n_train, reshape it.
      if (is.null(dim(ll))) {
        n_iter <- length(ll) / n_train
        ll_matrix <- matrix(ll, ncol = n_train, byrow = TRUE)
      } else {
        ll_matrix <- ll
      }
      # For each observation (i.e. each column), compute a stable log-mean-exp and variance.
      lppd <- sum(apply(ll_matrix, 2, log_mean_exp))
      p_waic <- sum(apply(ll_matrix, 2, var))
      WAIC <- (-2 * (lppd - p_waic)) / n_train
    }
    
    data.frame(
      Window = i,
      Training_Size = n_train,
      DIC = DIC,
      WAIC = WAIC
    )
  })
  
  do.call(rbind, window_stats)
}

print_all_diagnostics <- function(diagnostics) {
  for(dep_var in names(diagnostics)) {
    cat("\n===", dep_var, "===\n")
    print(diagnostics[[dep_var]])
  }
}
```

```{r}
all_diagnostics <- list()
for(dep_var in names(all_models_rwall)) {
  all_diagnostics[[dep_var]] <- get_model_diagnostics(all_models_rwall[[dep_var]], rolling_splits[[dep_var]])
}

print_all_diagnostics(all_diagnostics)
```


## Function of getting LOOIC 
```{r}
calculate_looic <- function(model, window_data) {
  # Get training sample size 
  n_train <- nrow(window_data$train)
  
  # Calculate LOOIC
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  # normalized LOOIC
  looic <- (-2 * sum(loo_liks)) / n_train
  
  return(looic)
}

# Function to apply for all windows
get_all_looic <- function(models, windows) {
  n_windows <- length(models)
  looic_results <- numeric(n_windows)
  
  for (i in 1:n_windows) {
    looic_results[i] <- calculate_looic(models[[i]], windows[[i]])
  }
  
  return(looic_results)
}
```

### Apply the calculation to all dependent variables
```{r}
all_looic <- list()
for (dep_var in names(all_models_rwall)) {
  all_looic[[dep_var]] <- get_all_looic(
    all_models_rwall[[dep_var]], 
    rolling_splits[[dep_var]]
  )
}
# Display results
for (dep_var in names(all_looic)) {
  cat(sprintf("\nNormalized LOOIC Results for %s\n", dep_var))
  for (i in 1:length(all_looic[[dep_var]])) {
    cat(sprintf("Window %d: %.4f\n", i, all_looic[[dep_var]][i]))
  }
}
```

## 95% confi interval
```{r}
calculate_confidence_intervals <- function(all_models_rwall, rolling_splits) {
  for(dep_var in names(all_models_rwall)) {
    cat("\n95% Confidence Intervals for", dep_var, "\n")
    for(window in seq_along(all_models_rwall[[dep_var]])) {
      cat("\nWindow", window, ":\n")
      
      model <- all_models_rwall[[dep_var]][[window]]
      coefficients <- model$coefficients
      
      # Calculate statistics
      stats <- apply(coefficients, 2, function(x) {
        quantiles <- quantile(x, probs = c(0.025, 0.975))
        c("2.5%" = quantiles[1],
          "Mean" = mean(x),
          "Median" = median(x),
          "97.5%" = quantiles[2])
      })
      
      stats_df <- as.data.frame(t(stats))
      stats_df <- round(stats_df, 4)
      print(stats_df)
    }
  }
}

confidence_intervals <- calculate_confidence_intervals(all_models_rwall, rolling_splits)
```

## mcmc size 
```{r}
library(coda)
get_mcmc_stats <- function(all_models_rwall, rolling_splits) {
  for(dep_var in names(all_models_rwall)) {
    cat("\nEffective Sample Size for", dep_var, "\n")
    cat("===============================\n")
    
    n_windows <- length(all_models_rwall[[dep_var]])
    
    for(window in 1:n_windows) {
      model <- all_models_rwall[[dep_var]][[window]]
      
      # Get coefficients for this window
      coef_matrix <- model$coefficients
      param_names <- colnames(coef_matrix)
      
      cat(sprintf("\nWindow %d\n", window))
      cat("-------------------\n")
      
      for(param in param_names) {
        # Extract chain for this parameter
        chain <- mcmc(coef_matrix[, param])
        
        # Calculate ESS
        ess <- try(effectiveSize(chain), silent = TRUE)
        
        # Print results
        cat(sprintf("%s: %.1f\n", param, 
                    if(inherits(ess, "try-error")) NA else ess))
      }
    }
  }
}

# Apply the function
mcmc_stats <- get_mcmc_stats(all_models_rwall, rolling_splits)
```



## Bayesian R²  (Gelman et al.)

```{r}
## Function to compute Bayesian R² using posterior draws on the training set
calculate_bayes_R2 <- function(model, train_data, burn = 100) {
  # Extract the observed response (assumes the first column is the dependent variable)
  y_train <- train_data[, 1]
  
  # Get posterior draws for the fitted (training) values.
  # When newdata is provided, predict() returns a list with a 'distribution'
  # element that is a matrix with rows = MCMC iterations and columns = training observations.
  pred_train <- predict(model, newdata = as.data.frame(train_data[, -1]), burn = burn)
  
  # Compute Bayesian R² for each MCMC draw:
  # For each draw (each row of the prediction matrix) compute:
  #   R² = var(fitted values) / (var(fitted values) + var(residuals))
  R2_draws <- apply(pred_train$distribution, 1, function(fitted_values) {
    var_fitted <- var(fitted_values)
    var_resid  <- var(y_train - fitted_values)
    R2 <- var_fitted / (var_fitted + var_resid)
    return(R2)
  })
  
  return(R2_draws)
}

## Loop over each dependent variable and each window to compute Bayesian R²
bayesian_R2_results <- list()

for (dep_var_name in names(all_models_rwall)) {
  models_list <- all_models_rwall[[dep_var_name]]
  
  # Data frame to store summary metrics for each window
  bayes_R2_summary <- data.frame(
    Window = integer(),
    Mean_R2 = numeric(),
    Median_R2 = numeric(),
    Lower_R2 = numeric(),
    Upper_R2 = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Loop through each of the 7 expanding windows
  for (i in 1:length(models_list)) {
    # Get the training data for the current window
    train_data <- rolling_splits[[dep_var_name]][[i]]$train
    
    # Extract the fitted model for this window
    model <- models_list[[i]]
    
    # Compute the Bayesian R² draws for the training set
    R2_draws <- calculate_bayes_R2(model, train_data, burn = 100)
    
    # Summarize the draws (you can adjust the summary metrics as desired)
    bayes_R2_summary <- rbind(
      bayes_R2_summary,
      data.frame(
        Window = i,
        Mean_R2 = mean(R2_draws),
        Median_R2 = median(R2_draws),
        Lower_R2 = quantile(R2_draws, 0.025),
        Upper_R2 = quantile(R2_draws, 0.975)
      )
    )
  }
  
  # Store the summary for the current dependent variable
  bayesian_R2_results[[dep_var_name]] <- bayes_R2_summary
}

## Print the Bayesian R² summary for each dependent variable and window
for (dep_var_name in names(bayesian_R2_results)) {
  cat("\nBayesian R² for", dep_var_name, ":\n")
  print(round(bayesian_R2_results[[dep_var_name]], 4))
}

```
## Ljung Box 
```{r}
# Load necessary package (Box.test is in stats, so this is optional)
library(stats)

# Function to compute Ljung-Box test diagnostics for each window
get_ljung_box_diagnostics <- function(models, window_data, lags = 10) {
  results <- list()
  
  # Loop over each dependent variable
  for (dep_var_name in names(models)) {
    window_results <- list()
    
    # Loop over each expanding window (for this dep var)
    for (i in seq_along(models[[dep_var_name]])) {
      # Extract the training data for the current window
      train_data <- window_data[[dep_var_name]][[i]]$train
      # Assume the first column is the dependent variable (actual values)
      y_train <- train_data[, 1]
      
      # Obtain one-step-ahead predictions for the training set
      # Here we use the same burn-in as in your evaluation (e.g., 100)
      pred <- predict(models[[dep_var_name]][[i]], 
                      newdata = as.data.frame(train_data[, -1]), 
                      burn = 100)
      
      # Compute the mean fitted value for each observation (across MCMC draws)
      fitted_values <- colMeans(pred$distribution)
      
      # Calculate residuals (actual minus fitted)
      residuals <- y_train - fitted_values
      
      # Perform the Ljung-Box test on the residuals using the specified number of lags
      lb_test <- Box.test(residuals, lag = lags, type = "Ljung-Box")
      
      # Save the results for the current window
      window_results[[i]] <- data.frame(
        Window = i,
        Training_Size = nrow(train_data),
        LB_Statistic = lb_test$statistic,
        LB_pvalue = lb_test$p.value
      )
    }
    
    # Combine the results for all windows for this dependent variable
    results[[dep_var_name]] <- do.call(rbind, window_results)
  }
  
  return(results)
}

# Calculate the Ljung-Box diagnostics (with default lag = 10)
ljung_box_results <- get_ljung_box_diagnostics(all_models_rwall, rolling_splits, lags = 10)

# Print the Ljung-Box test results for each dependent variable
for (dep_var in names(ljung_box_results)) {
  cat("\nLjung-Box Test Diagnostics for", dep_var, ":\n")
  print(ljung_box_results[[dep_var]])
}

```

## Posterior Prob
```{r}
analyze_pips <- function(all_models_rwall, rolling_splits) {
  for (dep_var in names(all_models_rwall)) {
    cat(sprintf("\nPosterior Inclusion Probabilities for %s\n", dep_var))
    
    models <- all_models_rwall[[dep_var]]
    for (window in seq_along(models)) {
      cat(sprintf("\nWindow %d:\n", window))
      model <- models[[window]]
      
      if (!is.null(model$coefficients)) {
        # Calculate average inclusion probabilities across MCMC iterations
        inclusion_matrix <- model$coefficients[,1:ncol(model$predictors)]
        pips <- colMeans(inclusion_matrix != 0) 
        names(pips) <- colnames(model$predictors)
        pips <- pips[-1] # Remove intercept
        pips <- sort(pips, decreasing = TRUE)
        print(pips)
      } else {
        cat("No coefficients available\n")
      }
    }
  }
}

pips_results <- analyze_pips(all_models_rwall, rolling_splits)
```


## Geweke’s Diagnostic
check if your MCMC chain has converged
p < 0.05 means there's a statistically significant difference between the means of first and last parts (bad for significant)
```{r}
calculate_geweke <- function(all_models_rwall, rolling_splits) {
  for(dep_var in names(all_models_rwall)) {
    cat("\nGeweke's Diagnostic for", dep_var, "\n")
    
    n_windows <- length(all_models_rwall[[dep_var]])
    
    for(window in 1:n_windows) {
      model <- all_models_rwall[[dep_var]][[window]]
      # Get coefficients matrix
      coef_matrix <- as.matrix(model$coefficients)
      cat(sprintf("\nWindow %d\n", window))
      cat("-------------------\n")
      # Loop each parameter (column)
      for(j in 1:ncol(coef_matrix)) {
        param_name <- colnames(coef_matrix)[j]
        chain <- mcmc(coef_matrix[, j])
        # the default function in R 
        geweke_stats <- geweke.diag(chain, frac1=0.1, frac2=0.5)
        
        # Extract z-score
        z_score <- as.numeric(geweke_stats$z)
        
        # Calculate p-value only if z_score is not NA
        if(!is.na(z_score)) {
          p_value <- 2 * pnorm(-abs(z_score))
          cat(sprintf("%s:\n", param_name))
          cat(sprintf("  Z-statistic: %.4f\n", z_score))
          cat(sprintf("  p-value: %.4f\n", p_value))
          
          if(!is.na(p_value) && p_value < 0.05) {
            cat("  ** Possible convergence issue (p < 0.05)\n")
          }
        } else {
          cat(sprintf("%s: Unable to calculate Geweke diagnostic\n", param_name))
        }
      }
    }
  }
}

# Apply 
geweke_results <- calculate_geweke(all_models_rwall, rolling_splits)
```


## Heidelberger-Welch
```{r}
calculate_heidel_welch <- function(model, window_data) {
  # Print diagnostic results for each parameter
  for(i in 1:length(model)) {
    cat("\nWindow", i, "\n")
    cat("-------------------\n")
    
    # Convert coefficients to mcmc object
    coef_matrix <- as.matrix(model[[i]]$coefficients)
    
    # Test each parameter
    for(j in 1:ncol(coef_matrix)) {
      param_name <- colnames(coef_matrix)[j]
      chain <- mcmc(coef_matrix[,j])
      
      # Run heidel.diag and capture output
      tryCatch({
        test <- heidel.diag(chain)
        
        if(!is.null(test)) {
          cat("\nParameter:", param_name, "\n")
          cat("Stationarity test:", ifelse(test[1,1] == 1, "PASSED", "FAILED"), "\n")
          cat("p-value:", format(test[1,3], digits=4), "\n")
          cat("Start iteration:", test[1,2], "\n")
          cat("Mean:", format(test[1,4], digits=4), "\n")
          cat("Halfwidth test:", ifelse(test[1,5] == 1, "PASSED", "FAILED"), "\n")
          cat("Halfwidth:", format(test[1,6], digits=4), "\n")
        } else {
          cat("\nParameter:", param_name, ": Test returned null result\n")
        }
      }, error = function(e) {
        cat("\nParameter:", param_name, ": Test failed -", e$message, "\n")
      })
    }
  }
}

# Apply to all dependent variables
for(dep_var in names(all_models_rwall)) {
  cat("\n\n=== Results for", dep_var, "===\n")
  heidel_results = calculate_heidel_welch(all_models_rwall[[dep_var]], rolling_splits[[dep_var]])
}
```


## Raftery_lewis
```{r}
calculate_convergence_diagnostics <- function(model, window_data) {
  for(i in 1:length(model)) {
    cat("\nWindow", i, "\n")
    cat("-------------------\n")
    
    coef_matrix <- as.matrix(model[[i]]$coefficients)
    
    for(j in 1:ncol(coef_matrix)) {
      param_name <- colnames(coef_matrix)[j]
      chain <- as.vector(coef_matrix[,j])
      
      cat("\nParameter:", param_name, "\n")
      
      # Basic diagnostics
      n_unique <- length(unique(chain))
      autocorr <- cor(chain[-1], chain[-length(chain)])
      
      cat("Chain diagnostics:\n")
      cat("Unique values:", n_unique, "\n")
      cat("Mean:", mean(chain), "\n")
      cat("SD:", sd(chain), "\n")
      cat("Autocorrelation:", autocorr, "\n")
      
      if(n_unique < 10) {
        cat("WARNING: Too few unique values for reliable convergence diagnostics\n")
        next
      }
      
      chain_mcmc <- as.mcmc(chain)
      
      # Both Raftery-Lewis and Geweke
      tryCatch({
        # Raftery-Lewis
        binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
        rl <- raftery.diag(binary_chain)
        
        cat("\nRaftery-Lewis Diagnostic:\n")
        cat("Iterations needed:", rl$resmatrix[1,"N"], "\n")
        cat("Burn-in needed:", rl$resmatrix[1,"M"], "\n")
        cat("Dependence factor:", rl$resmatrix[1,"I"], "\n")
        
        # Geweke
        gw <- geweke.diag(chain_mcmc)
        pvalue <- 2 * (1 - pnorm(abs(gw$z)))  # Two-tailed p-value from z-score
        
        cat("\nGeweke Test:\n")
        cat("Z-score:", gw$z, "\n")
        cat("P-value:", pvalue, "\n")
        
        # Combined assessment
        if(rl$resmatrix[1,"I"] > 5 || pvalue < 0.05) {
          cat("\nConvergence Issues Detected:\n")
          if(rl$resmatrix[1,"I"] > 5) {
            cat("- High dependence factor in Raftery-Lewis\n")
          }
          if(pvalue < 0.05) {
            cat("- Significant Geweke test (p < 0.05)\n")
          }
          if(autocorr > 0.7) {
            cat("- High autocorrelation detected\n")
          }
          if(length(chain) < rl$resmatrix[1,"N"]) {
            cat("- More iterations needed\n")
            cat(sprintf("  Current: %d, Recommended: %d\n", 
                       length(chain), rl$resmatrix[1,"N"]))
          }
        } else {
          cat("\nNo convergence issues detected\n")
        }
        
      }, error = function(e) {
        cat("\nDiagnostic calculation failed:\n")
        cat("- Error:", conditionMessage(e), "\n")
        if(autocorr > 0.7) {
          cat("- High autocorrelation might be causing issues\n")
        }
      })
    }
  }
}

# Run diagnostics
for(dep_var in names(all_models_rwall)) {
  cat("\n\n=== Convergence Diagnostics for", dep_var, "===\n")
  calculate_convergence_diagnostics(all_models_rwall[[dep_var]], rolling_splits[[dep_var]])
}
```


## predictive interval
```{r}
# Create extended test sets for rolling windows based on the new structure
extended_test_sets_rolling <- list()
for(dep_var_name in names(dep_var_datasets_modified)) {
  current_data <- dep_var_datasets_modified[[dep_var_name]]
  dataset_windows <- list()
  
  # Number of windows (should match the number of models)
  num_windows <- 5  # Assuming 5 windows as in the original code
  
  # Create test sets for each window using the new get_test_indices function
  for(window_num in 1:num_windows) {
    # Get the test indices based on the window number
    test_indices <- get_test_indices(window_num)
    
    # Extract test data
    test_data <- current_data[test_indices, ]
    dataset_windows[[window_num]] <- test_data
  }
  
  extended_test_sets_rolling[[dep_var_name]] <- dataset_windows
}

# Helper function to get test indices (same as in the evaluation code)
get_test_indices <- function(window_num) {
  total_rows <- 111  # Total number of rows in the dataset
  train_size <- 90   # Fixed training window size
  fixed_test_start <- 91  # Fixed test portion starts at row 71
  
  # Calculate training start and end based on window number
  train_start <- 1 + (window_num - 1) * 10
  train_end <- train_start + train_size - 1
  
  # Test indices consist of:
  # 1. Any rows before the training window (if training doesn't start at row 1)
  # 2. The fixed test portion (rows 71-111)
  
  # Part 1: Rows before training window (if any)
  before_train_indices <- if(train_start > 1) 1:(train_start-1) else integer(0)
  
  # Part 2: Fixed test portion (rows 71-111)
  fixed_test_indices <- fixed_test_start:total_rows
  
  # Combine both parts for complete test indices
  test_indices <- c(before_train_indices, fixed_test_indices)
  
  # Remove any indices that would overlap with training window
  test_indices <- test_indices[!test_indices %in% train_start:train_end]
  
  return(sort(test_indices))
}

# Predictive Intervals for Rolling Windows
get_predictive_intervals <- function(all_evaluations, all_models_rw, extended_test_sets_rolling) {
  for(dep_var in names(all_evaluations)) {
    # Get SMAPE values and window numbers for mean-based
    smape_mean <- sapply(all_evaluations[[dep_var]], function(x) x$errors_mean$SMAPE)
    top_5_mean <- order(smape_mean)[1:min(5, length(smape_mean))]  # Ensure we don't exceed available windows
    
    # Get SMAPE values and window numbers for median-based
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) x$errors_median$SMAPE)
    top_5_median <- order(smape_median)[1:min(5, length(smape_median))]  # Ensure we don't exceed available windows
    
    # Calculate intervals for mean-based top 5
    cat("\n=====================================")
    cat(sprintf("\n%s - Mean-Based Top 5 Models (by sMAPE)\n", dep_var))
    cat("=====================================\n")
    
    intervals_mean <- data.frame()
    for(window in top_5_mean) {
      pred_data <- extended_test_sets_rolling[[dep_var]][[window]]
      last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
      pred <- predict.bsts(all_models_rw[[dep_var]][[window]], 
                          newdata = as.data.frame(last_row_predictors),
                          burn = 100)
      dist <- pred$distribution[,1]
      intervals_mean <- rbind(intervals_mean, data.frame(
        Window = window,
        SMAPE = smape_mean[window],
        Lower_95 = quantile(dist, 0.025),
        Mean = mean(dist),
        Median = median(dist),
        Upper_95 = quantile(dist, 0.975)
      ))
    }
    print(round(intervals_mean, 4))
    
    # Calculate intervals for median-based top 5
    cat("\n=====================================")
    cat(sprintf("\n%s - Median-Based Top 5 Models (by sMAPE)\n", dep_var))
    cat("=====================================\n")
    
    intervals_median <- data.frame()
    for(window in top_5_median) {
      pred_data <- extended_test_sets_rolling[[dep_var]][[window]]
      last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
      pred <- predict.bsts(all_models_rw[[dep_var]][[window]], 
                          newdata = as.data.frame(last_row_predictors),
                          burn = 100)
      dist <- pred$distribution[,1]
      intervals_median <- rbind(intervals_median, data.frame(
        Window = window,
        SMAPE = smape_median[window],
        Lower_95 = quantile(dist, 0.025),
        Mean = mean(dist),
        Median = median(dist),
        Upper_95 = quantile(dist, 0.975)
      ))
    }
    print(round(intervals_median, 4))
  }
}

# Apply the function with rolling window splits
 get_predictive_intervals(
   all_evaluations_rwall, 
   all_models_rwall, 
   extended_test_sets_rolling
 )
```


## MCMC trace plot 
```{r}
## MCMC trace plot for Rolling Window Analysis
# Function to find top 2 best models based on evaluations (Rolling Window)
find_best_models_rolling <- function(all_evaluations) {
  best_models <- list()  # To store the best models for each dependent variable
  
  for(var_name in names(all_evaluations)) {
    cat("\nFor", var_name, ":\n")
    
    # Collect all sMAPE values
    all_smape_results <- data.frame(
      Window = integer(),
      Method = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Collect mean-based sMAPE values
    for(i in 1:4) {  
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "mean",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: mean_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Collect median-based sMAPE values
    for(i in 1:4) {  # Assuming 6 possible window sizes
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "median",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: median_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Sort by sMAPE (ascending) to find the best models overall
    all_smape_results <- all_smape_results[order(all_smape_results$SMAPE), ]
    
    if (nrow(all_smape_results) >= 2) {
      # Get top 2 models
      top_2_models <- all_smape_results[1:2, ]
      
      cat("Top 2 models for", var_name, "based on sMAPE:\n")
      for (i in 1:2) {
        cat("Rank", i, ": Window", top_2_models$Window[i], "using", top_2_models$Method[i], 
            "method with sMAPE =", round(top_2_models$SMAPE[i], 4), "\n")
      }
      
      # Store the top 2 models info
      best_models[[var_name]] <- list(
        best_window_1 = top_2_models$Window[1],
        best_method_1 = top_2_models$Method[1],
        best_smape_1 = top_2_models$SMAPE[1],
        best_window_2 = top_2_models$Window[2],
        best_method_2 = top_2_models$Method[2],
        best_smape_2 = top_2_models$SMAPE[2]
      )
    } else if (nrow(all_smape_results) == 1) {
      # Handle case where only one valid model is found
      cat("Only one valid model found for", var_name, ":\n")
      cat("Window:", all_smape_results$Window[1], "using", all_smape_results$Method[1], 
          "method with sMAPE =", round(all_smape_results$SMAPE[1], 4), "\n")
      
      best_models[[var_name]] <- list(
        best_window_1 = all_smape_results$Window[1],
        best_method_1 = all_smape_results$Method[1],
        best_smape_1 = all_smape_results$SMAPE[1]
      )
    } else {
      cat("No valid models found for", var_name, "\n")
    }
  }
  
  return(best_models)
}

# Analyze MCMC trace plots for Rolling Window
analyze_bsts_mcmc_trace_plots_rolling <- function(all_evaluations, all_models, best_models) {
  # Create a PDF device to save all plots
  pdf("MCMC_trace_plot_RW_ALL_2016Q4.pdf", width=12, height=10)
  
  for(dep_var in names(best_models)) {
    # Get the info for the top models
    best_model_info <- best_models[[dep_var]]
    
    # Process Rank 1 model
    window_1 <- best_model_info$best_window_1
    method_1 <- best_model_info$best_method_1
    smape_1 <- best_model_info$best_smape_1
    
    cat("\nCreating MCMC trace plots for", dep_var, ":\n")
    cat("Rank 1: Window:", window_1, "using", method_1, "method, sMAPE =", round(smape_1, 4), "\n")
    
    # Fetch the model
    model_1 <- all_models[[dep_var]][[window_1]]
    
    # Create trace plots for Rank 1 model
    create_trace_plots(model_1, dep_var, "Rank 1", window_1, method_1, smape_1)
    
    # Process Rank 2 model if available
    if(!is.null(best_model_info$best_window_2)) {
      window_2 <- best_model_info$best_window_2
      method_2 <- best_model_info$best_method_2
      smape_2 <- best_model_info$best_smape_2
      
      cat("Rank 2: Window:", window_2, "using", method_2, "method, sMAPE =", round(smape_2, 4), "\n")
      
      # Fetch the second-ranked model
      model_2 <- all_models[[dep_var]][[window_2]]
      
      # Create trace plots for Rank 2 model
      create_trace_plots(model_2, dep_var, "Rank 2", window_2, method_2, smape_2)
    } else {
      cat("No second-best model available for", dep_var, "\n")
    }
  }
  
  dev.off()
}

# Helper function to create trace plots for a model (unchanged from previous version)
create_trace_plots <- function(model, dep_var, rank_label, window, method, smape) {
  tryCatch({
    # Set up plot for state variances
    par(mfrow=c(3,1), mar=c(4,4,3,1))
    
    # Plot observation variance
    if (!is.null(model$sigma.obs)) {
      plot(1:length(model$sigma.obs), model$sigma.obs, type="l", col="blue",
           main="Observation Variance (sigma.obs)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot seasonal variance if it exists
    if (!is.null(model$sigma.seasonal.4)) {
      plot(1:length(model$sigma.seasonal.4), model$sigma.seasonal.4, type="l", col="red",
           main="Seasonal Variance (sigma.seasonal.4)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot trend level variance if it exists
    if (!is.null(model$sigma.trend.level)) {
      plot(1:length(model$sigma.trend.level), model$sigma.trend.level, type="l", col="green",
           main="Trend Level Variance (sigma.trend.level)",
           xlab="Iteration", ylab="Value")
    }
    
    mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Variances"), 
          outer=TRUE, line=-1.5, cex=1.2)
    
    # If the model has regression coefficients, plot those on a new page
    if (!is.null(model$coefficients) && !is.null(model$coefficients.samples) && 
        ncol(model$coefficients.samples) > 0) {
      
      # Calculate number of coefficient plots needed
      num_coefs <- ncol(model$coefficients.samples)
      rows_needed <- min(4, num_coefs)  # Maximum 4 coefficients per page
      
      # Start a new page
      par(mfrow=c(rows_needed, 1), mar=c(4,4,3,1))
      
      # Plot each coefficient
      for (i in 1:rows_needed) {
        coef_name <- colnames(model$coefficients.samples)[i]
        if (is.null(coef_name)) coef_name <- paste("Coefficient", i)
        
        coef_samples <- model$coefficients.samples[,i]
        plot(1:length(coef_samples), coef_samples, type="l", col="purple",
             main=paste("Regression Coefficient:", coef_name),
             xlab="Iteration", ylab="Value")
      }
      
      mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Coefficients"), 
            outer=TRUE, line=-1.5, cex=1.2)
    }
    
  }, error = function(e) {
    # If error occurs, print the error and try the built-in plotting function
    cat("Error in custom trace plotting for", rank_label, ":", e$message, "\n")
    cat("Falling back to built-in plotting...\n")
    
    # Reset the plot area
    par(mfrow=c(1,1))
    
    # Try the built-in plotting method
    tryCatch({
      # Try plotting state components instead
      plot(model, "components")
      title(main=paste0(dep_var, " (", rank_label, "): Component Contributions"))
    }, error = function(e2) {
      cat("Error in fallback plotting:", e2$message, "\n")
      
      # Create an empty plot with error message
      plot(1, 1, type="n", axes=FALSE, xlab="", ylab="")
      text(1, 1, paste("Error plotting MCMC traces for", rank_label), col="red", cex=1.5)
    })
  })
}

# Run the analysis for rolling window setting
best_models_rolling <- find_best_models_rolling(all_evaluations_rwall)
analyze_bsts_mcmc_trace_plots_rolling(all_evaluations_rwall, all_models_rwall, best_models_rolling)
```

## Distribution Plot
```{r}
create_prediction_plots <- function(all_evaluations, all_models, extended_test_sets_rolling, hold_out_dataset) {
  library(ggplot2)
  pdf("Prediction_Distribution_RW_ALL_2016Q4.pdf", width = 12, height = 8)
  
  for(dep_var in names(all_evaluations)) {
    # Create a data frame of all SMAPE values (mean and median) per window
    all_results <- data.frame(
      Window = numeric(),
      Type = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    for(i in 1:length(all_evaluations[[dep_var]])) {
      # Add mean results
      mean_smape <- all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE
      if(!is.na(mean_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "mean", SMAPE = mean_smape, stringsAsFactors = FALSE))
      }
      # Add median results
      median_smape <- all_evaluations[[dep_var]][[i]]$errors_median$SMAPE
      if(!is.na(median_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "median", SMAPE = median_smape, stringsAsFactors = FALSE))
      }
    }
    
    # Sort by SMAPE (ascending) and select the top 4 rows
    top_4 <- all_results[order(all_results$SMAPE), ][1:4, ]
    
    plot_data <- data.frame()
    label_list <- c()  # to track labels and catch duplicates
    
    for(i in 1:nrow(top_4)) {
      window <- top_4$Window[i]
      model_type <- top_4$Type[i]
      smape_value <- top_4$SMAPE[i]
      
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(extended_test_sets_rolling[[dep_var]][[window]][nrow(extended_test_sets_rolling[[dep_var]][[window]]), -1, drop = FALSE]),
                           burn = 100)
      
      # Create a label that includes rank, window, type, and SMAPE.
      base_label <- paste("Rank", i, ": Window", window, "-", model_type, "(SMAPE:", round(smape_value, 4), ")")
      # If the same label already exists, append a suffix.
      duplicate_count <- sum(label_list == base_label)
      if(duplicate_count > 0) {
        label <- paste0(base_label, " - Copy", duplicate_count + 1)
      } else {
        label <- base_label
      }
      label_list <- c(label_list, label)
      
      plot_data <- rbind(plot_data, 
                         data.frame(Value = pred$distribution[,1],
                                    Model = factor(label),
                                    stringsAsFactors = FALSE))
    }
    
    # Get the actual observed value from the hold_out_dataset (assume it's the last value in column 1)
    actual_value <- tail(hold_out_dataset[[dep_var]][, 1], 1)
    
    p <- ggplot(plot_data, aes(x = Value, fill = Model)) +
      geom_density(alpha = 0.4) +
      theme_minimal() +
      labs(title = paste(dep_var, "- Top 4 Models Based on sMAPE"),
           x = "Predicted Value",
           y = "Density") +
      geom_vline(xintercept = actual_value, color = "red", linetype = "dashed", size = 1) +
      annotate("text", x = actual_value, y = Inf, label = paste("Actual:", actual_value),
               vjust = -0.5, color = "red", size = 3)
    
    print(p)
  }
  
  dev.off()
}


# Apply the function
create_prediction_plots(all_evaluations_rwall, all_models_rwall, extended_test_sets_rolling , hold_out_dataset)
```


## Continuous Ranked Probability Score (CRPS)
```{r}
# Function to calculate CRPS
calculate_crps <- function(actual, pred_dist) {
  n <- length(pred_dist)
  sorted_pred <- sort(pred_dist)
  H <- function(x) ifelse(x >= 0, 1, 0)
  
  integral <- 0
  for(i in 1:(n-1)) {
    x <- sorted_pred[i]
    dx <- sorted_pred[i+1] - x
    F_x <- i/n
    integral <- integral + (F_x - H(x - actual))^2 * dx
  }
  return(integral)
}

# Function to get test indices based on window number
get_test_indices <- function(window_num) {
  total_rows <- 111  # Total number of rows in the dataset
  train_size <- 90   # Fixed training window size
  fixed_test_start <- 91  # Fixed test portion starts at row 71
  
  # Calculate training start and end based on window number
  train_start <- 1 + (window_num - 1) * 10
  train_end <- train_start + train_size - 1
  
  # Test indices consist of:
  # 1. Any rows before the training window (if training doesn't start at row 1)
  # 2. The fixed test portion (rows 71-111)
  
  # Part 1: Rows before training window (if any)
  before_train_indices <- if(train_start > 1) 1:(train_start-1) else integer(0)
  
  # Part 2: Fixed test portion (rows 71-111)
  fixed_test_indices <- fixed_test_start:total_rows
  
  # Combine both parts for complete test indices
  test_indices <- c(before_train_indices, fixed_test_indices)
  
  # Remove any indices that would overlap with training window
  test_indices <- test_indices[!test_indices %in% train_start:train_end]
  
  return(sort(test_indices))
}

# Helper function to summarize indices (for display purposes)
summarize_indices <- function(indices) {
  if(length(indices) == 0) return(character(0))
  
  gaps <- diff(indices) > 1
  group_starts <- c(TRUE, gaps)
  group_ends <- c(gaps, TRUE)
  
  sequences <- character(0)
  start_idx <- 1
  
  for(i in 1:length(indices)) {
    if(group_starts[i]) start_idx <- i
    if(group_ends[i]) {
      end_idx <- i
      if(start_idx == end_idx) {
        sequences <- c(sequences, as.character(indices[start_idx]))
      } else {
        sequences <- c(sequences, paste(indices[start_idx], "-", indices[end_idx], sep=""))
      }
    }
  }
  
  return(sequences)
}

# Improved CRPS calculation function with special handling for Window 5
get_crps_scores_custom_rolling <- function(all_models, full_data) {
  crps_results <- list()
  
  for(dep_var in names(all_models)) {
    cat("\n\nCalculating CRPS for dependent variable:", dep_var)
    window_crps <- data.frame()
    
    # Fixed number of windows
    num_windows <- length(all_models[[dep_var]])
    
    for(i in 1:num_windows) {
      cat("\n\nProcessing window", i, "for", dep_var)
      
      # Get the model
      model <- all_models[[dep_var]][[i]]
      
      # Skip if model is missing
      if(is.null(model)) {
        cat("\nWarning: No model available for window", i, "\n")
        next
      }
      
      # Get test indices for this window
      test_indices <- get_test_indices(i)
      
      # Check if any test indices are out of bounds
      if(max(test_indices) > nrow(full_data[[dep_var]])) {
        cat("\nWarning: Some test indices exceed data dimensions for window", i)
        # Filter out-of-bounds indices
        test_indices <- test_indices[test_indices <= nrow(full_data[[dep_var]])]
        cat("\nAdjusted test indices:", paste(summarize_indices(test_indices), collapse=", "))
      }
      
      cat("\nTest indices for window", i, ":", 
          paste(summarize_indices(test_indices), collapse=", "), 
          "(", length(test_indices), "rows)")
      
      # Get test data
      test_data <- full_data[[dep_var]][test_indices, ]
      
      # Skip if no test data
      if(is.null(test_data) || nrow(test_data) == 0) {
        cat("\nError: No test data available for window", i, "\n")
        next
      }
      
      cat("\nSuccessfully obtained test data. Dimensions:", nrow(test_data), "x", ncol(test_data))
      
      # Get actual values and predictors
      y_test <- test_data[, 1]
      X_test <- test_data[, -1, drop = FALSE]
      
      # Check for NA values
      na_rows <- which(apply(X_test, 1, function(x) any(is.na(x))))
      if(length(na_rows) > 0) {
        cat("\nWarning:", length(na_rows), "rows with NA values detected")
        cat("\nRemoving rows with NA values from test set")
        X_test <- X_test[-na_rows, , drop = FALSE]
        y_test <- y_test[-na_rows]
        test_indices <- test_indices[-na_rows]
      }
      
      # Skip if no valid data left
      if(length(y_test) == 0) {
        cat("\nError: No valid test data after removing NA values for window", i, "\n")
        next
      }
      
      # Special handling for Window 5 if needed
      if(i == 5) {
        cat("\nSpecial handling for Window 5 - ensuring compatibility")
        # You can add specific adjustments for Window 5 here if needed
        # For example, checking column names match, dimensions are as expected, etc.
      }
      
      # Make predictions
      tryCatch({
        cat("\nMaking predictions with", nrow(X_test), "observations...")
        
        # Handle potential errors in prediction
        pred <- tryCatch({
          predict(model, newdata = as.data.frame(X_test), burn = 100)
        }, error = function(e) {
          cat("\nError in prediction:", e$message)
          cat("\nAttempting prediction with imputed values...")
          
          # Try imputing missing values
          X_test_imputed <- X_test
          for(col in 1:ncol(X_test_imputed)) {
            if(any(is.na(X_test_imputed[, col]))) {
              X_test_imputed[is.na(X_test_imputed[, col]), col] <- mean(X_test_imputed[, col], na.rm = TRUE)
            }
          }
          
          # Retry prediction with imputed values
          tryCatch({
            predict(model, newdata = as.data.frame(X_test_imputed), burn = 100)
          }, error = function(e2) {
            cat("\nSecond attempt failed:", e2$message)
            return(NULL)
          })
        })
        
        # Skip if prediction failed
        if(is.null(pred)) {
          cat("\nPrediction failed for window", i, ", skipping CRPS calculation")
          next
        }
        
        # Check prediction dimensions
        cat("\nPrediction distribution dimensions:", nrow(pred$distribution), "x", ncol(pred$distribution))
        cat("\nActual values length:", length(y_test))
        
        # Ensure alignment between predictions and actual values
        if(ncol(pred$distribution) != length(y_test)) {
          cat("\nWarning: Mismatch between prediction columns (", ncol(pred$distribution), 
              ") and actual values length (", length(y_test), ")")
          cat("\nTruncating to match the shorter length.")
          min_length <- min(ncol(pred$distribution), length(y_test))
          pred$distribution <- pred$distribution[, 1:min_length, drop = FALSE]
          y_test <- y_test[1:min_length]
        }
        
        # Calculate CRPS for each observation
        cat("\nCalculating CRPS values...")
        crps_values <- numeric(length(y_test))
        
        for(j in 1:length(y_test)) {
          tryCatch({
            crps_values[j] <- calculate_crps(y_test[j], pred$distribution[, j])
          }, error = function(e) {
            cat("\nError calculating CRPS for observation", j, ":", e$message)
            crps_values[j] <- NA
          })
        }
        
        # Check for valid CRPS values
        valid_crps <- sum(!is.na(crps_values))
        if(valid_crps == 0) {
          cat("\nNo valid CRPS values calculated for window", i)
          next
        }
        
        # Add results to data frame
        window_crps <- rbind(window_crps, data.frame(
          Window = i,
          CRPS_Mean = mean(crps_values, na.rm = TRUE),
          CRPS_SD = sd(crps_values, na.rm = TRUE),
          Valid_Observations = valid_crps
        ))
        
        cat("\nCRPS calculation completed for window", i)
        cat("\nMean CRPS:", mean(crps_values, na.rm = TRUE))
        
      }, error = function(e) {
        cat("\nUnhandled error calculating CRPS for window", i, ":", e$message, "\n")
      })
    }
    
    # Check if we have any results
    if(nrow(window_crps) == 0) {
      cat("\nNo CRPS results available for", dep_var)
      crps_results[[dep_var]] <- data.frame(
        Window = integer(0),
        CRPS_Mean = numeric(0),
        CRPS_SD = numeric(0),
        Valid_Observations = integer(0)
      )
    } else {
      crps_results[[dep_var]] <- window_crps
    }
  }
  
  # Print results
  cat("\n\n=== CRPS Results Summary ===\n")
  for(dep_var in names(crps_results)) {
    cat("\n", dep_var, ":\n")
    if(nrow(crps_results[[dep_var]]) > 0) {
      print(crps_results[[dep_var]])
    } else {
      cat("No valid CRPS results available\n")
    }
  }
  
  return(crps_results)
}

# Example usage:
 crps_scores_custom_rolling <- get_crps_scores_custom_rolling(all_models_rwall, Indice_testing_dataset)
```

## log predictive density
```{r}
calculate_lpd <- function(all_models, rolling_splits) {
 lpd_results <- list()
 
 for(dep_var in names(all_models)) {
   window_lpd <- data.frame()
   
   for(i in 1:4) {
     model <- all_models[[dep_var]][[i]]
     test_data <- rolling_splits[[dep_var]][[i]]$test
     actual <- test_data[, 1]
     
     # Get predictions
     pred <- predict.bsts(model, newdata = as.data.frame(test_data[, -1]), burn = 100)
     
     # Calculate LPD for each observation
     lpd_values <- numeric(length(actual))
     for(j in seq_along(actual)) {
       # Get density estimate of prediction distribution
       density_est <- density(pred$distribution[,j])
       # Find density at actual value
       actual_density <- approx(density_est$x, density_est$y, xout = actual[j])$y
       # Log density
       lpd_values[j] <- log(actual_density)
     }
     
     window_lpd <- rbind(window_lpd, data.frame(
       Window = i,
       LPD_Mean = mean(lpd_values, na.rm = TRUE),
       LPD_SD = sd(lpd_values, na.rm = TRUE)
     ))
   }
   lpd_results[[dep_var]] <- window_lpd
 }
 
 # Print results
 for(dep_var in names(lpd_results)) {
   cat("\n=====================================")
   cat(sprintf("\n%s - Log Predictive Density\n", dep_var))
   cat("=====================================\n")
   print(round(lpd_results[[dep_var]], 4))
 }
 
 return(lpd_results)
}

# Apply function
lpd_scores <- calculate_lpd(all_models_rwall, rolling_splits)
```

## Sensitivity 
```{r}
# First, clean up any existing parallel connections
try(stopImplicitCluster(), silent = TRUE)
try(stopCluster(cl), silent = TRUE)
closeAllConnections()
gc()  # Force garbage collection

# Load required packages
library(bsts)
library(parallel)

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to identify the best models
get_best_models_safe <- function() {
  best_predictions <- list()
  
  for (dep_var in names(all_models_rwall)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:4) {
      if (!dep_var %in% names(all_models_rwall) || length(all_models_rwall[[dep_var]]) < i) {
        next
      }
      
      tryCatch({
        model <- all_models_rwall[[dep_var]][[i]]
        
        # Get the testing data for this window and variable
        test_data <- rolling_splits[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        mape_mean <- mean(abs((actual_values - pred_mean) / actual_values)) * 100
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                          (abs(actual_values) + abs(pred_mean))) * 100
        
        mape_median <- mean(abs((actual_values - pred_median) / actual_values)) * 100
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                            (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to results
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = mape_mean, SMAPE = smape_mean),
                       data.frame(Window = i, Method = "median", 
                                MAPE = mape_median, SMAPE = smape_median))
      }, error = function(e) {
        cat("Error processing", dep_var, "window", i, ":", e$message, "\n")
      })
    }
    
    # Select the best five models
    if (nrow(results) > 0) {
      best_five <- results[order(results$MAPE), ][1:min(5, nrow(results)), ]
      best_predictions[[dep_var]] <- best_five
    }
  }
  
  return(best_predictions)
}

# Function to analyze one configuration
analyze_config <- function(params) {
  dep_var <- params$dep_var
  window_size <- params$window_size
  method <- params$method
  sigma <- params$sigma
  slab_var <- params$slab_var
  
  cat("Processing", dep_var, "window", window_size, "method", method, 
      "sigma", sigma, "slab_var", slab_var, "\n")
  
  # Get training and testing data
  train_data <- rolling_splits[[dep_var]][[window_size]]$train
  test_data <- rolling_splits[[dep_var]][[window_size]]$test
  
  # Extract values
  y <- train_data[, 1]
  X <- as.data.frame(train_data[, -1])
  actual_values <- test_data[, 1]
  test_predictors <- as.data.frame(test_data[, -1])
  
  # Create formula
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Full data for model
  model_data <- as.data.frame(cbind(y = y, X))
  
  # Create state specification
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y, 
                           level.sigma.prior = SdPrior(sigma = sigma),
                           slope.sigma.prior = SdPrior(sigma = sigma))
  
  # Fit model
  result <- tryCatch({
    # Using the approach that worked in our test
    model <- bsts(formula, 
                 state.specification = ss,
                 niter = 10000,
                 data = model_data)
    
    # Make predictions
    all_predictions <- numeric(length(actual_values))
    
    for (t in 1:length(actual_values)) {
      # Prepare data for this period
      pred_data <- data.frame(y = NA)
      pred_data <- cbind(pred_data, test_predictors[t, , drop = FALSE])
      
      # Predict
      single_pred <- predict(model, newdata = pred_data, burn = 100)
      
      # Extract prediction
      if (method == "mean") {
        all_predictions[t] <- mean(single_pred$distribution[, 1])
      } else {
        all_predictions[t] <- median(single_pred$distribution[, 1])
      }
    }
    
    # Check if predictions vary
    has_varying_predictions <- length(unique(round(all_predictions, 2))) > 1
    
    # Calculate metrics
    mape <- mean(abs((actual_values - all_predictions) / actual_values)) * 100
    smape <- mean(2 * abs(actual_values - all_predictions) / 
                 (abs(actual_values) + abs(all_predictions))) * 100
    
    waic <- NA
    looic <- NA
    if (!is.null(model$log.likelihood)) {
      waic <- -2 * mean(model$log.likelihood)
      looic <- -2 * mean(model$log.likelihood)
    }
    
    return(list(
      success = TRUE,
      sigma = sigma,
      slab_var = slab_var,
      mape = mape,
      smape = smape,
      waic = waic,
      looic = looic,
      has_regression = has_varying_predictions
    ))
    
  }, error = function(e) {
    return(list(
      success = FALSE,
      error = e$message
    ))
  })
  
  return(result)
}

# Controlled parallel approach
run_sensitivity_analysis_parallel <- function() {
  # Define parameter combinations
  sigmas <- c(0.2, 0.4, 0.6, 0.8)
  slab_vars <- c(50, 100, 200)
  
  # Get best models
  best_models <- get_best_models_safe()
  
  # Initialize results
  sensitivity_results <- list()
  
  # For each dependent variable
  for (dep_var in names(best_models)) {
    cat("\n=== Starting analysis for", dep_var, "===\n")
    sensitivity_results[[dep_var]] <- list()
    models_info <- best_models[[dep_var]]
    
    # For each top model
    for (i in 1:min(5, nrow(models_info))) {
      model_info <- models_info[i, ]
      window_size <- model_info$Window
      method <- as.character(model_info$Method)
      model_name <- paste0("model_", i)
      
      cat("\n--- Processing", dep_var, "window", window_size, "method", method, "---\n")
      
      # Initialize results for this model
      model_results <- list()
      
      # Create parameter combinations
      param_list <- list()
      for (sigma in sigmas) {
        for (slab_var in slab_vars) {
          param_list[[length(param_list) + 1]] <- list(
            dep_var = dep_var,
            window_size = window_size,
            method = method,
            sigma = sigma,
            slab_var = slab_var
          )
        }
      }
      
      # Create a small cluster - use just 2 or 3 cores for stability
      num_cores <- min(3, detectCores() - 1)
      cat("Using", num_cores, "cores for parallel processing\n")
      cl <- makeCluster(num_cores)
      
      # Export required data and functions
      clusterExport(cl, c("rolling_splits"), envir = .GlobalEnv)
      clusterEvalQ(cl, {
        library(bsts)
      })
      
      # Run analysis in parallel
      config_results <- parLapply(cl, param_list, analyze_config)
      
      # Clean up
      stopCluster(cl)
      
      # Process results
      for (j in 1:length(param_list)) {
        params <- param_list[[j]]
        result <- config_results[[j]]
        
        config_name <- paste0("sigma_", params$sigma, "_slab_", params$slab_var)
        
        if (result$success) {
          # Store successful result
          model_results[[config_name]] <- list(
            sigma = params$sigma,
            slab_var = params$slab_var,
            mape = result$mape,
            smape = result$smape,
            waic = result$waic,
            looic = result$looic,
            has_regression = result$has_regression
          )
          
          cat("Configuration", config_name, "- MAPE:", round(result$mape, 4),
             "SMAPE:", round(result$smape, 4), "Has regression:", result$has_regression, "\n")
        } else {
          cat("Configuration", config_name, "failed:", result$error, "\n")
        }
      }
      
      # Store results for this model
      sensitivity_results[[dep_var]][[model_name]] <- list(
        window = window_size,
        method = method,
        results = model_results
      )
      
      # Find best configuration
      best_config <- NULL
      best_mape <- Inf
      best_config_name <- ""
      
      for (config_name in names(model_results)) {
        config <- model_results[[config_name]]
        if (!is.na(config$mape) && config$mape < best_mape) {
          best_mape <- config$mape
          best_config <- config
          best_config_name <- config_name
        }
      }
      
      if (is.null(best_config)) {
        cat("No valid configurations found.\n")
      } else {
        # Print best configuration
        cat("\nBest configuration:", best_config_name, "\n")
        cat("MAPE:", round(best_config$mape, 4), "\n")
        cat("SMAPE:", round(best_config$smape, 4), "\n")
        cat("Sigma:", best_config$sigma, "\n")
        cat("Slab variance:", best_config$slab_var, "\n")
        cat("Has regression:", best_config$has_regression, "\n\n")
        
        # Create summary table
        config_summary <- data.frame(
          Sigma = numeric(),
          SlabVar = numeric(),
          MAPE = numeric(),
          SMAPE = numeric(),
          WAIC = numeric(),
          LOOIC = numeric(),
          HasRegression = logical()
        )
        
        for (config_name in names(model_results)) {
          config <- model_results[[config_name]]
          config_summary <- rbind(config_summary, 
                                data.frame(
                                  Sigma = config$sigma,
                                  SlabVar = config$slab_var,
                                  MAPE = config$mape,
                                  SMAPE = config$smape,
                                  WAIC = config$waic,
                                  LOOIC = config$looic,
                                  HasRegression = config$has_regression
                                ))
        }
        
        # Set row names and sort
        rownames(config_summary) <- names(model_results)
        config_summary <- config_summary[order(config_summary$MAPE), ]
        print(round(config_summary, 4))
      }
      
      # Clean up after each model
      rm(model_results, config_results)
      gc()
    }
  }
  
  return(sensitivity_results)
}

# Run the analysis
start_time <- Sys.time()
cat("Starting sensitivity analysis at:", format(start_time), "\n")

sensitivity_results <- run_sensitivity_analysis_parallel()

end_time <- Sys.time()
execution_time <- end_time - start_time
cat("\nSensitivity analysis completed in:", format(execution_time), "\n")
```

```{r}
# Function to identify the top 5 models by SMAPE
find_top_models <- function(all_models_rwall, sensitivity_results) {
  top_models_summary <- list()
  
  # Process each dependent variable
  for(dep_var in names(sensitivity_results)) {
    cat("\n================================================================\n")
    cat("TOP 5 MODELS FOR", toupper(dep_var), "BASED ON SMAPE\n")
    cat("================================================================\n")
    
    # 1. Collect all model results including window performance and sensitivity analysis
    all_models <- data.frame(
      Source = character(),
      Window = integer(),
      Method = character(),
      Sigma = numeric(),
      SlabVar = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Add window performance results (from best_models)
    for (i in 1:4) { # Assuming 6 windows
      if (dep_var %in% names(all_models_rwall) && length(all_models_rwall[[dep_var]]) >= i) {
        # Get the testing data for this window and variable
        test_data <- rolling_splits[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(all_models_rwall[[dep_var]][[i]], newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate SMAPE for mean
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                         (abs(actual_values) + abs(pred_mean))) * 100
        
        # Calculate SMAPE for median
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                           (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to all_models dataframe
        all_models <- rbind(all_models, 
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "mean",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_mean,
                            stringsAsFactors = FALSE
                          ),
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "median",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_median,
                            stringsAsFactors = FALSE
                          ))
      }
    }
    
    # Add sensitivity analysis results
    if (dep_var %in% names(sensitivity_results)) {
      for (model_name in names(sensitivity_results[[dep_var]])) {
        model_data <- sensitivity_results[[dep_var]][[model_name]]
        window <- model_data$window
        method <- model_data$method
        
        for (config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_models <- rbind(all_models,
                            data.frame(
                              Source = "Sensitivity",
                              Window = window,
                              Method = method,
                              Sigma = config$sigma,
                              SlabVar = config$slab_var,
                              SMAPE = config$smape,
                              stringsAsFactors = FALSE
                            ))
        }
      }
    }
    
    # 2. Sort by SMAPE and select top 5 models
    top_models <- all_models[order(all_models$SMAPE), ][1:min(5, nrow(all_models)), ]
    
    # 3. Create a formatted output
    formatted_models <- data.frame(
      Rank = 1:nrow(top_models),
      Description = character(nrow(top_models)),
      SMAPE = top_models$SMAPE,
      stringsAsFactors = FALSE
    )
    
    for (i in 1:nrow(top_models)) {
      model <- top_models[i, ]
      
      if (model$Source == "Window") {
        desc <- sprintf("%s Window %d", model$Method, model$Window)
      } else {
        desc <- sprintf("%s Window %d (sigma=%.1f, slab=%.0f)", 
                      model$Method, model$Window, model$Sigma, model$SlabVar)
      }
      
      formatted_models$Description[i] <- desc
    }
    
    # Display the top models
    print(formatted_models)
    
    # Store the results
    top_models_summary[[dep_var]] <- list(
      top_models = top_models,
      formatted_output = formatted_models
    )
  }
  
  return(top_models_summary)
}

# Example usage:
 top_model_results <- find_top_models(all_models_rwall, sensitivity_results)
```

```{r}
# Function to predict holdout period using top 5 models and create weighted ensemble
predict_and_create_ensemble <- function(all_models_rwall, top_model_results, hold_out_dataset) {
  # Clean up any existing connections
  try(stopImplicitCluster(), silent = TRUE)
  try(stopCluster(cl), silent = TRUE)
  closeAllConnections()
  gc()  # Force garbage collection
  
  # Load required packages
  library(parallel)
  
  # To store all results
  all_predictions <- list()
  
  # For each dependent variable
  for(dep_var in names(top_model_results)) {
    cat("\n================================================================\n")
    cat("HOLDOUT PREDICTIONS FOR", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Get top models for this variable
    top_models <- top_model_results[[dep_var]]$top_models
    
    # Get holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      cat("Holdout data not available for", dep_var, "\n")
      next
    }
    
    holdout_data <- hold_out_dataset[[dep_var]]
    
    # Prepare parameters for parallel execution
    param_list <- list()
    for(i in 1:nrow(top_models)) {
      param_list[[i]] <- list(
        dep_var = dep_var,
        model_info = top_models[i, ],
        model_index = i
      )
    }
    
    # Create a small cluster - use just 2 or 3 cores for stability
    num_cores <- min(3, detectCores() - 1)
    cat("Using", num_cores, "cores for parallel processing\n")
    cl <- makeCluster(num_cores)
    
    # Export required data and functions
    clusterExport(cl, c("all_models_rwall", "hold_out_dataset", "rolling_splits"), envir = .GlobalEnv)
    clusterEvalQ(cl, {
      library(bsts)
    })
    
    # Worker function for parallel execution
    predict_model_worker <- function(params) {
      dep_var <- params$dep_var
      model_info <- params$model_info
      model_index <- params$model_index
      
      # Create result container
      result <- list(
        model_index = model_index,
        dep_var = dep_var,
        success = FALSE
      )
      
      # Get the holdout data
      holdout_data <- hold_out_dataset[[dep_var]]
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Create period labels
      period_labels <- c(as.character(80:86), "Q4")#period
      
      # Create model description
      if(model_info$Source == "Window") {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, ")")
      } else {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, 
                          ", sigma=", model_info$Sigma, 
                          ", slab=", model_info$SlabVar, ")")
      }
      
      tryCatch({
        # Get or create model based on source
        if(model_info$Source == "Window") {
          # Use existing model
          window <- model_info$Window
          method <- as.character(model_info$Method)
          
          if(!(dep_var %in% names(all_models_rwall)) || length(all_models_rwall[[dep_var]]) < window) {
            return(c(result, list(error = "Model not available")))
          }
          
          model <- all_models_rwall[[dep_var]][[window]]
          
        } else {
          # Create model with sensitivity parameters
          window <- model_info$Window
          method <- as.character(model_info$Method)
          sigma <- model_info$Sigma
          slab_var <- model_info$SlabVar
          
          # Get original model
          if(!(dep_var %in% names(all_models_rwall)) || length(all_models_rwall[[dep_var]]) < window) {
            return(c(result, list(error = "Original model not available")))
          }
          
          original_model <- all_models_rwall[[dep_var]][[window]]
          
          # Get training data
          train_data <- rolling_splits[[dep_var]][[window]]$train
          y <- train_data[, 1]
          X <- as.data.frame(train_data[, -1])
          
          # Create formula
          predictors <- colnames(X)
          formula_str <- paste("y ~", paste(predictors, collapse = " + "))
          formula <- as.formula(formula_str)
          
          # Create model data
          model_data <- as.data.frame(cbind(y = y, X))
          
          # Set up state specification
          ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
          ss <- AddLocalLinearTrend(ss, y, 
                                  level.sigma.prior = SdPrior(sigma = sigma),
                                  slope.sigma.prior = SdPrior(sigma = sigma))
          
          # Create model
          model <- bsts(formula, 
                       state.specification = ss,
                       niter = 10000,
                       data = model_data)
        }
        
        # Make predictions - using individual predictions for each period
        predictions <- numeric(nrow(holdout_data))
        
        for(t in 1:nrow(holdout_data)) {
          # Create data for this period
          if(model_info$Source == "Window") {
            # For window models
            this_period_data <- pred_data[t, , drop = FALSE]
            
            # Make prediction
            single_pred <- predict(model, newdata = this_period_data, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          } else {
            # For sensitivity models
            pred_row <- data.frame(y = NA)
            pred_row <- cbind(pred_row, pred_data[t, , drop = FALSE])
            
            # Make prediction
            single_pred <- predict(model, newdata = pred_row, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          }
        }
        
        # Calculate errors
        abs_errors <- abs(predictions - actual_values)
        avg_abs_error <- mean(abs_errors)
        
        # Return successful result
        return(list(
          model_index = model_index,
          dep_var = dep_var,
          success = TRUE,
          model_desc = model_desc,
          predictions = predictions,
          actual_values = actual_values,
          abs_errors = abs_errors,
          avg_abs_error = avg_abs_error,
          period_labels = period_labels,
          smape = model_info$SMAPE
        ))
        
      }, error = function(e) {
        return(c(result, list(error = e$message)))
      })
    }
    
    # Run predictions in parallel
    model_results <- parLapply(cl, param_list, predict_model_worker)
    
    # Stop cluster
    stopCluster(cl)
    
    # Process and display results
    var_predictions <- list()
    
    for(result in model_results) {
      if(result$success) {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, ":", result$model_desc, "\n")
        cat("----------------------------------------------------------------\n")
        
        # Print results in a tabular format
        cat("\nPeriod\tActual\t\tPredicted\tAbsError\n")
        for(p in 1:length(result$period_labels)) {
          cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                    result$period_labels[p], 
                    result$actual_values[p], 
                    result$predictions[p], 
                    result$abs_errors[p]))
        }
        
        cat("\nAverage Absolute Error:", round(result$avg_abs_error, 2), "\n")
        
        # Store successful predictions
        var_predictions[[result$model_index]] <- result
      } else {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, "failed:", result$error, "\n")
        cat("----------------------------------------------------------------\n")
      }
    }
    
    # Store predictions for this variable
    all_predictions[[dep_var]] <- var_predictions
    
    # Now create weighted ensemble
    successful_models <- model_results[sapply(model_results, function(r) r$success)]
    
    if(length(successful_models) > 0) {
      # Get the actual values and period labels from the first successful model
      actual_values <- successful_models[[1]]$actual_values
      period_labels <- successful_models[[1]]$period_labels
      
      # Create prediction matrix
      pred_matrix <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      smape_values <- numeric(length(successful_models))
      model_descriptions <- character(length(successful_models))
      
      # Fill the matrix with predictions
      for(i in 1:length(successful_models)) {
        pred_matrix[, i] <- successful_models[[i]]$predictions
        smape_values[i] <- successful_models[[i]]$smape
        model_descriptions[i] <- successful_models[[i]]$model_desc
      }
      
      # Calculate weights based on inverse SMAPE
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      
      # Display the weights
      cat("\n================================================================\n")
      cat("WEIGHTED ENSEMBLE FOR", toupper(dep_var), "\n")
      cat("================================================================\n")
      
      cat("\nModel Weights:\n")
      for(i in 1:length(weights)) {
        cat(sprintf("Model %d: %s - Weight: %.4f (SMAPE: %.4f)\n", 
                  i, model_descriptions[i], weights[i], smape_values[i]))
      }
      
      # Calculate weighted predictions
      weighted_predictions <- numeric(length(actual_values))
      for(i in 1:length(actual_values)) {
        weighted_predictions[i] <- sum(pred_matrix[i, ] * weights, na.rm = TRUE)
      }
      
      # Calculate errors
      abs_errors <- abs(weighted_predictions - actual_values)
      
      # Calculate ensemble SMAPE
      ensemble_smape <- mean(2 * abs(actual_values - weighted_predictions) / 
                           (abs(actual_values) + abs(weighted_predictions))) * 100
      
      # Display results
      cat("\n----------------------------------------------------------------\n")
      cat("WEIGHTED ENSEMBLE RESULTS\n")
      cat("----------------------------------------------------------------\n")
      cat("Ensemble SMAPE:", round(ensemble_smape, 4), "%\n\n")
      
      cat("Period\tActual\t\tPredicted\tAbsError\n")
      for(p in 1:length(period_labels)) {
        cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                  period_labels[p], 
                  actual_values[p], 
                  weighted_predictions[p], 
                  abs_errors[p]))
      }
      
      cat("\nAverage Absolute Error:", round(mean(abs_errors), 2), "\n")
    } else {
      cat("\nNo successful models for", dep_var, "- cannot create ensemble\n")
    }
  }
  
  # Return all predictions
  return(all_predictions)
}

# Run the function to predict and create weighted ensemble
all_results <- predict_and_create_ensemble(all_models_rwall, top_model_results, hold_out_dataset)
```

```{r}
# This function replicates your 'print_ensemble_tables' logic but returns a named vector
# of Weighted Ensemble SMAPEs (one entry per dep_var).
get_ensemble_smapes <- function(all_results, top_model_results, hold_out_dataset) {
  ensemble_smapes <- numeric(0)  # named vector
  
  for(dep_var in names(top_model_results)) {
    # Skip if no holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      next
    }
    holdout_data  <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    
    # Filter out NULL models
    successful_models <- all_results[[dep_var]]
    successful_models <- successful_models[!sapply(successful_models, is.null)]
    
    if(length(successful_models) > 0) {
      smape_values <- numeric(length(successful_models))
      pred_matrix  <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      
      for(i in seq_along(successful_models)) {
        pred_matrix[, i]  <- successful_models[[i]]$predictions
        smape_values[i]   <- successful_models[[i]]$smape
      }
      
      # Weighted predictions
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      weighted_predictions <- rowSums(t(t(pred_matrix) * weights), na.rm = TRUE)
      
      # Weighted Ensemble SMAPE
      ensemble_smape <- mean(
        2 * abs(actual_values - weighted_predictions) /
        (abs(actual_values) + abs(weighted_predictions))
      ) * 100
      
      # Store in named vector
      ensemble_smapes[dep_var] <- ensemble_smape
    }
  }
  
  return(ensemble_smapes)
}
ensemble_smapes = get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
```

## Influence 
```{r}
if (!requireNamespace("foreach", quietly = TRUE)) {
  install.packages("foreach")
}
if (!requireNamespace("doParallel", quietly = TRUE)) {
  install.packages("doParallel")
}

# Setup parallel processing
library(parallel)
library(foreach)
library(doParallel)
library(bsts)

# Register parallel backend
cores <- detectCores() - 1  # Leave one core free for system processes
registerDoParallel(cores)
cat("Using", cores, "cores for parallel processing\n")

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to get window data
get_window_data <- function(dep_var, window_size) {
  window_data <- rolling_splits[[dep_var]][[window_size]]$train
  return(window_data)
}

# Function to get the best models based on testing dataset performance (changed from holdout)
get_best_models_test <- function(all_models) {
  best_predictions <- list()
  
  for (dep_var in names(all_models)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:length(all_models[[dep_var]])) {
      model <- all_models[[dep_var]][[i]]
      
      # Get the testing data for this window and variable
      test_data <- rolling_splits[[dep_var]][[i]]$test
      actual_values <- test_data[, 1]
      test_predictors <- as.data.frame(test_data[, -1])
      
      # Make predictions for testing period
      tryCatch({
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        errors_mean <- calculate_errors(actual_values, pred_mean)
        errors_median <- calculate_errors(actual_values, pred_median)
        
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = errors_mean$MAPE, SMAPE = errors_mean$SMAPE),
                       data.frame(Window = i, Method = "median", 
                                MAPE = errors_median$MAPE, SMAPE = errors_median$SMAPE))
      }, error = function(e) {
        cat("Error predicting for", dep_var, "window", i, ":", conditionMessage(e), "\n")
      })
    }
    
    # If we have results, select the best two models based on MAPE
    if (nrow(results) > 0) {
      best_two <- results[order(results$MAPE), ][1:min(2, nrow(results)), ]
      best_predictions[[dep_var]] <- best_two
    } else {
      cat("No valid predictions for", dep_var, "- skipping\n")
    }
  }
  
  # Print best models
  cat("\n=== Best Models for Testing Period ===\n")
  for(dep_var in names(best_predictions)) {
    cat("\n", dep_var, ":\n")
    print(best_predictions[[dep_var]])
  }
  
  return(best_predictions)
}

# FIXED Function to calculate influence measures with LOOIC and LPD
calculate_influence_test <- function(model, train_data, method, dep_var, window_size) {
  # Get original training data
  y <- train_data[, 1]
  X <- train_data[, -1]
  n_obs <- length(y)
  
  # Create model matrix for X
  X_matrix <- model.matrix(~ ., data = as.data.frame(X))[, -1]
  
  # Create time labels starting from 1997 Q4
  start_year <- 1997
  time_labels <- paste0(rep(start_year:(start_year + floor(n_obs/4)), each=4)[1:n_obs], 
                       " Q", rep(1:4, length.out=n_obs))
  
  # Results storage
  results <- data.frame(
    time = time_labels,
    pointwise_lpd = numeric(n_obs),
    delta_looic = numeric(n_obs),
    delta_test_forecast = numeric(n_obs)
  )
  
  # Get test data - FIXED to use passed parameters
  test_data <- rolling_splits[[dep_var]][[window_size]]$test
  test_y <- test_data[, 1]
  test_X <- as.data.frame(test_data[, -1])
  
  # Make original predictions for test period
  original_pred <- predict(model, newdata = test_X, burn = 100)
  original_point_preds <- if(method == "median") {
    apply(original_pred$distribution, 2, median)
  } else {
    colMeans(original_pred$distribution)
  }
  
  # Calculate pointwise log predictive density 
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)  # Using number of samples from log-likelihood
  
  # Ensure we have enough samples (safety check)
  if (n_samples == 0) {
    stop("No log-likelihood values available.")
  }

  log_lik_matrix <- matrix(log_lik, nrow = n_samples, ncol = n_obs, byrow = FALSE)
  
  results$pointwise_lpd <- colMeans(log_lik_matrix, na.rm = TRUE)  # Average LPD
  
  # LOOIC Calculation and test prediction changes - Parallelized version
  looic_values <- rep(NA, n_obs)
  delta_forecasts <- rep(NA, n_obs)
  
  # Parallel loop over observations
  parallel_results <- foreach(i = 1:n_obs, 
                           .packages = c("bsts"),
                           .export = c("test_X", "original_point_preds", "method")) %dopar% {
    # Create leave-one-out dataset
    loo_data <- train_data[-i, ]
    y_loo <- loo_data[, 1]
    X_loo <- loo_data[, -1]
    
    # Fit model on leave-one-out data with reduced iterations for speed
    ss <- AddSeasonal(list(), y_loo, nseasons = 4, season.duration = 1)
    ss <- AddLocalLinearTrend(ss, y_loo)
    
    model_loo <- tryCatch({
      bsts(y_loo,
           X = X_loo,
           state.specification = ss,
           niter = 10000,  # Reduced from 10000 for speed
           ping = 0)
    }, 
    error = function(e) {
      return(NULL)  # Return NULL if fitting fails
    })
    
    if (is.null(model_loo)) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    # Calculate LOOIC for leave-out model
    log_lik_loo <- model_loo$log.likelihood
    if (length(log_lik_loo) == 0) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    loo_liks <- numeric(length(log_lik_loo))
    for (j in 1:length(log_lik_loo)) {
      loo_liks[j] <- mean(log_lik_loo[-j], na.rm = TRUE)
    }
    
    looic <- -2 * mean(loo_liks, na.rm = TRUE)
    
    # Calculate change in test forecasts
    # Predict for test period using the leave-one-out model
    pred_loo <- predict(model_loo, newdata = test_X, burn = 100)
    
    point_pred_loo <- if(method == "median") {
      apply(pred_loo$distribution, 2, median)
    } else {
      colMeans(pred_loo$distribution)
    }
    
    # Calculate average change across all test observations
    delta_forecast <- mean(point_pred_loo - original_point_preds)
    
    return(list(looic = looic, delta_forecast = delta_forecast))
  }
  
  # Collect the parallel results
  for (i in 1:n_obs) {
    if (!is.null(parallel_results[[i]])) {
      looic_values[i] <- parallel_results[[i]]$looic
      delta_forecasts[i] <- parallel_results[[i]]$delta_forecast
    }
  }
  
  # Calculate original LOOIC
  looic_original <- -2 * mean(colMeans(log_lik_matrix, na.rm = TRUE))
  
  # Assign results
  results$delta_looic <- looic_values - looic_original  # Calculate delta LOOIC
  results$delta_test_forecast <- delta_forecasts
  
  return(results)
}

# FIXED Function to analyze the top models with test data
analyze_top_models_parallel <- function(best_models, all_models) {
  all_influence_results <- list()
  
  for(dep_var in names(best_models)) {
    cat("\n\nAnalyzing dependent variable:", dep_var)
    dep_influence <- list()
    models <- best_models[[dep_var]]
    
    for(i in 1:nrow(models)) {
      cat("\n\nProcessing model", i, "for", dep_var)
      model_info <- models[i, ]
      
      # Get training data for this window
      window_data <- get_window_data(dep_var, model_info$Window)
      
      # Get the original model 
      original_model <- all_models[[dep_var]][[model_info$Window]]
      
      # Calculate influence measures - FIXED to pass dep_var and window_size
      influence_results <- calculate_influence_test(
        original_model, 
        window_data,
        model_info$Method,
        dep_var,
        model_info$Window
      )
      
      # Add model identifier
      identifier <- paste("Window", model_info$Window, "-", model_info$Method)
      dep_influence[[identifier]] <- influence_results
    }
    
    all_influence_results[[dep_var]] <- dep_influence
  }
  
  return(all_influence_results)
}

# Function to generate comprehensive results tables
generate_influence_summary <- function(influence_results) {
  for(dep_var in names(influence_results)) {
    cat("\n=== Results for", dep_var, "===\n")
    
    for(model_name in names(influence_results[[dep_var]])) {
      cat("\n--- Model:", model_name, "---\n")
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # Round numeric columns to 4 decimal places
      results_df[, 2:4] <- round(results_df[, 2:4], 4)
      
      # Sort by absolute delta_test_forecast to find most influential observations
      sorted_df <- results_df[order(abs(results_df$delta_test_forecast), decreasing = TRUE), ]
      
      cat("Top 25 most influential observations:\n")
      print(head(sorted_df, 25))
      
      cat("\nSummary statistics:\n")
      
      # Calculate summary statistics separately
      lpd_summary <- summary(results_df$pointwise_lpd)
      looic_summary <- summary(results_df$delta_looic)
      forecast_summary <- summary(results_df$delta_test_forecast)
      
      # Print each metric separately to avoid data frame mixing issues
      cat("\nPointwise LPD:\n")
      print(round(lpd_summary, 4))
      cat("Std Dev:", round(sd(results_df$pointwise_lpd, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta LOOIC:\n")
      print(round(looic_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_looic, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta Test Forecast:\n")
      print(round(forecast_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_test_forecast, na.rm = TRUE), 4), "\n")
    }
  }
}

# Run the analysis with parallel processing
run_influence_analysis_parallel <- function() {
  # Start timing
  start_time <- Sys.time()
  cat("Starting parallel influence analysis at:", format(start_time), "\n")
  
  # First get the best models based on test data performance
  best_models <- get_best_models_test(all_models_rwall)
  
  # Run the influence analysis in parallel
  influence_results <- analyze_top_models_parallel(best_models, all_models_rwall)
  
  # Generate summary tables
  generate_influence_summary(influence_results)
  
  # End timing
  end_time <- Sys.time()
  execution_time <- end_time - start_time
  cat("\nInfluence analysis completed in:", format(execution_time), "\n")
  
  return(influence_results)
}

# Run the analysis
influence_results_parallel <- run_influence_analysis_parallel()

# Clean up the parallel backend when done
stopImplicitCluster()
```
# Excel
```{r}
create_enhanced_prediction_tables <- function(all_evaluations, all_models, extended_test_sets_rolling, 
                                              crps_scores, lpd_scores, file_path, 
                                              aggregate_smape_results,
                                              ensemble_smapes) {
  dep_vars <- names(all_evaluations)
  wb <- createWorkbook()
  
  for(dep_var in dep_vars) {
    addWorksheet(wb, dep_var)
    
    mean_smape   <- aggregate_smape_results[[dep_var]]$Mean_SMAPE
    median_smape <- aggregate_smape_results[[dep_var]]$Median_SMAPE  # if your code uses Median_SMAPE
    # Use weighted ensemble SMAPE if available
    sensi_smape <- if(!is.null(ensemble_smapes[[dep_var]])) round(ensemble_smapes[[dep_var]], 2) else NA
    
    variable_data <- data.frame(
      Col1 = c(
        dep_var,
        "Holds out period",
        "Prediction based on testing SMAPE",
        "Use Mean",
        "Use Median",
        "Prediction based on Sensitivity"
      ),
      Col2 = c(
        "",
        "2016Q4 - 2018Q2 and 2024Q4",
        "SMAPE",
        round(mean_smape, 2),
        round(median_smape, 2),
        sensi_smape
      ),
      stringsAsFactors = FALSE
    )
    
    writeData(wb, dep_var, variable_data, startRow = 1, startCol = 1, colNames = FALSE)
    setColWidths(wb, dep_var, cols = 1:2, widths = c(25, 20))
    
    start_row <- 8  # row 7 is blank
    
    # Handle potential NULLs in evaluations
    smape_mean <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_mean) || is.null(x$errors_mean$SMAPE)) {
        return(NA)
      }
      return(x$errors_mean$SMAPE)
    })
    
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_median) || is.null(x$errors_median$SMAPE)) {
        return(NA)
      }
      return(x$errors_median$SMAPE)
    })
    
    # Ensure we only include valid values for ranking
    valid_mean_indices <- which(!is.na(smape_mean))
    valid_median_indices <- which(!is.na(smape_median))
    
    # Get top 5 from valid indices
    if(length(valid_mean_indices) >= 5) {
      top_5_mean <- valid_mean_indices[order(smape_mean[valid_mean_indices])[1:5]]
    } else if(length(valid_mean_indices) > 0) {
      top_5_mean <- valid_mean_indices[order(smape_mean[valid_mean_indices])[1:length(valid_mean_indices)]]
    } else {
      top_5_mean <- integer(0)
    }
    
    if(length(valid_median_indices) >= 5) {
      top_5_median <- valid_median_indices[order(smape_median[valid_median_indices])[1:5]]
    } else if(length(valid_median_indices) > 0) {
      top_5_median <- valid_median_indices[order(smape_median[valid_median_indices])[1:length(valid_median_indices)]]
    } else {
      top_5_median <- integer(0)
    }
    
    crps_mean <- numeric(length(all_evaluations[[dep_var]]))
    crps_sd   <- numeric(length(all_evaluations[[dep_var]]))
    for(i in seq_along(all_evaluations[[dep_var]])) {
      window_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        crps_mean[i] <- window_data$CRPS_Mean
        crps_sd[i]   <- window_data$CRPS_SD
      } else {
        crps_mean[i] <- NA
        crps_sd[i]   <- NA
      }
    }
    
    lpd_mean <- numeric(length(all_evaluations[[dep_var]]))
    lpd_sd   <- numeric(length(all_evaluations[[dep_var]]))
    for(i in seq_along(all_evaluations[[dep_var]])) {
      window_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        lpd_mean[i] <- window_data$LPD_Mean
        lpd_sd[i]   <- window_data$LPD_SD
      } else {
        lpd_mean[i] <- NA
        lpd_sd[i]   <- NA
      }
    }
    
    writeData(wb, dep_var, "Predictive interval for Top 5 models based on sMAPE", 
              startRow = start_row, startCol = 1)
    
    writeData(wb, dep_var, "Use Mean", startRow = start_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = start_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = start_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = start_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = start_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = start_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = start_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = start_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = start_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = start_row + 2, startCol = 9)
    
    current_row <- start_row + 3
    for(window in top_5_mean) {
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      
      # Check if the model exists and is a valid BSTS object
      if(is.null(all_models[[dep_var]][[window]]) || !inherits(all_models[[dep_var]][[window]], "bsts")) {
        # Model not available, write NA values
        writeData(wb, dep_var, "Model not available", startRow = current_row, startCol = 2)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      } else {
        # Check if the test data exists
        if(is.null(extended_test_sets_rolling[[dep_var]][[window]]) || 
           nrow(extended_test_sets_rolling[[dep_var]][[window]]) == 0) {
          # Test data not available
          writeData(wb, dep_var, "Test data not available", startRow = current_row, startCol = 2)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        } else {
          # Both model and test data are available
          tryCatch({
            pred_data <- extended_test_sets_rolling[[dep_var]][[window]]
            last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
            
            # Make predictions
            pred <- predict.bsts(all_models[[dep_var]][[window]], 
                                 newdata = as.data.frame(last_row_predictors),
                                 burn = 100)
            dist <- pred$distribution[,1]
            
            writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, mean(dist), startRow = current_row, startCol = 3)
            writeData(wb, dep_var, median(dist), startRow = current_row, startCol = 4)
            writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
          }, error = function(e) {
            # Handle prediction errors
            writeData(wb, dep_var, paste("Error:", e$message), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
          })
        }
        
        # Write performance metrics regardless of prediction success
        writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      }
      
      current_row <- current_row + 1
    }
    
    writeData(wb, dep_var, "Use Median", startRow = current_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = current_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = current_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = current_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = current_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = current_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = current_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = current_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = current_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = current_row + 2, startCol = 9)
    
    current_row <- current_row + 3
    for(window in top_5_median) {
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      
      # Check if the model exists and is a valid BSTS object
      if(is.null(all_models[[dep_var]][[window]]) || !inherits(all_models[[dep_var]][[window]], "bsts")) {
        # Model not available, write NA values
        writeData(wb, dep_var, "Model not available", startRow = current_row, startCol = 2)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      } else {
        # Check if the test data exists
        if(is.null(extended_test_sets_rolling[[dep_var]][[window]]) || 
           nrow(extended_test_sets_rolling[[dep_var]][[window]]) == 0) {
          # Test data not available
          writeData(wb, dep_var, "Test data not available", startRow = current_row, startCol = 2)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        } else {
          # Both model and test data are available
          tryCatch({
            pred_data <- extended_test_sets_rolling[[dep_var]][[window]]
            last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
            
            # Make predictions
            pred <- predict.bsts(all_models[[dep_var]][[window]], 
                                 newdata = as.data.frame(last_row_predictors),
                                 burn = 100)
            dist <- pred$distribution[,1]
            
            writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, mean(dist), startRow = current_row, startCol = 3)
            writeData(wb, dep_var, median(dist), startRow = current_row, startCol = 4)
            writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
          }, error = function(e) {
            # Handle prediction errors
            writeData(wb, dep_var, paste("Error:", e$message), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
          })
        }
        
        # Write performance metrics regardless of prediction success
        writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      }
      
      current_row <- current_row + 1
    }
    
    setColWidths(wb, dep_var, cols = 1, width = 50)
    setColWidths(wb, dep_var, cols = 2:9, width = 15)
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}



###############################################################################
## 3) add_sensitivity_analysis
##    Moves the sensitivity analysis block down by 3 rows and removes any extra row (e.g. SMAPE row)
###############################################################################
add_sensitivity_analysis <- function(sensitivity_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for(dep_var in names(sensitivity_results)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    
    # Calculate the exact row to place the sensitivity analysis
    # Find the last non-empty row of the existing content
    last_content_row <- max(which(!is.na(sheet_data[,1]) & sheet_data[,1] != ""), na.rm = TRUE)
    
    # Add a small gap (3 rows) after the last content
    last_row <- last_content_row + 6
    
    # Create the results data frame
    results_data <- data.frame(
      Window   = integer(),
      Method   = character(),
      Sigma    = numeric(),
      Slab_Var = numeric(),
      WAIC     = numeric(),
      LOOIC    = numeric(),
      MAPE     = numeric(),
      SMAPE    = numeric(),
      stringsAsFactors = FALSE
    )
    
    for (model_name in names(sensitivity_results[[dep_var]])) {
      model_info <- sensitivity_results[[dep_var]][[model_name]]
      window <- model_info$window
      method <- model_info$method
      
      for (result_name in names(model_info$results)) {
        result <- model_info$results[[result_name]]
        results_data <- rbind(results_data, data.frame(
          Window   = window,
          Method   = method,
          Sigma    = if(!is.null(result$sigma)) result$sigma else NA,
          Slab_Var = if(!is.null(result$slab_var)) result$slab_var else NA,
          WAIC     = if(!is.null(result$waic)) result$waic else NA,
          LOOIC    = if(!is.null(result$looic)) result$looic else NA,
          MAPE     = if(!is.null(result$mape)) result$mape else NA,
          SMAPE    = if(!is.null(result$smape)) result$smape else NA,
          stringsAsFactors = FALSE
        ))
      }
    }
    
    # Write the title directly (no mysterious dep var title)
    writeData(wb, dep_var, "Sensitivity Analysis Results", startRow = last_row, startCol = 1)
    
    # Immediately write the table
    writeData(wb, dep_var, results_data, startRow = last_row + 1, startCol = 1, colNames = TRUE)
    
    # Set column widths
    setColWidths(wb, dep_var, cols = 1:8, widths = c(10,10,10,10,12,12,10,10))
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## Fixed add_influence_analysis_to_workbook function
## Fixes the spacing issue and handles delta_test_forecast/delta_holdout_forecast
###############################################################################
add_influence_analysis_to_workbook <- function(influence_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for (dep_var in names(influence_results)) {
    if (!dep_var %in% names(wb)) {
      addWorksheet(wb, dep_var)
    }
    
    sheet_data <- tryCatch({
      readWorkbook(wb, sheet = dep_var)
    }, error = function(e) {
      data.frame()
    })
    
    # Calculate exact start row by finding the last sensitivity analysis row
    # Find the last non-empty row in the sheet
    non_empty_rows <- which(!is.na(sheet_data[,1]) & sheet_data[,1] != "")
    
    if (length(non_empty_rows) > 0) {
      last_content_row <- max(non_empty_rows, na.rm = TRUE)
      # Look for "Sensitivity Analysis Results" in the sheet
      sensitivity_rows <- which(sheet_data[,1] == "Sensitivity Analysis Results")
      
      if (length(sensitivity_rows) > 0) {
        sensitivity_row <- max(sensitivity_rows)
        # Find the last row of the sensitivity table
        # Usually it's a block of data after the title
        for (i in (sensitivity_row + 1):nrow(sheet_data)) {
          if (is.na(sheet_data[i,1]) || sheet_data[i,1] == "") {
            last_sensitivity_row <- i - 1
            break
          }
          # If we reach the end of the dataframe
          if (i == nrow(sheet_data)) {
            last_sensitivity_row <- i
          }
        }
        # Start influence analysis 5 rows after the sensitivity table
        current_row <- last_sensitivity_row + 9
      } else {
        # If sensitivity analysis not found, start 5 rows after last content
        current_row <- last_content_row + 9
      }
    } else {
      # If sheet is empty
      current_row <- 1
    }
    
    processed_models <- c()
    
    for (model_name in names(influence_results[[dep_var]])) {
      if (model_name %in% processed_models) next
      processed_models <- c(processed_models, model_name)
      
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # CRITICAL FIX: Map delta_test_forecast to delta_holdout_forecast if needed
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        results_df$delta_holdout_forecast <- results_df$delta_test_forecast
      }
      
      # Process delta_looic if present
      if ("delta_looic" %in% names(results_df)) {
        results_df$delta_looic <- as.numeric(as.character(results_df$delta_looic))
      } else {
        results_df$delta_looic <- rep(NA, nrow(results_df))
      }
      
      # Round numeric columns to 4 decimal places
      numeric_cols <- c("pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      for (col in numeric_cols) {
        if (col %in% names(results_df) && is.numeric(results_df[[col]])) {
          results_df[[col]] <- round(results_df[[col]], 4)
        }
      }
      
      # Sort by absolute delta_holdout_forecast
      if (sum(!is.na(results_df$delta_holdout_forecast)) > 0) {
        sorted_df <- results_df[order(abs(results_df$delta_holdout_forecast), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df <- results_df
      }
      top_25_forecast <- head(sorted_df, 25)
      
      # Write the title
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta Holdout Forecast - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      # Get the correct column names
      if (all(c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      } else if (all(c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast")
        colnames(top_25_forecast)[colnames(top_25_forecast) == "delta_test_forecast"] <- "delta_holdout_forecast"
      } else {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      }
      
      # Make sure we only write columns that actually exist
      write_cols <- intersect(write_cols, colnames(top_25_forecast))
      
      # Write the data
      writeData(wb, dep_var, top_25_forecast[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_forecast)), 
                cols = col_idx)
      }
      
      # Proceed to next section - 5 rows after this table
      current_row <- current_row + nrow(top_25_forecast) + 5
      
      # Sort by absolute delta_looic
      if (sum(!is.na(results_df$delta_looic)) > 0) {
        sorted_df_looic <- results_df[order(abs(results_df$delta_looic), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df_looic <- results_df
      }
      top_25_looic <- head(sorted_df_looic, 25)
      
      # Write looic section
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta LOOIC - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      writeData(wb, dep_var, top_25_looic[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_looic)), 
                cols = col_idx)
      }
      
      # Proceed to next model section - 5 rows after this table
      current_row <- current_row + nrow(top_25_looic) + 5
    }
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  cat("Influence analysis results added to workbook:", file_path, "\n")
}
###############################################################################
## Parallelized version of create_excel_results
###############################################################################
create_excel_results_parallel <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets_rolling, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  rolling_splits, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  all_results,         
  top_model_results,   
  hold_out_dataset     
) {
  file_path <- "RW_ALL.xlsx"
  
  # This part is harder to parallelize as it builds on previous results
  ensemble_smapes <- get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
  
  # Create prediction tables
  wb <- create_enhanced_prediction_tables(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets_rolling = extended_test_sets_rolling, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_smapes = ensemble_smapes
  )
  
  # Now we'll parallelize these steps which can be run independently
  
  # Set up parallel processing
  no_cores <- detectCores() - 1  # Leave one core free
  cat("Using", no_cores, "cores for parallel Excel processing\n")
  
  # Register the parallel backend
  cl <- makeCluster(no_cores)
  registerDoParallel(cl)
  
  # Export necessary packages to all workers
  clusterEvalQ(cl, {
    library(openxlsx)
    library(bsts)
    library(coda)
  })
  
  # Process the different analyses in parallel
  # Each function will load, modify, and save the workbook independently
  
  # Run tasks in parallel
  results <- foreach(task_num = 1:3, .packages = c("openxlsx", "bsts")) %dopar% {
    result <- switch(task_num,
      # Task 1: Add sensitivity analysis
      {
        tryCatch({
          add_sensitivity_analysis(sensitivity_results, file_path)
          "Sensitivity analysis completed successfully"
        }, error = function(e) {
          paste("Error in sensitivity analysis:", e$message)
        })
      },
      
      # Task 2: Add influence analysis
      {
        tryCatch({
          # Use the fixed version of the function
          add_influence_analysis_to_workbook(influence_results, file_path)
          "Influence analysis completed successfully"
        }, error = function(e) {
          paste("Error in influence analysis:", e$message)
        })
      },
      
      # Task 3: Add diagnostics
      {
        tryCatch({
          add_diagnostics_to_workbook(
            all_models = all_models, 
            rolling_splits = rolling_splits, 
            all_diagnostics = all_diagnostics, 
            all_looic = all_looic,
            bayesian_R2_results = bayesian_R2_results, 
            crps_scores = crps_scores, 
            lpd_scores = lpd_scores, 
            ljung_box_results = ljung_box_results,
            file_path = file_path
          )
          "Diagnostics completed successfully"
        }, error = function(e) {
          paste("Error in diagnostics:", e$message)
        })
      }
    )
    return(result)
  }
  
  # Stop the cluster
  stopCluster(cl)
  registerDoSEQ()  # Reset to sequential processing
  
  # Clean up
  rm(cl)
  gc()  # Force garbage collection
  
  # Summarize results
  summary <- paste("Excel processing results:", 
                   paste(results, collapse = "; "))
  
  return(paste("Results successfully written to", file_path, "-", summary))
}
###############################################################################
## 5) add_diagnostics_to_workbook (unchanged)
###############################################################################
add_diagnostics_to_workbook <- function(all_models, rolling_splits, all_diagnostics, all_looic, 
                                        bayesian_R2_results, crps_scores, lpd_scores, 
                                        ljung_box_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  suppressWarnings({
    if (!requireNamespace("coda", quietly = TRUE)) {
      install.packages("coda")
    }
    library(coda)
  })
  
  for(dep_var in names(all_models)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    current_row <- nrow(sheet_data) + 22
    
    for(window in 1:length(all_models[[dep_var]])) {
      if (is.null(all_models[[dep_var]][[window]])) next
      
      train_size <- if (!is.null(rolling_splits[[dep_var]]) && 
                        !is.null(rolling_splits[[dep_var]][[window]]) && 
                        !is.null(rolling_splits[[dep_var]][[window]]$train)) {
        nrow(rolling_splits[[dep_var]][[window]]$train)
      } else {
        NA
      }
      
      test_size <- if (!is.null(rolling_splits[[dep_var]]) && 
                       !is.null(rolling_splits[[dep_var]][[window]]) && 
                       !is.null(rolling_splits[[dep_var]][[window]]$test)) {
        nrow(rolling_splits[[dep_var]][[window]]$test)
      } else {
        NA
      }
      
      n_predictors <- if (!is.null(rolling_splits[[dep_var]]) && 
                          !is.null(rolling_splits[[dep_var]][[window]]) && 
                          !is.null(rolling_splits[[dep_var]][[window]]$train)) {
        ncol(rolling_splits[[dep_var]][[window]]$train) - 1
      } else {
        NA
      }
      
      dic_value <- if (!is.null(all_diagnostics) && 
                       !is.null(all_diagnostics[[dep_var]]) && 
                       !is.null(all_diagnostics[[dep_var]]$DIC) &&
                       length(all_diagnostics[[dep_var]]$DIC) >= window) {
        all_diagnostics[[dep_var]]$DIC[window]
      } else {
        NA
      }
      
      waic_value <- if (!is.null(all_diagnostics) && 
                        !is.null(all_diagnostics[[dep_var]]) && 
                        !is.null(all_diagnostics[[dep_var]]$WAIC) &&
                        length(all_diagnostics[[dep_var]]$WAIC) >= window) {
        all_diagnostics[[dep_var]]$WAIC[window]
      } else {
        NA
      }
      
      looic_value <- if (!is.null(all_looic) && 
                         !is.null(all_looic[[dep_var]]) && 
                         length(all_looic[[dep_var]]) >= window) {
        all_looic[[dep_var]][window]
      } else {
        NA
      }
      
      r2_value <- if (!is.null(bayesian_R2_results) && 
                      !is.null(bayesian_R2_results[[dep_var]]) && 
                      !is.null(bayesian_R2_results[[dep_var]]$Mean_R2) &&
                      length(bayesian_R2_results[[dep_var]]$Mean_R2) >= window) {
        bayesian_R2_results[[dep_var]]$Mean_R2[window]
      } else {
        NA
      }
      
      crps_value <- if (!is.null(crps_scores) && 
                        !is.null(crps_scores[[dep_var]]) && 
                        any(crps_scores[[dep_var]]$Window == window)) {
        crps_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == window, ]
        if (nrow(crps_data) > 0 && !is.na(crps_data$CRPS_Mean) && !is.na(crps_data$CRPS_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", crps_data$CRPS_Mean, crps_data$CRPS_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lpd_value <- if (!is.null(lpd_scores) && 
                       !is.null(lpd_scores[[dep_var]]) && 
                       any(lpd_scores[[dep_var]]$Window == window)) {
        lpd_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == window, ]
        if (nrow(lpd_data) > 0 && !is.na(lpd_data$LPD_Mean) && !is.na(lpd_data$LPD_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", lpd_data$LPD_Mean, lpd_data$LPD_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lb_stat <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_Statistic) &&
                     length(ljung_box_results[[dep_var]]$LB_Statistic) >= window) {
        ljung_box_results[[dep_var]]$LB_Statistic[window]
      } else {
        NA
      }
      
      lb_pval <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_pvalue) &&
                     length(ljung_box_results[[dep_var]]$LB_pvalue) >= window) {
        ljung_box_results[[dep_var]]$LB_pvalue[window]
      } else {
        NA
      }
      
      metrics <- data.frame(
        Metric = c(
          "Training Sample Size",
          "Test Sample Size",
          "Number of Predictors",
          "DIC/number of training period",
          "WAIC/number of training period",
          "LOOIC",
          "Bayesian R²",
          "Number of iterations used in BSTS",
          "CRPS",
          "LPD",
          "LB_Statistic",
          "LB_pvalue"
        ),
        Value = c(
          train_size,
          test_size,
          n_predictors,
          dic_value,
          waic_value,
          looic_value,
          r2_value,
          10000,
          crps_value,
          lpd_value,
          lb_stat,
          lb_pval
        )
      )
      
      writeData(wb, dep_var, paste("Window", window), startRow = current_row)
      current_row <- current_row + 2
      writeData(wb, dep_var, metrics, startRow = current_row, startCol = 1, colNames = FALSE)
      current_row <- current_row + nrow(metrics) + 2
      
      tryCatch({
        coef_stats <- data.frame(
          Parameter     = character(),
          Mean          = numeric(),
          Median        = numeric(),
          CI_2.5        = numeric(),
          CI_97.5       = numeric(),
          ESS           = numeric(),
          Post_Prob     = numeric(),
          Geweke_Pval   = numeric(),
          HW_Pval       = numeric(),
          RL_Autocorr   = numeric(),
          RL_Iterations = numeric(),
          RL_Burnin     = numeric(),
          RL_Dependence = numeric(),
          stringsAsFactors = FALSE
        )
        
        model <- all_models[[dep_var]][[window]]
        if (!is.null(model) && !is.null(model$coefficients)) {
          coef_matrix <- as.matrix(model$coefficients)
          
          for(j in 1:ncol(coef_matrix)) {
            param_name <- colnames(coef_matrix)[j]
            chain <- as.vector(coef_matrix[,j])
            if (length(chain) < 2) next
            
            ci <- tryCatch({
              quantile(chain, probs = c(0.025, 0.975))
            }, error = function(e) c(NA, NA))
            
            mean_val   <- tryCatch(mean(chain), error = function(e) NA)
            median_val <- tryCatch(median(chain), error = function(e) NA)
            ess_val    <- tryCatch(as.numeric(effectiveSize(mcmc(chain))), error = function(e) NA)
            pip        <- tryCatch(mean(chain != 0), error = function(e) NA)
            
            geweke_pval <- tryCatch({
              geweke <- geweke.diag(mcmc(chain))
              2 * pnorm(-abs(geweke$z))
            }, error = function(e) NA)
            
            hw_pval <- tryCatch({
              hw <- heidel.diag(mcmc(chain))
              if (is.null(hw)) NA else hw[1, "pvalue"]
            }, error = function(e) NA)
            
            rl_stats <- tryCatch({
              if (length(unique(chain)) >= 10) {
                binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
                rl <- raftery.diag(binary_chain)
                c(cor(chain[-1], chain[-length(chain)]),
                  rl$resmatrix[1,"N"],
                  rl$resmatrix[1,"M"],
                  rl$resmatrix[1,"I"])
              } else {
                rep(NA, 4)
              }
            }, error = function(e) rep(NA, 4))
            
            coef_stats <- rbind(coef_stats, data.frame(
              Parameter     = param_name,
              Mean          = round(mean_val, 4),
              Median        = round(median_val, 4),
              CI_2.5        = round(ci[1], 4),
              CI_97.5       = round(ci[2], 4),
              ESS           = round(ess_val, 0),
              Post_Prob     = round(pip, 4),
              Geweke_Pval   = round(geweke_pval, 4),
              HW_Pval       = round(hw_pval, 4),
              RL_Autocorr   = round(rl_stats[1], 4),
              RL_Iterations = round(rl_stats[2], 0),
              RL_Burnin     = round(rl_stats[3], 0),
              RL_Dependence = round(rl_stats[4], 2),
              stringsAsFactors=FALSE
            ))
          }
        }
        
        if (nrow(coef_stats) > 0) {
          writeData(wb, dep_var, coef_stats, startRow = current_row)
          current_row <- current_row + nrow(coef_stats) + 3
        } else {
          writeData(wb, dep_var, "No coefficient data available", startRow = current_row)
          current_row <- current_row + 4
        }
        
      }, error = function(e) {
        writeData(wb, dep_var, paste("Error processing coefficients:", e$message), startRow = current_row)
        current_row <- current_row + 4
      })
    }
    
    setColWidths(wb, dep_var, cols = 1:13, widths = "auto")
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## 6) format_window_range (unchanged)
###############################################################################
format_window_range <- function(window) {
  # Define the descriptive text for each rolling window's train and test sets
  # Updated to reflect the new window specifications and hold-out period (80-86)
  train_ranges <- c(
    "1 - 79 & 87 - 97",
    "5 - 79 & 87 - 101",
    "9 - 79 & 87 - 105",
    "13 - 79 & 87 - 109 "
  )
  
  test_ranges <- c(
    "98 - 111",
    "1 - 4 & 102 - 111",
    "1 - 8 & 106 - 111",
    "1 - 12 & 110 - 111"
  )
  
  paste(train_ranges[window], "as the training and the", test_ranges[window], "as the testing")
}
###############################################################################
## 7) create_excel_results
##    Computes ensemble SMAPEs then calls the above functions.
###############################################################################
create_excel_results <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets_rolling, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  rolling_splits, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  all_results,         
  top_model_results,   
  hold_out_dataset     
) {
  file_path <- "RW_ALL_2016Q4.xlsx"
  
  # Map delta_test_forecast to delta_holdout_forecast in influence results
  for (dep_var in names(influence_results)) {
    for (model_name in names(influence_results[[dep_var]])) {
      results_df <- influence_results[[dep_var]][[model_name]]
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        influence_results[[dep_var]][[model_name]]$delta_holdout_forecast <- 
          results_df$delta_test_forecast
      }
    }
  }
  
  # Get ensemble SMAPEs
  ensemble_smapes <- get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
  
  # Create prediction tables
  cat("Creating prediction tables...\n")
  wb <- create_enhanced_prediction_tables(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets_rolling = extended_test_sets_rolling, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_smapes = ensemble_smapes
  )
  
  # Add sensitivity analysis with proper spacing
  cat("Adding sensitivity analysis...\n")
  add_sensitivity_analysis(sensitivity_results, file_path)
  
  # Add influence analysis with proper spacing
  cat("Adding influence analysis...\n")
  add_influence_analysis_to_workbook(influence_results, file_path)
  
  # Add diagnostics
  cat("Adding diagnostics...\n")
  add_diagnostics_to_workbook(
    all_models = all_models, 
    rolling_splits = rolling_splits, 
    all_diagnostics = all_diagnostics, 
    all_looic = all_looic,
    bayesian_R2_results = bayesian_R2_results, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    ljung_box_results = ljung_box_results,
    file_path = file_path
  )
  
  return(paste("Results successfully written to", file_path))
}
###############################################################################
## 8) Example usage
###############################################################################
 result <- create_excel_results(
   all_evaluations = all_evaluations_rwall, 
   all_models = all_models_rwall, 
   extended_test_sets_rolling = extended_test_sets_rolling, 
   crps_scores = crps_scores_custom_rolling, 
   lpd_scores = lpd_scores, 
   sensitivity_results = sensitivity_results, 
   influence_results = influence_results_parallel, 
   rolling_splits = rolling_splits, 
   all_diagnostics = all_diagnostics, 
   all_looic = all_looic, 
   bayesian_R2_results = bayesian_R2_results,
   ljung_box_results = ljung_box_results,
   aggregate_smape_results = aggregate_smape_results_holdout,
   all_results = all_results,
   top_model_results = top_model_results,
   hold_out_dataset = hold_out_dataset
 )
 print(result)
```

# RW_BW
#  Variable selection 
```{r, results='hide', message=FALSE, warning=FALSE}
# Function to calculate LOOIC according to the specified method
calculate_looic <- function(model, train_data) {
  n_train <- nrow(train_data)
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  looic <- (-2 * sum(loo_liks)) / n_train
  return(looic)
}

# Backward selection function
backward_selection <- function(train_data) {
  set.seed(1234)
  y <- train_data[, 1]
  X <- train_data[, -1]
  all_predictors <- colnames(X)
  removed_vars <- c() # Track removed variables
  removed_indices <- c() # Track removed column indices
  
  # Set up state space components
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y)
  
  # Create mapping between predictors and their original column indices
  predictor_indices <- setNames(seq_along(all_predictors) + 2, all_predictors)  # +2 for the y and date columns
  
  current_predictors <- all_predictors
  
  fit_model <- function(predictors) {
    set.seed(1234)
    if (length(predictors) == 0) {
      model_data <- data.frame(y = y)
      model <- bsts(y ~ 1,
                    state.specification = ss,
                    niter = 10000,
                    data = model_data)
    } else {
      formula_str <- paste("y ~", paste(predictors, collapse = " + "))
      formula <- as.formula(formula_str)
      model_data <- as.data.frame(cbind(y = y, X[, predictors, drop = FALSE]))
      model <- bsts(formula,
                    state.specification = ss,
                    niter = 10000,
                    data = model_data)
    }
    return(model)
  }
  
  # Start with the full model
  cat("Fitting full model with", length(current_predictors), "predictors\n")
  full_model <- fit_model(current_predictors)
  current_looic <- calculate_looic(full_model, train_data)
  cat("Full model LOOIC:", current_looic, "\n")
  
  best_model <- full_model
  best_predictors <- current_predictors
  iteration <- 1
  
  # Define the threshold for LOOIC improvement (0.004%)
  looic_threshold <- current_looic * 0.00004
  
  while (length(current_predictors) > 1) {
    cat("\nIteration", iteration, "- Current predictors:", length(current_predictors), "\n")
    
    # Track the best improvement in this round
    best_looic_improvement <- 0
    var_to_remove <- NULL
    temp_best_model <- NULL
    
    # Test removing each predictor one at a time
    for (i in 1:length(current_predictors)) {
      var <- current_predictors[i]
      cat("  Testing removal of", var, "\n")
      candidate_predictors <- current_predictors[-i]
      
      tryCatch({
        candidate_model <- fit_model(candidate_predictors)
        candidate_looic <- calculate_looic(candidate_model, train_data)
        looic_change <- current_looic - candidate_looic
        
        cat("    LOOIC:", candidate_looic, "(change:", looic_change, ")\n")
        
        # Check if removing this predictor results in greater LOOIC improvement
        if (looic_change > best_looic_improvement) {
          best_looic_improvement <- looic_change
          var_to_remove <- i
          temp_best_model <- candidate_model
          cat("    Best improvement so far!\n")
        }
      }, error = function(e) {
        cat("    Error fitting model without", var, ":", e$message, "\n")
      })
    }
    
    # Check if the best improvement exceeds the threshold
    if (!is.null(var_to_remove) && best_looic_improvement > looic_threshold) {
      removed_var <- current_predictors[var_to_remove]
      removed_vars <- c(removed_vars, removed_var) # Add to removed variables list
      removed_indices <- c(removed_indices, predictor_indices[removed_var]) # Add column index
      
      cat("Removing variable:", removed_var, "(column index:", predictor_indices[removed_var], ")\n")
      cat("LOOIC improved from", current_looic, "to", current_looic - best_looic_improvement, "\n")
      
      best_model <- temp_best_model
      current_looic <- current_looic - best_looic_improvement
      best_predictors <- current_predictors[-var_to_remove]
      current_predictors <- best_predictors
    } else {
      cat("No significant LOOIC improvement found (threshold:", looic_threshold, ")\n")
      break # Stop if no meaningful improvement
    }
    
    iteration <- iteration + 1
  }
  
  cat("\nBackward selection complete\n")
  cat("Selected", length(best_predictors), "predictors\n")
  
  return(list(
    model = best_model,
    selected_vars = best_predictors,
    looic = current_looic,
    removed_vars = removed_vars, # Include removed variables in the output
    removed_indices = removed_indices # Include removed column indices in the output
  ))
}

# Function to find best models based on evaluations 
# Find best models based on sMAPE from existing evaluation results
find_best_models <- function(all_evaluations) {
  best_models <- list()  # To store the best models for each dep var
  
  for(var_name in names(all_evaluations)) {
    cat("\nFor", var_name, ":\n")
    
    # Collect all sMAPE values
    all_smape_results <- data.frame(
      Window = integer(),
      Method = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Collect mean-based sMAPE values
    for(i in 1:4) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      all_smape_results <- rbind(all_smape_results, data.frame(
        Window = i,
        Method = "mean",
        SMAPE = window_errors$SMAPE,
        stringsAsFactors = FALSE
      ))
    }
    
    # Collect median-based sMAPE values
    for(i in 1:4) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      all_smape_results <- rbind(all_smape_results, data.frame(
        Window = i,
        Method = "median",
        SMAPE = window_errors$SMAPE,
        stringsAsFactors = FALSE
      ))
    }
    
    # Sort by sMAPE (ascending) to find the best model overall
    all_smape_results <- all_smape_results[order(all_smape_results$SMAPE), ]
    best_model <- all_smape_results[1, ]
    
    cat("Best model for", var_name, "based on sMAPE:\n")
    cat("Window:", best_model$Window, "using", best_model$Method, 
        "method with sMAPE =", round(best_model$SMAPE, 4), "\n")
    
    # Store the best model info
    best_models[[var_name]] <- list(
      best_window = best_model$Window,
      best_method = best_model$Method,
      best_smape = best_model$SMAPE
    )
  }
  
  return(best_models)
}
# Revised function to run backward selection only on the best window per dependent variable
# and print removed columns
# Revised function to run backward selection on the best models
run_best_selection <- function(best_models) {
  all_models <- list()
  
  for (dep_var_name in names(best_models)) {
    cat("\n\n==== Variable Selection for", dep_var_name, "====\n")
    
    # Use the best window determined by sMAPE
    best_window <- best_models[[dep_var_name]]$best_window
    best_method <- best_models[[dep_var_name]]$best_method
    best_smape <- best_models[[dep_var_name]]$best_smape
    
    cat("\n--- Using best model: Window", best_window, "with", best_method, 
        "method (sMAPE =", round(best_smape, 4), ") for", dep_var_name, "---\n")
    
    # Retrieve training data for the best window (assuming 'rolling_splits' is available)
    train_data <- rolling_splits[[dep_var_name]][[best_window]]$train
    
    # Run backward selection on the training data
    selection_result <- backward_selection(train_data)
    
    # Store the model and selected variables for this dependent variable
    all_models[[dep_var_name]] <- selection_result
    
    # Print selected variables
    cat("\nSelected variables for", dep_var_name, ":", paste(selection_result$selected_vars, collapse=", "), "\n")
    
    # Print removed variables and their column indices
    cat("\nRemoved variables from original dataset for", dep_var_name, ":\n")
    for(i in 1:length(selection_result$removed_vars)) {
      cat(selection_result$removed_vars[i], "(Column", selection_result$removed_indices[i], ")\n")
    }
    
    # Store removed variables in a dataframe or table if needed
    if (dep_var_name == "senate") {
      cat("\n=== REMOVED COLUMN INDICES IN SENATE ORIGINAL DATASET ===\n")
      cat(paste(selection_result$removed_indices, collapse="\n"), "\n")
      cat("=================================================\n")
    }
  }
  
  return(all_models)
}

# Example usage:
# Assume 'all_evaluations' is already available and best_models has been computed:
 best_models_rwbw <- find_best_models(all_evaluations_rwall)
 final_models_rwbw <- run_best_selection( best_models_rwbw)
```


```{r}
# Create a new workbook
wb <- createWorkbook()

# Loop through each model and write the removed variables to a sheet
for(dep_var_name in names(final_models_rwbw)) {
  removed_vars <- final_models_rwbw[[dep_var_name]]$removed_vars
  
  # Create a dataframe
  removed_df <- data.frame(
    Variable = removed_vars,
    stringsAsFactors = FALSE
  )
  
  # Add a sheet with the name of the dependent variable
  addWorksheet(wb, sheetName = dep_var_name)
  writeData(wb, sheet = dep_var_name, x = removed_df)
}

# Save the workbook
saveWorkbook(wb, file = "Removed_pre_BWRW_2016.xlsx", overwrite = TRUE)
cat("Removed predictors written to 'removed_predictors.xlsx'\n")
```


## print dep var removed col num
```{r}
# Function to find the exact column indices for removed variables
find_exact_indices <- function(final_models, full_datasets) {
  for (dep_var in names(final_models)) {
    removed_vars <- final_models[[dep_var]]$removed_vars
    
    # Get the corresponding dataset
    dataset <- full_datasets[[dep_var]]
    
    cat("\nDependent variable:", dep_var, "\n")
    cat("Removed variables:", paste(removed_vars, collapse=", "), "\n")
    
    # Print all column names with their indices for debugging
    cat("Dataset columns:\n")
    for (i in 1:ncol(dataset)) {
      cat("Column", i, ":", colnames(dataset)[i], "\n")
    }
    
    # Try exact match first
    cat("\nMatching indices:\n")
    for (var in removed_vars) {
      exact_match <- which(colnames(dataset) == var)
      if (length(exact_match) > 0) {
        cat(var, "=> index:", exact_match, "\n")
      } else {
        # Try partial match if exact match fails
        partial_matches <- grep(var, colnames(dataset), fixed=TRUE)
        if (length(partial_matches) > 0) {
          cat(var, "=> potential indices (partial match):", 
              paste(partial_matches, collapse=", "), "\n")
        } else {
          cat(var, "=> NOT FOUND\n")
        }
      }
    }
    cat("\n----------------------------\n")
  }
}

# Usage:
 find_exact_indices(final_models_rwbw ,Indice_testing_dataset)
```


## New dataset use removed colm number

```{r}
# Create datasets for each dependent variable
dep_var_datasets <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Store in list
  dep_var_datasets[[dep_var_name]] <- dataset
}


# Take lag of the missing value of 2024 Q4
dep_var_datasets_modified <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Repeat the first column (dep_var) and add it before the first column
  dataset <- cbind(dataset[, 1, drop = FALSE], dataset)  # Add first column as the first column again
  
  # Loop over columns starting from the second column
  for(col in 2:ncol(dataset)) {
    # Check if the last row value is missing
    if(is.na(dataset[nrow(dataset), col])) {
      # Apply lag: take the value from the previous row for all rows of the column
      dataset[, col] <- lag(dataset[, col], 1, default = NA)
      
      # Modify column name to indicate lag if it was modified
      new_col_name <- paste0(colnames(dataset)[col], "_lag1")
      colnames(dataset)[col] <- new_col_name
    }
  }
  
  # Store in list
  dep_var_datasets_modified[[dep_var_name]] <- dataset
}

# Delete 
dep_var_datasets_modified <- lapply(dep_var_datasets_modified, function(x) {
  x[-(1:8), ]
})

# Extract last row from each dataset
last_row <- lapply(dep_var_datasets_modified, function(x) {
  x[nrow(x), ]
})

# Remove last row from each dataset
dep_var_datasets <- lapply(dep_var_datasets_modified, function(x) {
  x[-nrow(x), ]
})
```


# get 2024Q4
```{r}
create_quarter_four_from_last_row <- function(dep_var_datasets, dep_var_sets) {
  quarter_four <- list()
  
  for (dep_var_name in names(dep_var_sets)) {
    # Get the original dep_var column name
    original_dep_var_name <- dep_var_sets[[dep_var_name]]$dep_var
    
    # Extract last row from dataset
    last_row_data <- dep_var_datasets[[dep_var_name]][nrow(dep_var_datasets[[dep_var_name]]), , drop = FALSE]
    
    # Separate and rename dep_var column
    dep_var_column <- last_row_data[, original_dep_var_name, drop = FALSE]
    colnames(dep_var_column) <- "dep_var"
    
    # Get the independent variables
    indep_vars_data <- last_row_data[, setdiff(colnames(last_row_data), original_dep_var_name), drop = FALSE]
    
    # Combine into final row format
    quarter_four[[dep_var_name]] <- cbind(dep_var_column, indep_vars_data)
    
    # Remove the last row from the dataset
    dep_var_datasets[[dep_var_name]] <- dep_var_datasets[[dep_var_name]][-nrow(dep_var_datasets[[dep_var_name]]), ]
  }
  
  return(list(quarter_four = quarter_four, dep_var_datasets = dep_var_datasets))
}

# Usage:
result <- create_quarter_four_from_last_row(dep_var_datasets, dep_var_sets)
quarter_four <- result$quarter_four
dep_var_datasets <- result$dep_var_datasets

```

```{r}
# Function to find and remove columns based on variable names in final_models
remove_by_found_indices <- function(final_models, dep_var_datasets) {
  # For each dependent variable
  for (dep_var in names(final_models)) {
    # Get the removed variable names
    removed_vars <- final_models[[dep_var]]$removed_vars
    
    # Get the dataset
    dataset <- dep_var_datasets[[dep_var]]
    
    # Find indices to remove
    indices_to_remove <- c()
    for (var in removed_vars) {
      # Try exact match first
      exact_match <- which(colnames(dataset) == var)
      if (length(exact_match) > 0) {
        indices_to_remove <- c(indices_to_remove, exact_match)
        cat("Found", var, "at index", exact_match, "\n")
      } else {
        # Try partial match if exact match fails
        partial_matches <- grep(var, colnames(dataset), fixed=TRUE)
        if (length(partial_matches) > 0) {
          indices_to_remove <- c(indices_to_remove, partial_matches)
          cat("Found", var, "at indices (partial match):", 
              paste(partial_matches, collapse=", "), "\n")
        } else {
          cat("Warning: Could not find", var, "in", dep_var, "dataset\n")
        }
      }
    }
    
    # Remove the columns if any were found
    if (length(indices_to_remove) > 0) {
      # Ensure unique indices (no duplicates)
      indices_to_remove <- unique(indices_to_remove)
      
      # Remove columns
      dep_var_datasets[[dep_var]] <- dataset[, -indices_to_remove, drop = FALSE]
      cat("Removed", length(indices_to_remove), "columns from", dep_var, "dataset\n")
      cat("Removed column indices:", paste(indices_to_remove, collapse=", "), "\n")
    } else {
      cat("No columns to remove from", dep_var, "dataset\n")
    }
  }
  
  return(dep_var_datasets)
}

# Apply the removal of columns
dep_var_datasets <- remove_by_found_indices(final_models_rwbw,dep_var_datasets)
quarter_four = remove_by_found_indices(final_models_rwbw,quarter_four)
dep_var_datasets
```

 
# Extract the hold-out period (rows 80-86) + 2024 Q4

```{r}
# Extracting rows 80-87 from each dataset in dep_var_datasets
hold_out_dataset <- lapply(dep_var_datasets, function(df) {
  df[80:86, ]#period
})

# Create a list to store the updated datasets with quarter_four at the end
holdout_with_last_row <- list()

# Iterate through each dep_var_name and append quarter_four at the end
for(dep_var_name in names(hold_out_dataset)) {
  
  # Ensure column names match between quarter_four and the data frame in hold_out_dataset
  if (!all(names(quarter_four[[dep_var_name]]) == names(hold_out_dataset[[dep_var_name]]))) {
    # Manually adjust the column names of quarter_four to match hold_out_dataset
    names(quarter_four[[dep_var_name]]) <- names(hold_out_dataset[[dep_var_name]])
  }
  
  # Combine the selected holdout (80-87 rows) and the corresponding last row from quarter_four
  holdout_with_last_row[[dep_var_name]] <- rbind(
    hold_out_dataset[[dep_var_name]],
    quarter_four[[dep_var_name]]
  )
}

# Update hold_out_dataset with the modified datasets
hold_out_dataset <- holdout_with_last_row

hold_out_dataset 
```


# Data without hold out

```{r}
hold_out_period <- 80:86#period

# Function to exclude the hold-out period from the dataset
exclude_hold_out_1 <- function(df) {
  # Exclude the rows that are in the hold-out period (81-88)
  df_no_hold <- df[!rownames(df) %in% hold_out_period, ]
  return(df_no_hold)
}

#--------------------------------------------------------------
# This is the dataset without the hold out period 
Indice_testing_dataset <- lapply(dep_var_datasets, function(df) {
  # Exclude rows 81-88 from the dataset and return the modified dataset
  exclude_hold_out_1(df)
})

dep_var_datasets = Indice_testing_dataset
# Verify the result
Indice_testing_dataset
```


```{r}
# Create rolling windows with hold-out period exclusion (4 windows only)
# Create rolling windows with consistent sizes (4-5 windows)
rolling_splits <- list()
for(dep_var_name in names(dep_var_datasets)) {
  current_data <- dep_var_datasets[[dep_var_name]]
  dataset_windows <- list()
  
  # Get total number of rows in the dataset
  total_rows <- nrow(current_data)
  
  # Fixed parameters
  hold_out_period <- 80:86#period
  train_size <- 90  # Fixed training window size
  window_step <- ceiling((total_rows - length(hold_out_period) - train_size) / 4)  # Step size to create 4-5 windows
  
  # Calculate number of windows possible
  max_windows <- 1 + floor((total_rows - length(hold_out_period) - train_size) / window_step)
  max_windows <- min(max_windows, 5)  # Cap at 5 windows
  
  # Available indices (excluding hold-out period)
  available_indices <- setdiff(1:total_rows, hold_out_period)
  
  # Create windows
  for(window_index in 1:max_windows) {
    # Calculate start position for this window
    start_pos <- 1 + (window_index - 1) * window_step
    
    # Get train indices (first train_size available indices from start_pos)
    # Make sure we don't go beyond available indices
    potential_train_indices <- available_indices[available_indices >= start_pos]
    if(length(potential_train_indices) < train_size) {
      cat("Not enough data for window", window_index, "in", dep_var_name, "\n")
      break
    }
    
    train_indices <- head(potential_train_indices, train_size)
    
    # All remaining indices are test indices
    test_indices <- setdiff(available_indices, train_indices)
    
    # Extract the data
    train_data <- current_data[train_indices, ]
    test_data <- current_data[test_indices, ]
    
    # Check for NA values in training data
    train_na_rows <- which(apply(train_data, 1, function(x) any(is.na(x))))
    if(length(train_na_rows) > 0) {
      cat("Removing", length(train_na_rows), "rows with NA values from", 
          dep_var_name, "window", window_index, "training data\n")
      
      # Remove rows with NA
      train_data <- train_data[-train_na_rows, ]
      train_indices <- train_indices[-train_na_rows]
    }
    
    # Check for NA values in test data
    test_na_rows <- which(apply(test_data, 1, function(x) any(is.na(x))))
    if(length(test_na_rows) > 0) {
      cat("Removing", length(test_na_rows), "rows with NA values from", 
          dep_var_name, "window", window_index, "test data\n")
      
      # Remove rows with NA
      test_data <- test_data[-test_na_rows, ]
      test_indices <- test_indices[-test_na_rows]
    }
    
    # Store the window
    dataset_windows[[window_index]] <- list(
      train = train_data, 
      test = test_data,
      train_indices = train_indices,
      test_indices = test_indices
    )
  }
  
  # Now ensure all windows have the same train and test size
  # Find the minimum sizes across all windows
  train_sizes <- sapply(dataset_windows, function(w) nrow(w$train))
  test_sizes <- sapply(dataset_windows, function(w) nrow(w$test))
  min_train_size <- min(train_sizes)
  min_test_size <- min(test_sizes)
  
  # Trim data in each window to match the minimum sizes
  for(i in 1:length(dataset_windows)) {
    # Trim training data if needed
    if(nrow(dataset_windows[[i]]$train) > min_train_size) {
      cat("Trimming window", i, "training data from", nrow(dataset_windows[[i]]$train), 
          "to", min_train_size, "rows\n")
      
      # Take the first min_train_size rows for consistency
      keep_indices <- head(1:nrow(dataset_windows[[i]]$train), min_train_size)
      dataset_windows[[i]]$train <- dataset_windows[[i]]$train[keep_indices, ]
      dataset_windows[[i]]$train_indices <- dataset_windows[[i]]$train_indices[keep_indices]
    }
    
    # Trim test data if needed
    if(nrow(dataset_windows[[i]]$test) > min_test_size) {
      cat("Trimming window", i, "test data from", nrow(dataset_windows[[i]]$test), 
          "to", min_test_size, "rows\n")
      
      # Take the first min_test_size rows for consistency
      keep_indices <- head(1:nrow(dataset_windows[[i]]$test), min_test_size)
      dataset_windows[[i]]$test <- dataset_windows[[i]]$test[keep_indices, ]
      dataset_windows[[i]]$test_indices <- dataset_windows[[i]]$test_indices[keep_indices]
    }
  }
  
  rolling_splits[[dep_var_name]] <- dataset_windows
}

# Function to summarize window information
summarize_windows <- function(windows_list) {
  cat("\n======= Window Summary =======\n")
  
  for (dep_var_name in names(windows_list)) {
    cat("\nSummary for", dep_var_name, ":\n")
    windows <- windows_list[[dep_var_name]]
    
    # Create a table for sizes
    test_sizes <- sapply(windows, function(w) nrow(w$test))
    train_sizes <- sapply(windows, function(w) nrow(w$train))
    
    size_table <- data.frame(
      Window = 1:length(windows),
      Train_Size = train_sizes,
      Test_Size = test_sizes
    )
    
    print(size_table)
    
    # Check if all sizes are the same
    if(length(unique(train_sizes)) == 1) {
      cat("\nAll windows have uniform train size of", train_sizes[1], "rows\n")
    } else {
      cat("\nWarning: Train sizes vary across windows:", paste(train_sizes, collapse=", "), "\n")
    }
    
    if(length(unique(test_sizes)) == 1) {
      cat("All windows have uniform test size of", test_sizes[1], "rows\n")
    } else {
      cat("Warning: Test sizes vary across windows:", paste(test_sizes, collapse=", "), "\n")
    }
    
    # Function to find consecutive sequences for readability
    find_sequences <- function(indices) {
      if(length(indices) == 0) return(character(0))
      
      indices <- sort(indices)
      gaps <- diff(indices) > 1
      group_starts <- c(TRUE, gaps)
      group_ends <- c(gaps, TRUE)
      
      sequences <- character(0)
      start_idx <- 1
      
      for(i in 1:length(indices)) {
        if(group_starts[i]) start_idx <- i
        if(group_ends[i]) {
          end_idx <- i
          if(start_idx == end_idx) {
            sequences <- c(sequences, as.character(indices[start_idx]))
          } else {
            sequences <- c(sequences, paste(indices[start_idx], "-", indices[end_idx]))
          }
        }
      }
      
      return(sequences)
    }
    
    # Show detailed information for each window
    for (i in 1:length(windows)) {
      window <- windows[[i]]
      
      cat("\nWindow", i, ":\n")
      
      # Print train ranges in desired format
      train_sequences <- find_sequences(window$train_indices)
      train_display <- paste(train_sequences, collapse=", ")
      cat("  Training indices:", train_display, "\n")
      cat("  Training size:", nrow(window$train), "rows\n")
      
      # Print test ranges in desired format
      test_sequences <- find_sequences(window$test_indices)
      if(length(test_sequences) > 1) {
        test_display <- paste(test_sequences, collapse=" & ")
      } else {
        test_display <- test_sequences
      }
      cat("  Testing indices:", test_display, "\n")
      cat("  Testing size:", nrow(window$test), "rows\n")
      
      # Check hold-out period exclusion
      if(any(window$train_indices %in% hold_out_period)) {
        cat("  WARNING: Hold-out period found in training indices\n")
      }
      if(any(window$test_indices %in% hold_out_period)) {
        cat("  WARNING: Hold-out period found in test indices\n")
      }
    }
  }
}

# Run the summary function
summarize_windows(rolling_splits)

# Check for any remaining NA values in the final dataset
check_for_remaining_nas <- function(windows_list) {
  cat("\n======= Checking for remaining NA values =======\n")
  
  for(dep_var_name in names(windows_list)) {
    has_na <- FALSE
    
    for(i in 1:length(windows_list[[dep_var_name]])) {
      window <- windows_list[[dep_var_name]][[i]]
      
      # Check training data
      if(any(is.na(window$train))) {
        has_na <- TRUE
        cat("\nFound NA values in", dep_var_name, "window", i, "training data\n")
      }
      
      # Check test data
      if(any(is.na(window$test))) {
        has_na <- TRUE
        cat("\nFound NA values in", dep_var_name, "window", i, "test data\n")
      }
    }
    
    if(!has_na) {
      cat("\n", dep_var_name, ": No remaining NA values in any window\n")
    }
  }
}

# Check for NA values
check_for_remaining_nas(rolling_splits)
```

```{r}
all_models_rwbw <- list()
########### 4 dep cars 
# Net Interest Income
all_models_rwbw$net_interest_income <- fit_all_windows(rolling_splits$net_interest_income)

# Non-Interest Income
all_models_rwbw$non_interest_income <- fit_all_windows(rolling_splits$non_interest_income)

# Provision Credit Loss
all_models_rwbw$provision_credit_loss <- fit_all_windows(rolling_splits$provision_credit_loss)

# Non-Interest Expense
all_models_rwbw$non_interest_expense <- fit_all_windows(rolling_splits$non_interest_expense)
```



## Error
```{r}
all_evaluations_rwbw <- list()
 for(dep_var_name in names(dep_var_datasets)) {
   cat("\nEvaluating", dep_var_name, "\n")
   all_evaluations_rwbw[[dep_var_name]] <- evaluate_all_windows(all_models_rwbw[[dep_var_name]], dep_var_datasets[[dep_var_name]])
 }
 print_summary_tables(all_evaluations_rwbw)
```

```{r}
# Calculate model weights
model_weights_rwbw <- calculate_model_weights(all_evaluations_rwbw)
```

```{r}
smape_results_holdout <- predict_all_rolling_windows_holdout(all_models_rwbw, hold_out_dataset)
```

```{r}
# Function to aggregate hold-out period predictions for all windows and calculate SMAPE for each dep var
aggregate_and_calculate_holdout_smape <- function(smape_results_holdout, all_modelewall, hold_out_dataset) {
  # Initialize results storage for aggregated SMAPE
  aggregate_smape_results <- list()
  
  # Loop through each dependent variable
  for (var_name in names(all_modelewall)) {
    cat("\n================================================================\n")
    cat("AGGREGATED HOLD-OUT PREDICTIONS AND SMAPE FOR", toupper(var_name), "\n")
    cat("================================================================\n")
    
    # Initialize vectors to store actual, mean predictions, and median predictions
    all_actuals <- numeric()
    all_mean_preds <- numeric()
    all_median_preds <- numeric()
    
    # Get the hold-out data for this variable
    holdout_data <- hold_out_dataset[[var_name]]
    actual_values <- holdout_data[, 1]
    pred_data <- as.data.frame(holdout_data[, -1])
    
    # Loop through each window to collect predictions
    for(window_num in 1:4) {
      # Get the model for this window and variable
      model <- all_modelewall[[var_name]][[window_num]]
      
      # Skip if model is missing
      if(is.null(model)) {
        cat("\nModel not available for", var_name, "window", window_num, ". Skipping.\n")
        next
      }
      
      # Make predictions on hold-out period
      pred <- predict(model, newdata = pred_data, burn = 100)
      mean_predictions <- colMeans(pred$distribution)
      median_predictions <- apply(pred$distribution, 2, median)
      
      # Add to the vectors
      all_actuals <- c(all_actuals, actual_values)
      all_mean_preds <- c(all_mean_preds, mean_predictions)
      all_median_preds <- c(all_median_preds, median_predictions)
    }
    
    # Calculate SMAPE for the aggregated predictions using mean method
    mean_smape <- mean(2 * abs(all_actuals - all_mean_preds) / 
                         (abs(all_actuals) + abs(all_mean_preds))) * 100
    
    # Calculate SMAPE for the aggregated predictions using median method
    median_smape <- mean(2 * abs(all_actuals - all_median_preds) / 
                           (abs(all_actuals) + abs(all_median_preds))) * 100
    
    # Store aggregated SMAPE results
    aggregate_smape_results[[var_name]] <- data.frame(
      Mean_SMAPE = mean_smape,
      Median_SMAPE = median_smape
    )
    
    # Display aggregated SMAPE results for this variable
    cat("\n----------------------------------------------------------------\n")
    cat("AGGREGATED HOLD-OUT SMAPE FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    cat("Mean SMAPE:", round(mean_smape, 4), "%\n")
    cat("Median SMAPE:", round(median_smape, 4), "%\n")
  }
  
  # Return the aggregated SMAPE results for all variables
  return(aggregate_smape_results)
}
# Run the function to calculate aggregated SMAPE for all dep vars
aggregate_smape_results <- aggregate_and_calculate_holdout_smape(smape_results_holdout, all_models_rwbw, hold_out_dataset)

# Display aggregated results
cat("\n================================================================\n")
cat("SUMMARY OF AGGREGATED HOLD-OUT SMAPE RESULTS FOR ALL VARIABLES\n")
cat("================================================================\n")
for (var_name in names(aggregate_smape_results)) {
  cat("\n----------------------------------------------------------------\n")
  cat("AGGREGATED HOLD-OUT SMAPE SUMMARY FOR", toupper(var_name), "\n")
  cat("----------------------------------------------------------------\n")
  print(round(aggregate_smape_results[[var_name]], 4))
}
```


# Summary stat
```{r}
all_diagnostics_rwbw <- list()
for(dep_var in names(all_models_rwbw)) {
  all_diagnostics_rwbw[[dep_var]] <- get_model_diagnostics(all_models_rwbw[[dep_var]], rolling_splits[[dep_var]])
}

print_all_diagnostics(all_diagnostics_rwbw)
```


## Function of getting LOOIC 

```{r}
calculate_looic <- function(model, window_data) {
  # Get training sample size 
  n_train <- nrow(window_data$train)
  
  # Calculate LOOIC
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  # normalized LOOIC
  looic <- (-2 * sum(loo_liks)) / n_train
  
  return(looic)
}

# Function to apply for all windows
get_all_looic <- function(models, windows) {
  n_windows <- length(models)
  looic_results <- numeric(n_windows)
  
  for (i in 1:n_windows) {
    looic_results[i] <- calculate_looic(models[[i]], windows[[i]])
  }
  
  return(looic_results)
}
all_looic_rwbw <- list()
for (dep_var in names(all_models_rwbw)) {
  all_looic_rwbw[[dep_var]] <- get_all_looic(
    all_models_rwbw[[dep_var]], 
    rolling_splits[[dep_var]]
  )
}
# Display results
for (dep_var in names(all_looic_rwbw)) {
  cat(sprintf("\nNormalized LOOIC Results for %s\n", dep_var))
  for (i in 1:length(all_looic_rwbw[[dep_var]])) {
    cat(sprintf("Window %d: %.4f\n", i, all_looic_rwbw[[dep_var]][i]))
  }
}
```

## 95% confi interval
```{r}
calculate_confidence_intervals <- function(all_models_rwbw, rolling_splits) {
  for(dep_var in names(all_models_rwbw)) {
    cat("\n95% Confidence Intervals for", dep_var, "\n")
    for(window in seq_along(all_models_rwbw[[dep_var]])) {
      cat("\nWindow", window, ":\n")
      
      model <- all_models_rwbw[[dep_var]][[window]]
      coefficients <- model$coefficients
      
      # Calculate statistics
      stats <- apply(coefficients, 2, function(x) {
        quantiles <- quantile(x, probs = c(0.025, 0.975))
        c("2.5%" = quantiles[1],
          "Mean" = mean(x),
          "Median" = median(x),
          "97.5%" = quantiles[2])
      })
      
      stats_df <- as.data.frame(t(stats))
      stats_df <- round(stats_df, 4)
      print(stats_df)
    }
  }
}

confidence_intervals_bw <- calculate_confidence_intervals(all_models_rwbw, rolling_splits)
```


## mcmc size 
```{r}
mcmc_stats_bw <- get_mcmc_stats(all_models_rwbw,rolling_splits)
```


## Bayesian R²  (Gelman et al.)

```{r}

## Loop over each dependent variable and each window to compute Bayesian R²
bayesian_R2_results_bw <- list()

for (dep_var_name in names(all_models_rwbw)) {
  models_list <- all_models_rwbw[[dep_var_name]]
  
  # Data frame to store summary metrics for each window
  bayes_R2_summary_bw <- data.frame(
    Window = integer(),
    Mean_R2 = numeric(),
    Median_R2 = numeric(),
    Lower_R2 = numeric(),
    Upper_R2 = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Loop through each of the 7 expanding windows
  for (i in 1:length(models_list)) {
    # Get the training data for the current window
    train_data <- rolling_splits[[dep_var_name]][[i]]$train
    
    # Extract the fitted model for this window
    model <- models_list[[i]]
    
    # Compute the Bayesian R² draws for the training set
    R2_draws <- calculate_bayes_R2(model, train_data, burn = 100)
    
    # Summarize the draws (you can adjust the summary metrics as desired)
    bayes_R2_summary_bw <- rbind(
      bayes_R2_summary,
      data.frame(
        Window = i,
        Mean_R2 = mean(R2_draws),
        Median_R2 = median(R2_draws),
        Lower_R2 = quantile(R2_draws, 0.025),
        Upper_R2 = quantile(R2_draws, 0.975)
      )
    )
  }
  
  # Store the summary for the current dependent variable
  bayesian_R2_results_bw[[dep_var_name]] <- bayes_R2_summary
}

## Print the Bayesian R² summary for each dependent variable and window
for (dep_var_name in names(bayesian_R2_results_bw)) {
  cat("\nBayesian R² for", dep_var_name, ":\n")
  print(round(bayesian_R2_results_bw[[dep_var_name]], 4))
}

```

## Ljung Box 
```{r}
# Calculate the Ljung-Box diagnostics (with default lag = 10)
ljung_box_results_bw <- get_ljung_box_diagnostics(all_models_rwbw, rolling_splits, lags = 10)

# Print the Ljung-Box test results for each dependent variable
for (dep_var in names(ljung_box_results_bw)) {
  cat("\nLjung-Box Test Diagnostics for", dep_var, ":\n")
  print(ljung_box_results_bw[[dep_var]])
}

```

## Posterior Prob
```{r}
pips_results_bw <- analyze_pips(all_models_rwbw, rolling_splits)
```

## Geweke’s Diagnostic
```{r}
# Apply 
geweke_results_bw <- calculate_geweke(all_models_rwbw, rolling_splits)
```

## Heidelberger-Welch
```{r}
for(dep_var in names(all_models_rwbw)) {
  cat("\n\n=== Results for", dep_var, "===\n")
  heidel_results_bw = calculate_heidel_welch(all_models_rwbw[[dep_var]], rolling_splits[[dep_var]])
}
```


## Raftery_lewis
```{r}
calculate_convergence_diagnostics <- function(model, window_data) {
  for(i in 1:length(model)) {
    cat("\nWindow", i, "\n")
    cat("-------------------\n")
    
    coef_matrix <- as.matrix(model[[i]]$coefficients)
    
    for(j in 1:ncol(coef_matrix)) {
      param_name <- colnames(coef_matrix)[j]
      chain <- as.vector(coef_matrix[,j])
      
      cat("\nParameter:", param_name, "\n")
      
      # Basic diagnostics
      n_unique <- length(unique(chain))
      autocorr <- cor(chain[-1], chain[-length(chain)])
      
      cat("Chain diagnostics:\n")
      cat("Unique values:", n_unique, "\n")
      cat("Mean:", mean(chain), "\n")
      cat("SD:", sd(chain), "\n")
      cat("Autocorrelation:", autocorr, "\n")
      
      if(n_unique < 10) {
        cat("WARNING: Too few unique values for reliable convergence diagnostics\n")
        next
      }
      
      chain_mcmc <- as.mcmc(chain)
      
      # Both Raftery-Lewis and Geweke
      tryCatch({
        # Raftery-Lewis
        binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
        rl <- raftery.diag(binary_chain)
        
        cat("\nRaftery-Lewis Diagnostic:\n")
        cat("Iterations needed:", rl$resmatrix[1,"N"], "\n")
        cat("Burn-in needed:", rl$resmatrix[1,"M"], "\n")
        cat("Dependence factor:", rl$resmatrix[1,"I"], "\n")
        
        # Geweke
        gw <- geweke.diag(chain_mcmc)
        pvalue <- 2 * (1 - pnorm(abs(gw$z)))  # Two-tailed p-value from z-score
        
        cat("\nGeweke Test:\n")
        cat("Z-score:", gw$z, "\n")
        cat("P-value:", pvalue, "\n")
        
        # Combined assessment
        if(rl$resmatrix[1,"I"] > 5 || pvalue < 0.05) {
          cat("\nConvergence Issues Detected:\n")
          if(rl$resmatrix[1,"I"] > 5) {
            cat("- High dependence factor in Raftery-Lewis\n")
          }
          if(pvalue < 0.05) {
            cat("- Significant Geweke test (p < 0.05)\n")
          }
          if(autocorr > 0.7) {
            cat("- High autocorrelation detected\n")
          }
          if(length(chain) < rl$resmatrix[1,"N"]) {
            cat("- More iterations needed\n")
            cat(sprintf("  Current: %d, Recommended: %d\n", 
                       length(chain), rl$resmatrix[1,"N"]))
          }
        } else {
          cat("\nNo convergence issues detected\n")
        }
        
      }, error = function(e) {
        cat("\nDiagnostic calculation failed:\n")
        cat("- Error:", conditionMessage(e), "\n")
        if(autocorr > 0.7) {
          cat("- High autocorrelation might be causing issues\n")
        }
      })
    }
  }
}

# Run diagnostics
for(dep_var in names(all_models_rwbw)) {
  cat("\n\n=== Convergence Diagnostics for", dep_var, "===\n")
  calculate_convergence_diagnostics(all_models_rwbw[[dep_var]], rolling_splits[[dep_var]])
}
```
# Predicitive interval
```{r}

# Apply the function with rolling window splits
get_predictive_intervals(
  all_evaluations_rwbw, 
  all_models_rwbw, 
  extended_test_sets_rolling
)

```

## MCMC trace plot 
```{r}
## MCMC trace plot for Rolling Window Analysis
# Function to find top 2 best models based on evaluations (Rolling Window)
find_best_models_rolling <- function(all_evaluations) {
  best_models <- list()  # To store the best models for each dependent variable
  
  for(var_name in names(all_evaluations)) {
    cat("\nFor", var_name, ":\n")
    
    # Collect all sMAPE values
    all_smape_results <- data.frame(
      Window = integer(),
      Method = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Collect mean-based sMAPE values
    for(i in 1:4) {  
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "mean",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: mean_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Collect median-based sMAPE values
    for(i in 1:4) {  # Assuming 6 possible window sizes
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "median",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: median_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Sort by sMAPE (ascending) to find the best models overall
    all_smape_results <- all_smape_results[order(all_smape_results$SMAPE), ]
    
    if (nrow(all_smape_results) >= 2) {
      # Get top 2 models
      top_2_models <- all_smape_results[1:2, ]
      
      cat("Top 2 models for", var_name, "based on sMAPE:\n")
      for (i in 1:2) {
        cat("Rank", i, ": Window", top_2_models$Window[i], "using", top_2_models$Method[i], 
            "method with sMAPE =", round(top_2_models$SMAPE[i], 4), "\n")
      }
      
      # Store the top 2 models info
      best_models[[var_name]] <- list(
        best_window_1 = top_2_models$Window[1],
        best_method_1 = top_2_models$Method[1],
        best_smape_1 = top_2_models$SMAPE[1],
        best_window_2 = top_2_models$Window[2],
        best_method_2 = top_2_models$Method[2],
        best_smape_2 = top_2_models$SMAPE[2]
      )
    } else if (nrow(all_smape_results) == 1) {
      # Handle case where only one valid model is found
      cat("Only one valid model found for", var_name, ":\n")
      cat("Window:", all_smape_results$Window[1], "using", all_smape_results$Method[1], 
          "method with sMAPE =", round(all_smape_results$SMAPE[1], 4), "\n")
      
      best_models[[var_name]] <- list(
        best_window_1 = all_smape_results$Window[1],
        best_method_1 = all_smape_results$Method[1],
        best_smape_1 = all_smape_results$SMAPE[1]
      )
    } else {
      cat("No valid models found for", var_name, "\n")
    }
  }
  
  return(best_models)
}

# Analyze MCMC trace plots for Rolling Window
analyze_bsts_mcmc_trace_plots_rolling <- function(all_evaluations, all_models, best_models) {
  # Create a PDF device to save all plots
  pdf("MCMC_trace_plot_RW_BW_2016Q4.pdf", width=12, height=10)
  
  for(dep_var in names(best_models)) {
    # Get the info for the top models
    best_model_info <- best_models[[dep_var]]
    
    # Process Rank 1 model
    window_1 <- best_model_info$best_window_1
    method_1 <- best_model_info$best_method_1
    smape_1 <- best_model_info$best_smape_1
    
    cat("\nCreating MCMC trace plots for", dep_var, ":\n")
    cat("Rank 1: Window:", window_1, "using", method_1, "method, sMAPE =", round(smape_1, 4), "\n")
    
    # Fetch the model
    model_1 <- all_models[[dep_var]][[window_1]]
    
    # Create trace plots for Rank 1 model
    create_trace_plots(model_1, dep_var, "Rank 1", window_1, method_1, smape_1)
    
    # Process Rank 2 model if available
    if(!is.null(best_model_info$best_window_2)) {
      window_2 <- best_model_info$best_window_2
      method_2 <- best_model_info$best_method_2
      smape_2 <- best_model_info$best_smape_2
      
      cat("Rank 2: Window:", window_2, "using", method_2, "method, sMAPE =", round(smape_2, 4), "\n")
      
      # Fetch the second-ranked model
      model_2 <- all_models[[dep_var]][[window_2]]
      
      # Create trace plots for Rank 2 model
      create_trace_plots(model_2, dep_var, "Rank 2", window_2, method_2, smape_2)
    } else {
      cat("No second-best model available for", dep_var, "\n")
    }
  }
  
  dev.off()
}

# Helper function to create trace plots for a model (unchanged from previous version)
create_trace_plots <- function(model, dep_var, rank_label, window, method, smape) {
  tryCatch({
    # Set up plot for state variances
    par(mfrow=c(3,1), mar=c(4,4,3,1))
    
    # Plot observation variance
    if (!is.null(model$sigma.obs)) {
      plot(1:length(model$sigma.obs), model$sigma.obs, type="l", col="blue",
           main="Observation Variance (sigma.obs)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot seasonal variance if it exists
    if (!is.null(model$sigma.seasonal.4)) {
      plot(1:length(model$sigma.seasonal.4), model$sigma.seasonal.4, type="l", col="red",
           main="Seasonal Variance (sigma.seasonal.4)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot trend level variance if it exists
    if (!is.null(model$sigma.trend.level)) {
      plot(1:length(model$sigma.trend.level), model$sigma.trend.level, type="l", col="green",
           main="Trend Level Variance (sigma.trend.level)",
           xlab="Iteration", ylab="Value")
    }
    
    mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Variances"), 
          outer=TRUE, line=-1.5, cex=1.2)
    
    # If the model has regression coefficients, plot those on a new page
    if (!is.null(model$coefficients) && !is.null(model$coefficients.samples) && 
        ncol(model$coefficients.samples) > 0) {
      
      # Calculate number of coefficient plots needed
      num_coefs <- ncol(model$coefficients.samples)
      rows_needed <- min(4, num_coefs)  # Maximum 4 coefficients per page
      
      # Start a new page
      par(mfrow=c(rows_needed, 1), mar=c(4,4,3,1))
      
      # Plot each coefficient
      for (i in 1:rows_needed) {
        coef_name <- colnames(model$coefficients.samples)[i]
        if (is.null(coef_name)) coef_name <- paste("Coefficient", i)
        
        coef_samples <- model$coefficients.samples[,i]
        plot(1:length(coef_samples), coef_samples, type="l", col="purple",
             main=paste("Regression Coefficient:", coef_name),
             xlab="Iteration", ylab="Value")
      }
      
      mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Coefficients"), 
            outer=TRUE, line=-1.5, cex=1.2)
    }
    
  }, error = function(e) {
    # If error occurs, print the error and try the built-in plotting function
    cat("Error in custom trace plotting for", rank_label, ":", e$message, "\n")
    cat("Falling back to built-in plotting...\n")
    
    # Reset the plot area
    par(mfrow=c(1,1))
    
    # Try the built-in plotting method
    tryCatch({
      # Try plotting state components instead
      plot(model, "components")
      title(main=paste0(dep_var, " (", rank_label, "): Component Contributions"))
    }, error = function(e2) {
      cat("Error in fallback plotting:", e2$message, "\n")
      
      # Create an empty plot with error message
      plot(1, 1, type="n", axes=FALSE, xlab="", ylab="")
      text(1, 1, paste("Error plotting MCMC traces for", rank_label), col="red", cex=1.5)
    })
  })
}

# Run the analysis for rolling window setting
best_models_rolling <- find_best_models_rolling(all_evaluations_rwbw)
analyze_bsts_mcmc_trace_plots_rolling(all_evaluations_rwbw, all_models_rwbw, best_models_rolling)
```

## Distribution Plot
```{r}
create_prediction_plots <- function(all_evaluations, all_models, extended_test_sets_rolling, hold_out_dataset) {
  library(ggplot2)
  pdf("Prediction_Distribution_RW_BW_2016Q4.pdf", width = 12, height = 8)
  
  for(dep_var in names(all_evaluations)) {
    # Create a data frame of all SMAPE values (mean and median) per window
    all_results <- data.frame(
      Window = numeric(),
      Type = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    for(i in 1:length(all_evaluations[[dep_var]])) {
      # Add mean results
      mean_smape <- all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE
      if(!is.na(mean_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "mean", SMAPE = mean_smape, stringsAsFactors = FALSE))
      }
      # Add median results
      median_smape <- all_evaluations[[dep_var]][[i]]$errors_median$SMAPE
      if(!is.na(median_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "median", SMAPE = median_smape, stringsAsFactors = FALSE))
      }
    }
    
    # Sort by SMAPE (ascending) and select the top 4 rows
    top_4 <- all_results[order(all_results$SMAPE), ][1:4, ]
    
    plot_data <- data.frame()
    label_list <- c()  # to track labels and catch duplicates
    
    for(i in 1:nrow(top_4)) {
      window <- top_4$Window[i]
      model_type <- top_4$Type[i]
      smape_value <- top_4$SMAPE[i]
      
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(extended_test_sets_rolling[[dep_var]][[window]][nrow(extended_test_sets_rolling[[dep_var]][[window]]), -1, drop = FALSE]),
                           burn = 100)
      
      # Create a label that includes rank, window, type, and SMAPE.
      base_label <- paste("Rank", i, ": Window", window, "-", model_type, "(SMAPE:", round(smape_value, 4), ")")
      # If the same label already exists, append a suffix.
      duplicate_count <- sum(label_list == base_label)
      if(duplicate_count > 0) {
        label <- paste0(base_label, " - Copy", duplicate_count + 1)
      } else {
        label <- base_label
      }
      label_list <- c(label_list, label)
      
      plot_data <- rbind(plot_data, 
                         data.frame(Value = pred$distribution[,1],
                                    Model = factor(label),
                                    stringsAsFactors = FALSE))
    }
    
    # Get the actual observed value from the hold_out_dataset (assume it's the last value in column 1)
    actual_value <- tail(hold_out_dataset[[dep_var]][, 1], 1)
    
    p <- ggplot(plot_data, aes(x = Value, fill = Model)) +
      geom_density(alpha = 0.4) +
      theme_minimal() +
      labs(title = paste(dep_var, "- Top 4 Models Based on sMAPE"),
           x = "Predicted Value",
           y = "Density") +
      geom_vline(xintercept = actual_value, color = "red", linetype = "dashed", size = 1) +
      annotate("text", x = actual_value, y = Inf, label = paste("Actual:", actual_value),
               vjust = -0.5, color = "red", size = 3)
    
    print(p)
  }
  
  dev.off()
}


# Apply the function
create_prediction_plots(all_evaluations_rwbw, all_models_rwbw, extended_test_sets_rolling, hold_out_dataset)
```

```{r}
crps_scores_custom_rolling <- get_crps_scores_custom_rolling(all_models_rwbw, Indice_testing_dataset)
```

## log predictive density
```{r}
lpd_scores_bw <- calculate_lpd(all_models_rwbw, rolling_splits)
```

## Sensitivity 
```{r}
# First, clean up any existing parallel connections
try(stopImplicitCluster(), silent = TRUE)
try(stopCluster(cl), silent = TRUE)
closeAllConnections()
gc()  # Force garbage collection

# Load required packages
library(bsts)
library(parallel)

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to identify the best models
get_best_models_safe <- function() {
  best_predictions <- list()
  
  for (dep_var in names(all_models_rwbw)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:6) {
      if (!dep_var %in% names(all_models_rwbw) || length(all_models_rwbw[[dep_var]]) < i) {
        next
      }
      
      tryCatch({
        model <- all_models_rwbw[[dep_var]][[i]]
        
        # Get the testing data for this window and variable
        test_data <- rolling_splits[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        mape_mean <- mean(abs((actual_values - pred_mean) / actual_values)) * 100
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                          (abs(actual_values) + abs(pred_mean))) * 100
        
        mape_median <- mean(abs((actual_values - pred_median) / actual_values)) * 100
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                            (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to results
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = mape_mean, SMAPE = smape_mean),
                       data.frame(Window = i, Method = "median", 
                                MAPE = mape_median, SMAPE = smape_median))
      }, error = function(e) {
        cat("Error processing", dep_var, "window", i, ":", e$message, "\n")
      })
    }
    
    # Select the best five models
    if (nrow(results) > 0) {
      best_five <- results[order(results$MAPE), ][1:min(5, nrow(results)), ]
      best_predictions[[dep_var]] <- best_five
    }
  }
  
  return(best_predictions)
}

# Function to analyze one configuration
analyze_config <- function(params) {
  dep_var <- params$dep_var
  window_size <- params$window_size
  method <- params$method
  sigma <- params$sigma
  slab_var <- params$slab_var
  
  cat("Processing", dep_var, "window", window_size, "method", method, 
      "sigma", sigma, "slab_var", slab_var, "\n")
  
  # Get training and testing data
  train_data <- rolling_splits[[dep_var]][[window_size]]$train
  test_data <- rolling_splits[[dep_var]][[window_size]]$test
  
  # Extract values
  y <- train_data[, 1]
  X <- as.data.frame(train_data[, -1])
  actual_values <- test_data[, 1]
  test_predictors <- as.data.frame(test_data[, -1])
  
  # Create formula
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Full data for model
  model_data <- as.data.frame(cbind(y = y, X))
  
  # Create state specification
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y, 
                           level.sigma.prior = SdPrior(sigma = sigma),
                           slope.sigma.prior = SdPrior(sigma = sigma))
  
  # Fit model
  result <- tryCatch({
    # Using the approach that worked in our test
    model <- bsts(formula, 
                 state.specification = ss,
                 niter = 10000,
                 data = model_data)
    
    # Make predictions
    all_predictions <- numeric(length(actual_values))
    
    for (t in 1:length(actual_values)) {
      # Prepare data for this period
      pred_data <- data.frame(y = NA)
      pred_data <- cbind(pred_data, test_predictors[t, , drop = FALSE])
      
      # Predict
      single_pred <- predict(model, newdata = pred_data, burn = 100)
      
      # Extract prediction
      if (method == "mean") {
        all_predictions[t] <- mean(single_pred$distribution[, 1])
      } else {
        all_predictions[t] <- median(single_pred$distribution[, 1])
      }
    }
    
    # Check if predictions vary
    has_varying_predictions <- length(unique(round(all_predictions, 2))) > 1
    
    # Calculate metrics
    mape <- mean(abs((actual_values - all_predictions) / actual_values)) * 100
    smape <- mean(2 * abs(actual_values - all_predictions) / 
                 (abs(actual_values) + abs(all_predictions))) * 100
    
    waic <- NA
    looic <- NA
    if (!is.null(model$log.likelihood)) {
      waic <- -2 * mean(model$log.likelihood)
      looic <- -2 * mean(model$log.likelihood)
    }
    
    return(list(
      success = TRUE,
      sigma = sigma,
      slab_var = slab_var,
      mape = mape,
      smape = smape,
      waic = waic,
      looic = looic,
      has_regression = has_varying_predictions
    ))
    
  }, error = function(e) {
    return(list(
      success = FALSE,
      error = e$message
    ))
  })
  
  return(result)
}

# Controlled parallel approach
run_sensitivity_analysis_parallel <- function() {
  # Define parameter combinations
  sigmas <- c(0.2, 0.4, 0.6, 0.8)
  slab_vars <- c(50, 100, 200)
  
  # Get best models
  best_models <- get_best_models_safe()
  
  # Initialize results
  sensitivity_results <- list()
  
  # For each dependent variable
  for (dep_var in names(best_models)) {
    cat("\n=== Starting analysis for", dep_var, "===\n")
    sensitivity_results[[dep_var]] <- list()
    models_info <- best_models[[dep_var]]
    
    # For each top model
    for (i in 1:min(5, nrow(models_info))) {
      model_info <- models_info[i, ]
      window_size <- model_info$Window
      method <- as.character(model_info$Method)
      model_name <- paste0("model_", i)
      
      cat("\n--- Processing", dep_var, "window", window_size, "method", method, "---\n")
      
      # Initialize results for this model
      model_results <- list()
      
      # Create parameter combinations
      param_list <- list()
      for (sigma in sigmas) {
        for (slab_var in slab_vars) {
          param_list[[length(param_list) + 1]] <- list(
            dep_var = dep_var,
            window_size = window_size,
            method = method,
            sigma = sigma,
            slab_var = slab_var
          )
        }
      }
      
      # Create a small cluster - use just 2 or 3 cores for stability
      num_cores <- min(3, detectCores() - 1)
      cat("Using", num_cores, "cores for parallel processing\n")
      cl <- makeCluster(num_cores)
      
      # Export required data and functions
      clusterExport(cl, c("rolling_splits"), envir = .GlobalEnv)
      clusterEvalQ(cl, {
        library(bsts)
      })
      
      # Run analysis in parallel
      config_results <- parLapply(cl, param_list, analyze_config)
      
      # Clean up
      stopCluster(cl)
      
      # Process results
      for (j in 1:length(param_list)) {
        params <- param_list[[j]]
        result <- config_results[[j]]
        
        config_name <- paste0("sigma_", params$sigma, "_slab_", params$slab_var)
        
        if (result$success) {
          # Store successful result
          model_results[[config_name]] <- list(
            sigma = params$sigma,
            slab_var = params$slab_var,
            mape = result$mape,
            smape = result$smape,
            waic = result$waic,
            looic = result$looic,
            has_regression = result$has_regression
          )
          
          cat("Configuration", config_name, "- MAPE:", round(result$mape, 4),
             "SMAPE:", round(result$smape, 4), "Has regression:", result$has_regression, "\n")
        } else {
          cat("Configuration", config_name, "failed:", result$error, "\n")
        }
      }
      
      # Store results for this model
      sensitivity_results[[dep_var]][[model_name]] <- list(
        window = window_size,
        method = method,
        results = model_results
      )
      
      # Find best configuration
      best_config <- NULL
      best_mape <- Inf
      best_config_name <- ""
      
      for (config_name in names(model_results)) {
        config <- model_results[[config_name]]
        if (!is.na(config$mape) && config$mape < best_mape) {
          best_mape <- config$mape
          best_config <- config
          best_config_name <- config_name
        }
      }
      
      if (is.null(best_config)) {
        cat("No valid configurations found.\n")
      } else {
        # Print best configuration
        cat("\nBest configuration:", best_config_name, "\n")
        cat("MAPE:", round(best_config$mape, 4), "\n")
        cat("SMAPE:", round(best_config$smape, 4), "\n")
        cat("Sigma:", best_config$sigma, "\n")
        cat("Slab variance:", best_config$slab_var, "\n")
        cat("Has regression:", best_config$has_regression, "\n\n")
        
        # Create summary table
        config_summary <- data.frame(
          Sigma = numeric(),
          SlabVar = numeric(),
          MAPE = numeric(),
          SMAPE = numeric(),
          WAIC = numeric(),
          LOOIC = numeric(),
          HasRegression = logical()
        )
        
        for (config_name in names(model_results)) {
          config <- model_results[[config_name]]
          config_summary <- rbind(config_summary, 
                                data.frame(
                                  Sigma = config$sigma,
                                  SlabVar = config$slab_var,
                                  MAPE = config$mape,
                                  SMAPE = config$smape,
                                  WAIC = config$waic,
                                  LOOIC = config$looic,
                                  HasRegression = config$has_regression
                                ))
        }
        
        # Set row names and sort
        rownames(config_summary) <- names(model_results)
        config_summary <- config_summary[order(config_summary$MAPE), ]
        print(round(config_summary, 4))
      }
      
      # Clean up after each model
      rm(model_results, config_results)
      gc()
    }
  }
  
  return(sensitivity_results)
}

# Run the analysis
start_time <- Sys.time()
cat("Starting sensitivity analysis at:", format(start_time), "\n")

sensitivity_results <- run_sensitivity_analysis_parallel()

end_time <- Sys.time()
execution_time <- end_time - start_time
cat("\nSensitivity analysis completed in:", format(execution_time), "\n")
```

```{r}
# Function to identify the top 5 models by SMAPE
find_top_models <- function(all_models_rwbw, sensitivity_results) {
  top_models_summary <- list()
  
  # Process each dependent variable
  for(dep_var in names(sensitivity_results)) {
    cat("\n================================================================\n")
    cat("TOP 5 MODELS FOR", toupper(dep_var), "BASED ON SMAPE\n")
    cat("================================================================\n")
    
    # 1. Collect all model results including window performance and sensitivity analysis
    all_models <- data.frame(
      Source = character(),
      Window = integer(),
      Method = character(),
      Sigma = numeric(),
      SlabVar = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Add window performance results (from best_models)
    for (i in 1:6) { # Assuming 6 windows
      if (dep_var %in% names(all_models_rwbw) && length(all_models_rwbw[[dep_var]]) >= i) {
        # Get the testing data for this window and variable
        test_data <- rolling_splits[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(all_models_rwbw[[dep_var]][[i]], newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate SMAPE for mean
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                         (abs(actual_values) + abs(pred_mean))) * 100
        
        # Calculate SMAPE for median
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                           (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to all_models dataframe
        all_models <- rbind(all_models, 
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "mean",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_mean,
                            stringsAsFactors = FALSE
                          ),
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "median",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_median,
                            stringsAsFactors = FALSE
                          ))
      }
    }
    
    # Add sensitivity analysis results
    if (dep_var %in% names(sensitivity_results)) {
      for (model_name in names(sensitivity_results[[dep_var]])) {
        model_data <- sensitivity_results[[dep_var]][[model_name]]
        window <- model_data$window
        method <- model_data$method
        
        for (config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_models <- rbind(all_models,
                            data.frame(
                              Source = "Sensitivity",
                              Window = window,
                              Method = method,
                              Sigma = config$sigma,
                              SlabVar = config$slab_var,
                              SMAPE = config$smape,
                              stringsAsFactors = FALSE
                            ))
        }
      }
    }
    
    # 2. Sort by SMAPE and select top 5 models
    top_models <- all_models[order(all_models$SMAPE), ][1:min(5, nrow(all_models)), ]
    
    # 3. Create a formatted output
    formatted_models <- data.frame(
      Rank = 1:nrow(top_models),
      Description = character(nrow(top_models)),
      SMAPE = top_models$SMAPE,
      stringsAsFactors = FALSE
    )
    
    for (i in 1:nrow(top_models)) {
      model <- top_models[i, ]
      
      if (model$Source == "Window") {
        desc <- sprintf("%s Window %d", model$Method, model$Window)
      } else {
        desc <- sprintf("%s Window %d (sigma=%.1f, slab=%.0f)", 
                      model$Method, model$Window, model$Sigma, model$SlabVar)
      }
      
      formatted_models$Description[i] <- desc
    }
    
    # Display the top models
    print(formatted_models)
    
    # Store the results
    top_models_summary[[dep_var]] <- list(
      top_models = top_models,
      formatted_output = formatted_models
    )
  }
  
  return(top_models_summary)
}

# Example usage:
 top_model_results <- find_top_models(all_models_rwbw, sensitivity_results)
```

```{r}
# Function to predict holdout period using top 5 models and create weighted ensemble
predict_and_create_ensemble <- function(all_models_rwbw, top_model_results, hold_out_dataset) {
  # Clean up any existing connections
  try(stopImplicitCluster(), silent = TRUE)
  try(stopCluster(cl), silent = TRUE)
  closeAllConnections()
  gc()  # Force garbage collection
  
  # Load required packages
  library(parallel)
  
  # To store all results
  all_predictions <- list()
  
  # For each dependent variable
  for(dep_var in names(top_model_results)) {
    cat("\n================================================================\n")
    cat("HOLDOUT PREDICTIONS FOR", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Get top models for this variable
    top_models <- top_model_results[[dep_var]]$top_models
    
    # Get holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      cat("Holdout data not available for", dep_var, "\n")
      next
    }
    
    holdout_data <- hold_out_dataset[[dep_var]]
    
    # Prepare parameters for parallel execution
    param_list <- list()
    for(i in 1:nrow(top_models)) {
      param_list[[i]] <- list(
        dep_var = dep_var,
        model_info = top_models[i, ],
        model_index = i
      )
    }
    
    # Create a small cluster - use just 2 or 3 cores for stability
    num_cores <- min(3, detectCores() - 1)
    cat("Using", num_cores, "cores for parallel processing\n")
    cl <- makeCluster(num_cores)
    
    # Export required data and functions
    clusterExport(cl, c("all_models_rwbw", "hold_out_dataset", "rolling_splits"), envir = .GlobalEnv)
    clusterEvalQ(cl, {
      library(bsts)
    })
    
    # Worker function for parallel execution
    predict_model_worker <- function(params) {
      dep_var <- params$dep_var
      model_info <- params$model_info
      model_index <- params$model_index
      
      # Create result container
      result <- list(
        model_index = model_index,
        dep_var = dep_var,
        success = FALSE
      )
      
      # Get the holdout data
      holdout_data <- hold_out_dataset[[dep_var]]
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Create period labels
      period_labels <- c(as.character(80:86), "Q4")#period
      
      # Create model description
      if(model_info$Source == "Window") {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, ")")
      } else {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, 
                          ", sigma=", model_info$Sigma, 
                          ", slab=", model_info$SlabVar, ")")
      }
      
      tryCatch({
        # Get or create model based on source
        if(model_info$Source == "Window") {
          # Use existing model
          window <- model_info$Window
          method <- as.character(model_info$Method)
          
          if(!(dep_var %in% names(all_models_rwbw)) || length(all_models_rwbw[[dep_var]]) < window) {
            return(c(result, list(error = "Model not available")))
          }
          
          model <- all_models_rwbw[[dep_var]][[window]]
          
        } else {
          # Create model with sensitivity parameters
          window <- model_info$Window
          method <- as.character(model_info$Method)
          sigma <- model_info$Sigma
          slab_var <- model_info$SlabVar
          
          # Get original model
          if(!(dep_var %in% names(all_models_rwbw)) || length(all_models_rwbw[[dep_var]]) < window) {
            return(c(result, list(error = "Original model not available")))
          }
          
          original_model <- all_models_rwbw[[dep_var]][[window]]
          
          # Get training data
          train_data <- rolling_splits[[dep_var]][[window]]$train
          y <- train_data[, 1]
          X <- as.data.frame(train_data[, -1])
          
          # Create formula
          predictors <- colnames(X)
          formula_str <- paste("y ~", paste(predictors, collapse = " + "))
          formula <- as.formula(formula_str)
          
          # Create model data
          model_data <- as.data.frame(cbind(y = y, X))
          
          # Set up state specification
          ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
          ss <- AddLocalLinearTrend(ss, y, 
                                  level.sigma.prior = SdPrior(sigma = sigma),
                                  slope.sigma.prior = SdPrior(sigma = sigma))
          
          # Create model
          model <- bsts(formula, 
                       state.specification = ss,
                       niter = 10000,
                       data = model_data)
        }
        
        # Make predictions - using individual predictions for each period
        predictions <- numeric(nrow(holdout_data))
        
        for(t in 1:nrow(holdout_data)) {
          # Create data for this period
          if(model_info$Source == "Window") {
            # For window models
            this_period_data <- pred_data[t, , drop = FALSE]
            
            # Make prediction
            single_pred <- predict(model, newdata = this_period_data, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          } else {
            # For sensitivity models
            pred_row <- data.frame(y = NA)
            pred_row <- cbind(pred_row, pred_data[t, , drop = FALSE])
            
            # Make prediction
            single_pred <- predict(model, newdata = pred_row, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          }
        }
        
        # Calculate errors
        abs_errors <- abs(predictions - actual_values)
        avg_abs_error <- mean(abs_errors)
        
        # Return successful result
        return(list(
          model_index = model_index,
          dep_var = dep_var,
          success = TRUE,
          model_desc = model_desc,
          predictions = predictions,
          actual_values = actual_values,
          abs_errors = abs_errors,
          avg_abs_error = avg_abs_error,
          period_labels = period_labels,
          smape = model_info$SMAPE
        ))
        
      }, error = function(e) {
        return(c(result, list(error = e$message)))
      })
    }
    
    # Run predictions in parallel
    model_results <- parLapply(cl, param_list, predict_model_worker)
    
    # Stop cluster
    stopCluster(cl)
    
    # Process and display results
    var_predictions <- list()
    
    for(result in model_results) {
      if(result$success) {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, ":", result$model_desc, "\n")
        cat("----------------------------------------------------------------\n")
        
        # Print results in a tabular format
        cat("\nPeriod\tActual\t\tPredicted\tAbsError\n")
        for(p in 1:length(result$period_labels)) {
          cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                    result$period_labels[p], 
                    result$actual_values[p], 
                    result$predictions[p], 
                    result$abs_errors[p]))
        }
        
        cat("\nAverage Absolute Error:", round(result$avg_abs_error, 2), "\n")
        
        # Store successful predictions
        var_predictions[[result$model_index]] <- result
      } else {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, "failed:", result$error, "\n")
        cat("----------------------------------------------------------------\n")
      }
    }
    
    # Store predictions for this variable
    all_predictions[[dep_var]] <- var_predictions
    
    # Now create weighted ensemble
    successful_models <- model_results[sapply(model_results, function(r) r$success)]
    
    if(length(successful_models) > 0) {
      # Get the actual values and period labels from the first successful model
      actual_values <- successful_models[[1]]$actual_values
      period_labels <- successful_models[[1]]$period_labels
      
      # Create prediction matrix
      pred_matrix <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      smape_values <- numeric(length(successful_models))
      model_descriptions <- character(length(successful_models))
      
      # Fill the matrix with predictions
      for(i in 1:length(successful_models)) {
        pred_matrix[, i] <- successful_models[[i]]$predictions
        smape_values[i] <- successful_models[[i]]$smape
        model_descriptions[i] <- successful_models[[i]]$model_desc
      }
      
      # Calculate weights based on inverse SMAPE
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      
      # Display the weights
      cat("\n================================================================\n")
      cat("WEIGHTED ENSEMBLE FOR", toupper(dep_var), "\n")
      cat("================================================================\n")
      
      cat("\nModel Weights:\n")
      for(i in 1:length(weights)) {
        cat(sprintf("Model %d: %s - Weight: %.4f (SMAPE: %.4f)\n", 
                  i, model_descriptions[i], weights[i], smape_values[i]))
      }
      
      # Calculate weighted predictions
      weighted_predictions <- numeric(length(actual_values))
      for(i in 1:length(actual_values)) {
        weighted_predictions[i] <- sum(pred_matrix[i, ] * weights, na.rm = TRUE)
      }
      
      # Calculate errors
      abs_errors <- abs(weighted_predictions - actual_values)
      
      # Calculate ensemble SMAPE
      ensemble_smape <- mean(2 * abs(actual_values - weighted_predictions) / 
                           (abs(actual_values) + abs(weighted_predictions))) * 100
      
      # Display results
      cat("\n----------------------------------------------------------------\n")
      cat("WEIGHTED ENSEMBLE RESULTS\n")
      cat("----------------------------------------------------------------\n")
      cat("Ensemble SMAPE:", round(ensemble_smape, 4), "%\n\n")
      
      cat("Period\tActual\t\tPredicted\tAbsError\n")
      for(p in 1:length(period_labels)) {
        cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                  period_labels[p], 
                  actual_values[p], 
                  weighted_predictions[p], 
                  abs_errors[p]))
      }
      
      cat("\nAverage Absolute Error:", round(mean(abs_errors), 2), "\n")
    } else {
      cat("\nNo successful models for", dep_var, "- cannot create ensemble\n")
    }
  }
  
  # Return all predictions
  return(all_predictions)
}

# Run the function to predict and create weighted ensemble
all_results <- predict_and_create_ensemble(all_models_rwbw, top_model_results, hold_out_dataset)
```

```{r}
# This function replicates your 'print_ensemble_tables' logic but returns a named vector
# of Weighted Ensemble SMAPEs (one entry per dep_var).
get_ensemble_smapes <- function(all_results, top_model_results, hold_out_dataset) {
  ensemble_smapes <- numeric(0)  # named vector
  
  for(dep_var in names(top_model_results)) {
    # Skip if no holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      next
    }
    holdout_data  <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    
    # Filter out NULL models
    successful_models <- all_results[[dep_var]]
    successful_models <- successful_models[!sapply(successful_models, is.null)]
    
    if(length(successful_models) > 0) {
      smape_values <- numeric(length(successful_models))
      pred_matrix  <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      
      for(i in seq_along(successful_models)) {
        pred_matrix[, i]  <- successful_models[[i]]$predictions
        smape_values[i]   <- successful_models[[i]]$smape
      }
      
      # Weighted predictions
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      weighted_predictions <- rowSums(t(t(pred_matrix) * weights), na.rm = TRUE)
      
      # Weighted Ensemble SMAPE
      ensemble_smape <- mean(
        2 * abs(actual_values - weighted_predictions) /
        (abs(actual_values) + abs(weighted_predictions))
      ) * 100
      
      # Store in named vector
      ensemble_smapes[dep_var] <- ensemble_smape
    }
  }
  
  return(ensemble_smapes)
}
ensemble_smapes = get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
```

## Influence 
```{r}
if (!requireNamespace("foreach", quietly = TRUE)) {
  install.packages("foreach")
}
if (!requireNamespace("doParallel", quietly = TRUE)) {
  install.packages("doParallel")
}


# Register parallel backend
cores <- detectCores() - 1  # Leave one core free for system processes
registerDoParallel(cores)
cat("Using", cores, "cores for parallel processing\n")

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to get window data
get_window_data <- function(dep_var, window_size) {
  window_data <- rolling_splits[[dep_var]][[window_size]]$train
  return(window_data)
}

# Function to get the best models based on testing dataset performance (changed from holdout)
get_best_models_test <- function(all_models) {
  best_predictions <- list()
  
  for (dep_var in names(all_models)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:length(all_models[[dep_var]])) {
      model <- all_models[[dep_var]][[i]]
      
      # Get the testing data for this window and variable
      test_data <- rolling_splits[[dep_var]][[i]]$test
      actual_values <- test_data[, 1]
      test_predictors <- as.data.frame(test_data[, -1])
      
      # Make predictions for testing period
      tryCatch({
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        errors_mean <- calculate_errors(actual_values, pred_mean)
        errors_median <- calculate_errors(actual_values, pred_median)
        
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = errors_mean$MAPE, SMAPE = errors_mean$SMAPE),
                       data.frame(Window = i, Method = "median", 
                                MAPE = errors_median$MAPE, SMAPE = errors_median$SMAPE))
      }, error = function(e) {
        cat("Error predicting for", dep_var, "window", i, ":", conditionMessage(e), "\n")
      })
    }
    
    # If we have results, select the best two models based on MAPE
    if (nrow(results) > 0) {
      best_two <- results[order(results$MAPE), ][1:min(2, nrow(results)), ]
      best_predictions[[dep_var]] <- best_two
    } else {
      cat("No valid predictions for", dep_var, "- skipping\n")
    }
  }
  
  # Print best models
  cat("\n=== Best Models for Testing Period ===\n")
  for(dep_var in names(best_predictions)) {
    cat("\n", dep_var, ":\n")
    print(best_predictions[[dep_var]])
  }
  
  return(best_predictions)
}

# FIXED Function to calculate influence measures with LOOIC and LPD
calculate_influence_test <- function(model, train_data, method, dep_var, window_size) {
  # Get original training data
  y <- train_data[, 1]
  X <- train_data[, -1]
  n_obs <- length(y)
  
  # Create model matrix for X
  X_matrix <- model.matrix(~ ., data = as.data.frame(X))[, -1]
  
  # Create time labels starting from 1997 Q4
  start_year <- 1997
  time_labels <- paste0(rep(start_year:(start_year + floor(n_obs/4)), each=4)[1:n_obs], 
                       " Q", rep(1:4, length.out=n_obs))
  
  # Results storage
  results <- data.frame(
    time = time_labels,
    pointwise_lpd = numeric(n_obs),
    delta_looic = numeric(n_obs),
    delta_test_forecast = numeric(n_obs)
  )
  
  # Get test data - FIXED to use passed parameters
  test_data <- rolling_splits[[dep_var]][[window_size]]$test
  test_y <- test_data[, 1]
  test_X <- as.data.frame(test_data[, -1])
  
  # Make original predictions for test period
  original_pred <- predict(model, newdata = test_X, burn = 100)
  original_point_preds <- if(method == "median") {
    apply(original_pred$distribution, 2, median)
  } else {
    colMeans(original_pred$distribution)
  }
  
  # Calculate pointwise log predictive density 
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)  # Using number of samples from log-likelihood
  
  # Ensure we have enough samples (safety check)
  if (n_samples == 0) {
    stop("No log-likelihood values available.")
  }

  log_lik_matrix <- matrix(log_lik, nrow = n_samples, ncol = n_obs, byrow = FALSE)
  
  results$pointwise_lpd <- colMeans(log_lik_matrix, na.rm = TRUE)  # Average LPD
  
  # LOOIC Calculation and test prediction changes - Parallelized version
  looic_values <- rep(NA, n_obs)
  delta_forecasts <- rep(NA, n_obs)
  
  # Parallel loop over observations
  parallel_results <- foreach(i = 1:n_obs, 
                           .packages = c("bsts"),
                           .export = c("test_X", "original_point_preds", "method")) %dopar% {
    # Create leave-one-out dataset
    loo_data <- train_data[-i, ]
    y_loo <- loo_data[, 1]
    X_loo <- loo_data[, -1]
    
    # Fit model on leave-one-out data with reduced iterations for speed
    ss <- AddSeasonal(list(), y_loo, nseasons = 4, season.duration = 1)
    ss <- AddLocalLinearTrend(ss, y_loo)
    
    model_loo <- tryCatch({
      bsts(y_loo,
           X = X_loo,
           state.specification = ss,
           niter = 10000,  # Reduced from 10000 for speed
           ping = 0)
    }, 
    error = function(e) {
      return(NULL)  # Return NULL if fitting fails
    })
    
    if (is.null(model_loo)) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    # Calculate LOOIC for leave-out model
    log_lik_loo <- model_loo$log.likelihood
    if (length(log_lik_loo) == 0) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    loo_liks <- numeric(length(log_lik_loo))
    for (j in 1:length(log_lik_loo)) {
      loo_liks[j] <- mean(log_lik_loo[-j], na.rm = TRUE)
    }
    
    looic <- -2 * mean(loo_liks, na.rm = TRUE)
    
    # Calculate change in test forecasts
    # Predict for test period using the leave-one-out model
    pred_loo <- predict(model_loo, newdata = test_X, burn = 100)
    
    point_pred_loo <- if(method == "median") {
      apply(pred_loo$distribution, 2, median)
    } else {
      colMeans(pred_loo$distribution)
    }
    
    # Calculate average change across all test observations
    delta_forecast <- mean(point_pred_loo - original_point_preds)
    
    return(list(looic = looic, delta_forecast = delta_forecast))
  }
  
  # Collect the parallel results
  for (i in 1:n_obs) {
    if (!is.null(parallel_results[[i]])) {
      looic_values[i] <- parallel_results[[i]]$looic
      delta_forecasts[i] <- parallel_results[[i]]$delta_forecast
    }
  }
  
  # Calculate original LOOIC
  looic_original <- -2 * mean(colMeans(log_lik_matrix, na.rm = TRUE))
  
  # Assign results
  results$delta_looic <- looic_values - looic_original  # Calculate delta LOOIC
  results$delta_test_forecast <- delta_forecasts
  
  return(results)
}

# FIXED Function to analyze the top models with test data
analyze_top_models_parallel <- function(best_models, all_models) {
  all_influence_results <- list()
  
  for(dep_var in names(best_models)) {
    cat("\n\nAnalyzing dependent variable:", dep_var)
    dep_influence <- list()
    models <- best_models[[dep_var]]
    
    for(i in 1:nrow(models)) {
      cat("\n\nProcessing model", i, "for", dep_var)
      model_info <- models[i, ]
      
      # Get training data for this window
      window_data <- get_window_data(dep_var, model_info$Window)
      
      # Get the original model 
      original_model <- all_models[[dep_var]][[model_info$Window]]
      
      # Calculate influence measures - FIXED to pass dep_var and window_size
      influence_results <- calculate_influence_test(
        original_model, 
        window_data,
        model_info$Method,
        dep_var,
        model_info$Window
      )
      
      # Add model identifier
      identifier <- paste("Window", model_info$Window, "-", model_info$Method)
      dep_influence[[identifier]] <- influence_results
    }
    
    all_influence_results[[dep_var]] <- dep_influence
  }
  
  return(all_influence_results)
}

# Function to generate comprehensive results tables
generate_influence_summary <- function(influence_results) {
  for(dep_var in names(influence_results)) {
    cat("\n=== Results for", dep_var, "===\n")
    
    for(model_name in names(influence_results[[dep_var]])) {
      cat("\n--- Model:", model_name, "---\n")
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # Round numeric columns to 4 decimal places
      results_df[, 2:4] <- round(results_df[, 2:4], 4)
      
      # Sort by absolute delta_test_forecast to find most influential observations
      sorted_df <- results_df[order(abs(results_df$delta_test_forecast), decreasing = TRUE), ]
      
      cat("Top 25 most influential observations:\n")
      print(head(sorted_df, 25))
      
      cat("\nSummary statistics:\n")
      
      # Calculate summary statistics separately
      lpd_summary <- summary(results_df$pointwise_lpd)
      looic_summary <- summary(results_df$delta_looic)
      forecast_summary <- summary(results_df$delta_test_forecast)
      
      # Print each metric separately to avoid data frame mixing issues
      cat("\nPointwise LPD:\n")
      print(round(lpd_summary, 4))
      cat("Std Dev:", round(sd(results_df$pointwise_lpd, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta LOOIC:\n")
      print(round(looic_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_looic, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta Test Forecast:\n")
      print(round(forecast_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_test_forecast, na.rm = TRUE), 4), "\n")
    }
  }
}

# Run the analysis with parallel processing
run_influence_analysis_parallel <- function() {
  # Start timing
  start_time <- Sys.time()
  cat("Starting parallel influence analysis at:", format(start_time), "\n")
  
  # First get the best models based on test data performance
  best_models <- get_best_models_test(all_models_rwbw)
  
  # Run the influence analysis in parallel
  influence_results <- analyze_top_models_parallel(best_models, all_models_rwbw)
  
  # Generate summary tables
  generate_influence_summary(influence_results)
  
  # End timing
  end_time <- Sys.time()
  execution_time <- end_time - start_time
  cat("\nInfluence analysis completed in:", format(execution_time), "\n")
  
  return(influence_results)
}

# Run the analysis
influence_results_parallel <- run_influence_analysis_parallel()

# Clean up the parallel backend when done
stopImplicitCluster()
```
# Excel
```{r}
create_enhanced_prediction_tables <- function(all_evaluations, all_models, extended_test_sets_rolling, 
                                              crps_scores, lpd_scores, file_path, 
                                              aggregate_smape_results,
                                              ensemble_smapes) {
  dep_vars <- names(all_evaluations)
  wb <- createWorkbook()
  
  for(dep_var in dep_vars) {
    addWorksheet(wb, dep_var)
    
    mean_smape   <- aggregate_smape_results[[dep_var]]$Mean_SMAPE
    median_smape <- aggregate_smape_results[[dep_var]]$Median_SMAPE  # if your code uses Median_SMAPE
    # Use weighted ensemble SMAPE if available
    sensi_smape <- if(!is.null(ensemble_smapes[[dep_var]])) round(ensemble_smapes[[dep_var]], 2) else NA
    
    variable_data <- data.frame(
      Col1 = c(
        dep_var,
        "Holds out period",
        "Prediction based on testing SMAPE",
        "Use Mean",
        "Use Median",
        "Prediction based on Sensitivity"
      ),
      Col2 = c(
        "",
        "2016Q4 - 2018Q2 and 2024Q4",
        "SMAPE",
        round(mean_smape, 2),
        round(median_smape, 2),
        sensi_smape
      ),
      stringsAsFactors = FALSE
    )
    
    writeData(wb, dep_var, variable_data, startRow = 1, startCol = 1, colNames = FALSE)
    setColWidths(wb, dep_var, cols = 1:2, widths = c(25, 20))
    
    start_row <- 8  # row 7 is blank
    
    # Handle potential NULLs in evaluations
    smape_mean <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_mean) || is.null(x$errors_mean$SMAPE)) {
        return(NA)
      }
      return(x$errors_mean$SMAPE)
    })
    
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_median) || is.null(x$errors_median$SMAPE)) {
        return(NA)
      }
      return(x$errors_median$SMAPE)
    })
    
    # Ensure we only include valid values for ranking
    valid_mean_indices <- which(!is.na(smape_mean))
    valid_median_indices <- which(!is.na(smape_median))
    
    # Get top 5 from valid indices
    if(length(valid_mean_indices) >= 5) {
      top_5_mean <- valid_mean_indices[order(smape_mean[valid_mean_indices])[1:5]]
    } else if(length(valid_mean_indices) > 0) {
      top_5_mean <- valid_mean_indices[order(smape_mean[valid_mean_indices])[1:length(valid_mean_indices)]]
    } else {
      top_5_mean <- integer(0)
    }
    
    if(length(valid_median_indices) >= 5) {
      top_5_median <- valid_median_indices[order(smape_median[valid_median_indices])[1:5]]
    } else if(length(valid_median_indices) > 0) {
      top_5_median <- valid_median_indices[order(smape_median[valid_median_indices])[1:length(valid_median_indices)]]
    } else {
      top_5_median <- integer(0)
    }
    
    crps_mean <- numeric(length(all_evaluations[[dep_var]]))
    crps_sd   <- numeric(length(all_evaluations[[dep_var]]))
    for(i in seq_along(all_evaluations[[dep_var]])) {
      window_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        crps_mean[i] <- window_data$CRPS_Mean
        crps_sd[i]   <- window_data$CRPS_SD
      } else {
        crps_mean[i] <- NA
        crps_sd[i]   <- NA
      }
    }
    
    lpd_mean <- numeric(length(all_evaluations[[dep_var]]))
    lpd_sd   <- numeric(length(all_evaluations[[dep_var]]))
    for(i in seq_along(all_evaluations[[dep_var]])) {
      window_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        lpd_mean[i] <- window_data$LPD_Mean
        lpd_sd[i]   <- window_data$LPD_SD
      } else {
        lpd_mean[i] <- NA
        lpd_sd[i]   <- NA
      }
    }
    
    writeData(wb, dep_var, "Predictive interval for Top 5 models based on sMAPE", 
              startRow = start_row, startCol = 1)
    
    writeData(wb, dep_var, "Use Mean", startRow = start_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = start_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = start_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = start_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = start_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = start_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = start_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = start_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = start_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = start_row + 2, startCol = 9)
    
    current_row <- start_row + 3
    for(window in top_5_mean) {
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      
      # Check if the model exists and is a valid BSTS object
      if(is.null(all_models[[dep_var]][[window]]) || !inherits(all_models[[dep_var]][[window]], "bsts")) {
        # Model not available, write NA values
        writeData(wb, dep_var, "Model not available", startRow = current_row, startCol = 2)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      } else {
        # Check if the test data exists
        if(is.null(extended_test_sets_rolling[[dep_var]][[window]]) || 
           nrow(extended_test_sets_rolling[[dep_var]][[window]]) == 0) {
          # Test data not available
          writeData(wb, dep_var, "Test data not available", startRow = current_row, startCol = 2)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        } else {
          # Both model and test data are available
          tryCatch({
            pred_data <- extended_test_sets_rolling[[dep_var]][[window]]
            last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
            
            # Make predictions
            pred <- predict.bsts(all_models[[dep_var]][[window]], 
                                 newdata = as.data.frame(last_row_predictors),
                                 burn = 100)
            dist <- pred$distribution[,1]
            
            writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, mean(dist), startRow = current_row, startCol = 3)
            writeData(wb, dep_var, median(dist), startRow = current_row, startCol = 4)
            writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
          }, error = function(e) {
            # Handle prediction errors
            writeData(wb, dep_var, paste("Error:", e$message), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
          })
        }
        
        # Write performance metrics regardless of prediction success
        writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      }
      
      current_row <- current_row + 1
    }
    
    writeData(wb, dep_var, "Use Median", startRow = current_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = current_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = current_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = current_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = current_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = current_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = current_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = current_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = current_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = current_row + 2, startCol = 9)
    
    current_row <- current_row + 3
    for(window in top_5_median) {
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      
      # Check if the model exists and is a valid BSTS object
      if(is.null(all_models[[dep_var]][[window]]) || !inherits(all_models[[dep_var]][[window]], "bsts")) {
        # Model not available, write NA values
        writeData(wb, dep_var, "Model not available", startRow = current_row, startCol = 2)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      } else {
        # Check if the test data exists
        if(is.null(extended_test_sets_rolling[[dep_var]][[window]]) || 
           nrow(extended_test_sets_rolling[[dep_var]][[window]]) == 0) {
          # Test data not available
          writeData(wb, dep_var, "Test data not available", startRow = current_row, startCol = 2)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        } else {
          # Both model and test data are available
          tryCatch({
            pred_data <- extended_test_sets_rolling[[dep_var]][[window]]
            last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
            
            # Make predictions
            pred <- predict.bsts(all_models[[dep_var]][[window]], 
                                 newdata = as.data.frame(last_row_predictors),
                                 burn = 100)
            dist <- pred$distribution[,1]
            
            writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, mean(dist), startRow = current_row, startCol = 3)
            writeData(wb, dep_var, median(dist), startRow = current_row, startCol = 4)
            writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
          }, error = function(e) {
            # Handle prediction errors
            writeData(wb, dep_var, paste("Error:", e$message), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
          })
        }
        
        # Write performance metrics regardless of prediction success
        writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      }
      
      current_row <- current_row + 1
    }
    
    setColWidths(wb, dep_var, cols = 1, width = 50)
    setColWidths(wb, dep_var, cols = 2:9, width = 15)
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}


###############################################################################
## 3) add_sensitivity_analysis
##    Moves the sensitivity analysis block down by 3 rows and removes any extra row (e.g. SMAPE row)
###############################################################################
add_sensitivity_analysis <- function(sensitivity_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for(dep_var in names(sensitivity_results)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    
    # Calculate the exact row to place the sensitivity analysis
    # Find the last non-empty row of the existing content
    last_content_row <- max(which(!is.na(sheet_data[,1]) & sheet_data[,1] != ""), na.rm = TRUE)
    
    # Add a small gap (3 rows) after the last content
    last_row <- last_content_row + 6
    
    # Create the results data frame
    results_data <- data.frame(
      Window   = integer(),
      Method   = character(),
      Sigma    = numeric(),
      Slab_Var = numeric(),
      WAIC     = numeric(),
      LOOIC    = numeric(),
      MAPE     = numeric(),
      SMAPE    = numeric(),
      stringsAsFactors = FALSE
    )
    
    for (model_name in names(sensitivity_results[[dep_var]])) {
      model_info <- sensitivity_results[[dep_var]][[model_name]]
      window <- model_info$window
      method <- model_info$method
      
      for (result_name in names(model_info$results)) {
        result <- model_info$results[[result_name]]
        results_data <- rbind(results_data, data.frame(
          Window   = window,
          Method   = method,
          Sigma    = if(!is.null(result$sigma)) result$sigma else NA,
          Slab_Var = if(!is.null(result$slab_var)) result$slab_var else NA,
          WAIC     = if(!is.null(result$waic)) result$waic else NA,
          LOOIC    = if(!is.null(result$looic)) result$looic else NA,
          MAPE     = if(!is.null(result$mape)) result$mape else NA,
          SMAPE    = if(!is.null(result$smape)) result$smape else NA,
          stringsAsFactors = FALSE
        ))
      }
    }
    
    # Write the title directly (no mysterious dep var title)
    writeData(wb, dep_var, "Sensitivity Analysis Results", startRow = last_row, startCol = 1)
    
    # Immediately write the table
    writeData(wb, dep_var, results_data, startRow = last_row + 1, startCol = 1, colNames = TRUE)
    
    # Set column widths
    setColWidths(wb, dep_var, cols = 1:8, widths = c(10,10,10,10,12,12,10,10))
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## Fixed add_influence_analysis_to_workbook function
## Fixes the spacing issue and handles delta_test_forecast/delta_holdout_forecast
###############################################################################
add_influence_analysis_to_workbook <- function(influence_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for (dep_var in names(influence_results)) {
    if (!dep_var %in% names(wb)) {
      addWorksheet(wb, dep_var)
    }
    
    sheet_data <- tryCatch({
      readWorkbook(wb, sheet = dep_var)
    }, error = function(e) {
      data.frame()
    })
    
    # Calculate exact start row by finding the last sensitivity analysis row
    # Find the last non-empty row in the sheet
    non_empty_rows <- which(!is.na(sheet_data[,1]) & sheet_data[,1] != "")
    
    if (length(non_empty_rows) > 0) {
      last_content_row <- max(non_empty_rows, na.rm = TRUE)
      # Look for "Sensitivity Analysis Results" in the sheet
      sensitivity_rows <- which(sheet_data[,1] == "Sensitivity Analysis Results")
      
      if (length(sensitivity_rows) > 0) {
        sensitivity_row <- max(sensitivity_rows)
        # Find the last row of the sensitivity table
        # Usually it's a block of data after the title
        for (i in (sensitivity_row + 1):nrow(sheet_data)) {
          if (is.na(sheet_data[i,1]) || sheet_data[i,1] == "") {
            last_sensitivity_row <- i - 1
            break
          }
          # If we reach the end of the dataframe
          if (i == nrow(sheet_data)) {
            last_sensitivity_row <- i
          }
        }
        # Start influence analysis 5 rows after the sensitivity table
        current_row <- last_sensitivity_row + 9
      } else {
        # If sensitivity analysis not found, start 5 rows after last content
        current_row <- last_content_row + 9
      }
    } else {
      # If sheet is empty
      current_row <- 1
    }
    
    processed_models <- c()
    
    for (model_name in names(influence_results[[dep_var]])) {
      if (model_name %in% processed_models) next
      processed_models <- c(processed_models, model_name)
      
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # CRITICAL FIX: Map delta_test_forecast to delta_holdout_forecast if needed
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        results_df$delta_holdout_forecast <- results_df$delta_test_forecast
      }
      
      # Process delta_looic if present
      if ("delta_looic" %in% names(results_df)) {
        results_df$delta_looic <- as.numeric(as.character(results_df$delta_looic))
      } else {
        results_df$delta_looic <- rep(NA, nrow(results_df))
      }
      
      # Round numeric columns to 4 decimal places
      numeric_cols <- c("pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      for (col in numeric_cols) {
        if (col %in% names(results_df) && is.numeric(results_df[[col]])) {
          results_df[[col]] <- round(results_df[[col]], 4)
        }
      }
      
      # Sort by absolute delta_holdout_forecast
      if (sum(!is.na(results_df$delta_holdout_forecast)) > 0) {
        sorted_df <- results_df[order(abs(results_df$delta_holdout_forecast), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df <- results_df
      }
      top_25_forecast <- head(sorted_df, 25)
      
      # Write the title
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta Holdout Forecast - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      # Get the correct column names
      if (all(c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      } else if (all(c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast")
        colnames(top_25_forecast)[colnames(top_25_forecast) == "delta_test_forecast"] <- "delta_holdout_forecast"
      } else {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      }
      
      # Make sure we only write columns that actually exist
      write_cols <- intersect(write_cols, colnames(top_25_forecast))
      
      # Write the data
      writeData(wb, dep_var, top_25_forecast[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_forecast)), 
                cols = col_idx)
      }
      
      # Proceed to next section - 5 rows after this table
      current_row <- current_row + nrow(top_25_forecast) + 5
      
      # Sort by absolute delta_looic
      if (sum(!is.na(results_df$delta_looic)) > 0) {
        sorted_df_looic <- results_df[order(abs(results_df$delta_looic), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df_looic <- results_df
      }
      top_25_looic <- head(sorted_df_looic, 25)
      
      # Write looic section
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta LOOIC - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      writeData(wb, dep_var, top_25_looic[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_looic)), 
                cols = col_idx)
      }
      
      # Proceed to next model section - 5 rows after this table
      current_row <- current_row + nrow(top_25_looic) + 5
    }
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  cat("Influence analysis results added to workbook:", file_path, "\n")
}
###############################################################################
## Parallelized version of create_excel_results
###############################################################################
create_excel_results_parallel <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets_rolling, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  rolling_splits, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  all_results,         
  top_model_results,   
  hold_out_dataset     
) {
  file_path <- "RW_ALL.xlsx"
  
  # This part is harder to parallelize as it builds on previous results
  ensemble_smapes <- get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
  
  # Create prediction tables
  wb <- create_enhanced_prediction_tables(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets_rolling = extended_test_sets_rolling, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_smapes = ensemble_smapes
  )
  
  # Now we'll parallelize these steps which can be run independently
  
  # Set up parallel processing
  no_cores <- detectCores() - 1  # Leave one core free
  cat("Using", no_cores, "cores for parallel Excel processing\n")
  
  # Register the parallel backend
  cl <- makeCluster(no_cores)
  registerDoParallel(cl)
  
  # Export necessary packages to all workers
  clusterEvalQ(cl, {
    library(openxlsx)
    library(bsts)
    library(coda)
  })
  
  # Process the different analyses in parallel
  # Each function will load, modify, and save the workbook independently
  
  # Run tasks in parallel
  results <- foreach(task_num = 1:3, .packages = c("openxlsx", "bsts")) %dopar% {
    result <- switch(task_num,
      # Task 1: Add sensitivity analysis
      {
        tryCatch({
          add_sensitivity_analysis(sensitivity_results, file_path)
          "Sensitivity analysis completed successfully"
        }, error = function(e) {
          paste("Error in sensitivity analysis:", e$message)
        })
      },
      
      # Task 2: Add influence analysis
      {
        tryCatch({
          # Use the fixed version of the function
          add_influence_analysis_to_workbook(influence_results, file_path)
          "Influence analysis completed successfully"
        }, error = function(e) {
          paste("Error in influence analysis:", e$message)
        })
      },
      
      # Task 3: Add diagnostics
      {
        tryCatch({
          add_diagnostics_to_workbook(
            all_models = all_models, 
            rolling_splits = rolling_splits, 
            all_diagnostics = all_diagnostics, 
            all_looic = all_looic,
            bayesian_R2_results = bayesian_R2_results, 
            crps_scores = crps_scores, 
            lpd_scores = lpd_scores, 
            ljung_box_results = ljung_box_results,
            file_path = file_path
          )
          "Diagnostics completed successfully"
        }, error = function(e) {
          paste("Error in diagnostics:", e$message)
        })
      }
    )
    return(result)
  }
  
  # Stop the cluster
  stopCluster(cl)
  registerDoSEQ()  # Reset to sequential processing
  
  # Clean up
  rm(cl)
  gc()  # Force garbage collection
  
  # Summarize results
  summary <- paste("Excel processing results:", 
                   paste(results, collapse = "; "))
  
  return(paste("Results successfully written to", file_path, "-", summary))
}
###############################################################################
## 5) add_diagnostics_to_workbook (unchanged)
###############################################################################
add_diagnostics_to_workbook <- function(all_models, rolling_splits, all_diagnostics, all_looic, 
                                        bayesian_R2_results, crps_scores, lpd_scores, 
                                        ljung_box_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  suppressWarnings({
    if (!requireNamespace("coda", quietly = TRUE)) {
      install.packages("coda")
    }
    library(coda)
  })
  
  for(dep_var in names(all_models)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    current_row <- nrow(sheet_data) + 22
    
    for(window in 1:length(all_models[[dep_var]])) {
      if (is.null(all_models[[dep_var]][[window]])) next
      
      train_size <- if (!is.null(rolling_splits[[dep_var]]) && 
                        !is.null(rolling_splits[[dep_var]][[window]]) && 
                        !is.null(rolling_splits[[dep_var]][[window]]$train)) {
        nrow(rolling_splits[[dep_var]][[window]]$train)
      } else {
        NA
      }
      
      test_size <- if (!is.null(rolling_splits[[dep_var]]) && 
                       !is.null(rolling_splits[[dep_var]][[window]]) && 
                       !is.null(rolling_splits[[dep_var]][[window]]$test)) {
        nrow(rolling_splits[[dep_var]][[window]]$test)
      } else {
        NA
      }
      
      n_predictors <- if (!is.null(rolling_splits[[dep_var]]) && 
                          !is.null(rolling_splits[[dep_var]][[window]]) && 
                          !is.null(rolling_splits[[dep_var]][[window]]$train)) {
        ncol(rolling_splits[[dep_var]][[window]]$train) - 1
      } else {
        NA
      }
      
      dic_value <- if (!is.null(all_diagnostics) && 
                       !is.null(all_diagnostics[[dep_var]]) && 
                       !is.null(all_diagnostics[[dep_var]]$DIC) &&
                       length(all_diagnostics[[dep_var]]$DIC) >= window) {
        all_diagnostics[[dep_var]]$DIC[window]
      } else {
        NA
      }
      
      waic_value <- if (!is.null(all_diagnostics) && 
                        !is.null(all_diagnostics[[dep_var]]) && 
                        !is.null(all_diagnostics[[dep_var]]$WAIC) &&
                        length(all_diagnostics[[dep_var]]$WAIC) >= window) {
        all_diagnostics[[dep_var]]$WAIC[window]
      } else {
        NA
      }
      
      looic_value <- if (!is.null(all_looic) && 
                         !is.null(all_looic[[dep_var]]) && 
                         length(all_looic[[dep_var]]) >= window) {
        all_looic[[dep_var]][window]
      } else {
        NA
      }
      
      r2_value <- if (!is.null(bayesian_R2_results) && 
                      !is.null(bayesian_R2_results[[dep_var]]) && 
                      !is.null(bayesian_R2_results[[dep_var]]$Mean_R2) &&
                      length(bayesian_R2_results[[dep_var]]$Mean_R2) >= window) {
        bayesian_R2_results[[dep_var]]$Mean_R2[window]
      } else {
        NA
      }
      
      crps_value <- if (!is.null(crps_scores) && 
                        !is.null(crps_scores[[dep_var]]) && 
                        any(crps_scores[[dep_var]]$Window == window)) {
        crps_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == window, ]
        if (nrow(crps_data) > 0 && !is.na(crps_data$CRPS_Mean) && !is.na(crps_data$CRPS_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", crps_data$CRPS_Mean, crps_data$CRPS_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lpd_value <- if (!is.null(lpd_scores) && 
                       !is.null(lpd_scores[[dep_var]]) && 
                       any(lpd_scores[[dep_var]]$Window == window)) {
        lpd_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == window, ]
        if (nrow(lpd_data) > 0 && !is.na(lpd_data$LPD_Mean) && !is.na(lpd_data$LPD_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", lpd_data$LPD_Mean, lpd_data$LPD_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lb_stat <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_Statistic) &&
                     length(ljung_box_results[[dep_var]]$LB_Statistic) >= window) {
        ljung_box_results[[dep_var]]$LB_Statistic[window]
      } else {
        NA
      }
      
      lb_pval <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_pvalue) &&
                     length(ljung_box_results[[dep_var]]$LB_pvalue) >= window) {
        ljung_box_results[[dep_var]]$LB_pvalue[window]
      } else {
        NA
      }
      
      metrics <- data.frame(
        Metric = c(
          "Training Sample Size",
          "Test Sample Size",
          "Number of Predictors",
          "DIC/number of training period",
          "WAIC/number of training period",
          "LOOIC",
          "Bayesian R²",
          "Number of iterations used in BSTS",
          "CRPS",
          "LPD",
          "LB_Statistic",
          "LB_pvalue"
        ),
        Value = c(
          train_size,
          test_size,
          n_predictors,
          dic_value,
          waic_value,
          looic_value,
          r2_value,
          10000,
          crps_value,
          lpd_value,
          lb_stat,
          lb_pval
        )
      )
      
      writeData(wb, dep_var, paste("Window", window), startRow = current_row)
      current_row <- current_row + 2
      writeData(wb, dep_var, metrics, startRow = current_row, startCol = 1, colNames = FALSE)
      current_row <- current_row + nrow(metrics) + 2
      
      tryCatch({
        coef_stats <- data.frame(
          Parameter     = character(),
          Mean          = numeric(),
          Median        = numeric(),
          CI_2.5        = numeric(),
          CI_97.5       = numeric(),
          ESS           = numeric(),
          Post_Prob     = numeric(),
          Geweke_Pval   = numeric(),
          HW_Pval       = numeric(),
          RL_Autocorr   = numeric(),
          RL_Iterations = numeric(),
          RL_Burnin     = numeric(),
          RL_Dependence = numeric(),
          stringsAsFactors = FALSE
        )
        
        model <- all_models[[dep_var]][[window]]
        if (!is.null(model) && !is.null(model$coefficients)) {
          coef_matrix <- as.matrix(model$coefficients)
          
          for(j in 1:ncol(coef_matrix)) {
            param_name <- colnames(coef_matrix)[j]
            chain <- as.vector(coef_matrix[,j])
            if (length(chain) < 2) next
            
            ci <- tryCatch({
              quantile(chain, probs = c(0.025, 0.975))
            }, error = function(e) c(NA, NA))
            
            mean_val   <- tryCatch(mean(chain), error = function(e) NA)
            median_val <- tryCatch(median(chain), error = function(e) NA)
            ess_val    <- tryCatch(as.numeric(effectiveSize(mcmc(chain))), error = function(e) NA)
            pip        <- tryCatch(mean(chain != 0), error = function(e) NA)
            
            geweke_pval <- tryCatch({
              geweke <- geweke.diag(mcmc(chain))
              2 * pnorm(-abs(geweke$z))
            }, error = function(e) NA)
            
            hw_pval <- tryCatch({
              hw <- heidel.diag(mcmc(chain))
              if (is.null(hw)) NA else hw[1, "pvalue"]
            }, error = function(e) NA)
            
            rl_stats <- tryCatch({
              if (length(unique(chain)) >= 10) {
                binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
                rl <- raftery.diag(binary_chain)
                c(cor(chain[-1], chain[-length(chain)]),
                  rl$resmatrix[1,"N"],
                  rl$resmatrix[1,"M"],
                  rl$resmatrix[1,"I"])
              } else {
                rep(NA, 4)
              }
            }, error = function(e) rep(NA, 4))
            
            coef_stats <- rbind(coef_stats, data.frame(
              Parameter     = param_name,
              Mean          = round(mean_val, 4),
              Median        = round(median_val, 4),
              CI_2.5        = round(ci[1], 4),
              CI_97.5       = round(ci[2], 4),
              ESS           = round(ess_val, 0),
              Post_Prob     = round(pip, 4),
              Geweke_Pval   = round(geweke_pval, 4),
              HW_Pval       = round(hw_pval, 4),
              RL_Autocorr   = round(rl_stats[1], 4),
              RL_Iterations = round(rl_stats[2], 0),
              RL_Burnin     = round(rl_stats[3], 0),
              RL_Dependence = round(rl_stats[4], 2),
              stringsAsFactors=FALSE
            ))
          }
        }
        
        if (nrow(coef_stats) > 0) {
          writeData(wb, dep_var, coef_stats, startRow = current_row)
          current_row <- current_row + nrow(coef_stats) + 3
        } else {
          writeData(wb, dep_var, "No coefficient data available", startRow = current_row)
          current_row <- current_row + 4
        }
        
      }, error = function(e) {
        writeData(wb, dep_var, paste("Error processing coefficients:", e$message), startRow = current_row)
        current_row <- current_row + 4
      })
    }
    
    setColWidths(wb, dep_var, cols = 1:13, widths = "auto")
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## 6) format_window_range (unchanged)
###############################################################################
format_window_range <- function(window) {
  # Define the descriptive text for each rolling window's train and test sets
  # Updated to reflect the new window specifications and hold-out period (80-86)
  train_ranges <- c(
    "1 - 79 & 87 - 97",
    "5 - 79 & 87 - 101",
    "9 - 79 & 87 - 105",
    "13 - 79 & 87 - 109 "
  )
  
  test_ranges <- c(
    "98 - 111",
    "1 - 4 & 102 - 111",
    "1 - 8 & 106 - 111",
    "1 - 12 & 110 - 111"
  )
  
  paste(train_ranges[window], "as the training and the", test_ranges[window], "as the testing")
}
###############################################################################
## 7) create_excel_results
##    Computes ensemble SMAPEs then calls the above functions.
###############################################################################
create_excel_results <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets_rolling, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  rolling_splits, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  all_results,         
  top_model_results,   
  hold_out_dataset     
) {
  file_path <- "RW_BW_2016Q4.xlsx"
  
  # Map delta_test_forecast to delta_holdout_forecast in influence results
  for (dep_var in names(influence_results)) {
    for (model_name in names(influence_results[[dep_var]])) {
      results_df <- influence_results[[dep_var]][[model_name]]
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        influence_results[[dep_var]][[model_name]]$delta_holdout_forecast <- 
          results_df$delta_test_forecast
      }
    }
  }
  
  # Get ensemble SMAPEs
  ensemble_smapes <- get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
  
  # Create prediction tables
  cat("Creating prediction tables...\n")
  wb <- create_enhanced_prediction_tables(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets_rolling = extended_test_sets_rolling, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_smapes = ensemble_smapes
  )
  
  # Add sensitivity analysis with proper spacing
  cat("Adding sensitivity analysis...\n")
  add_sensitivity_analysis(sensitivity_results, file_path)
  
  # Add influence analysis with proper spacing
  cat("Adding influence analysis...\n")
  add_influence_analysis_to_workbook(influence_results, file_path)
  
  # Add diagnostics
  cat("Adding diagnostics...\n")
  add_diagnostics_to_workbook(
    all_models = all_models, 
    rolling_splits = rolling_splits, 
    all_diagnostics = all_diagnostics, 
    all_looic = all_looic,
    bayesian_R2_results = bayesian_R2_results, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    ljung_box_results = ljung_box_results,
    file_path = file_path
  )
  
  return(paste("Results successfully written to", file_path))
}
###############################################################################
## 8) Example usage
###############################################################################
 
result <- create_excel_results(
  all_evaluations = all_evaluations_rwbw, 
  all_models = all_models_rwbw, 
  extended_test_sets_rolling = extended_test_sets_rolling, 
  crps_scores = crps_scores_custom_rolling, 
  lpd_scores = lpd_scores_bw, 
  sensitivity_results = sensitivity_results, 
  influence_results = influence_results_parallel, 
  rolling_splits = rolling_splits,  
  all_diagnostics = all_diagnostics_rwbw, 
  all_looic = all_looic_rwbw, 
  bayesian_R2_results = bayesian_R2_results_bw,
  ljung_box_results = ljung_box_results_bw,
  aggregate_smape_results = aggregate_smape_results,
   all_results = all_results,
   top_model_results = top_model_results,
   hold_out_dataset = hold_out_dataset
)

print(result)
```

# Step

# original dataset lagged for 24NA （ create DATASET from 1997Q4 to 2024Q3）
```{r}
# Create datasets for each dependent variable
dep_var_datasets <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Store in list
  dep_var_datasets[[dep_var_name]] <- dataset
}


# Take lag of the missing value of the quarter we want to predict 
dep_var_datasets_modified <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  # Repeat the first column (dep_var) and add it before the first column
  dataset <- cbind(dataset[, 1, drop = FALSE], dataset)  # Add first column as the first column again
  # Loop over columns starting from the second column
  for(col in 2:ncol(dataset)) {
    # Check if the last row value is missing
    if(is.na(dataset[nrow(dataset), col])) {
      # Apply lag: take the value from the previous row for all rows of the column
      dataset[, col] <- lag(dataset[, col], 1, default = NA)
      # Modify column name to indicate lag if it was modified
      new_col_name <- paste0(colnames(dataset)[col], "_lag1")
      colnames(dataset)[col] <- new_col_name
    }
  }
  # Store in list
  dep_var_datasets_modified[[dep_var_name]] <- dataset
}

# Delete before 1997Q4
dep_var_datasets_modified <- lapply(dep_var_datasets_modified, function(x) {
  x[-(1:8), ]
})

# Extract last row from each dataset
last_row <- lapply(dep_var_datasets_modified, function(x) {
  x[nrow(x), ]
})

# Remove last row from each dataset
dep_var_datasets <- lapply(dep_var_datasets_modified, function(x) {
  x[-nrow(x), ]
})

# Verify dimensions
cat("Number of rows in main datasets:\n")
sapply(dep_var_datasets, nrow)
cat("\nNumber of columns in main datasets:\n")
sapply(dep_var_datasets, ncol)
print(dep_var_datasets[[1]][nrow(dep_var_datasets[[1]]), ])  # For the first dataset

create_quarter_four_from_last_row <- function(dep_var_datasets, dep_var_sets) {
  quarter_four <- list()
  
  for (dep_var_name in names(dep_var_sets)) {
    # Get the original dep_var column name
    original_dep_var_name <- dep_var_sets[[dep_var_name]]$dep_var
    
    # Extract last row from dataset
    last_row_data <- dep_var_datasets[[dep_var_name]][nrow(dep_var_datasets[[dep_var_name]]), , drop = FALSE]
    
    # Separate and rename dep_var column
    dep_var_column <- last_row_data[, original_dep_var_name, drop = FALSE]
    colnames(dep_var_column) <- "dep_var"
    
    # Get the independent variables
    indep_vars_data <- last_row_data[, setdiff(colnames(last_row_data), original_dep_var_name), drop = FALSE]
    
    # Combine into final row format
    quarter_four[[dep_var_name]] <- cbind(dep_var_column, indep_vars_data)
    
    # Remove the last row from the dataset
    dep_var_datasets[[dep_var_name]] <- dep_var_datasets[[dep_var_name]][-nrow(dep_var_datasets[[dep_var_name]]), ]
  }
  
  return(list(quarter_four = quarter_four, dep_var_datasets = dep_var_datasets))
}

# Usage:
result <- create_quarter_four_from_last_row(dep_var_datasets, dep_var_sets)
quarter_four <- result$quarter_four
dep_var_datasets <- result$dep_var_datasets

# Extracting rows 80-87 from each dataset in dep_var_datasets
hold_out_dataset <- lapply(dep_var_datasets, function(df) {
  df[80:86, ]#period
})

# Create a list to store the updated datasets with quarter_four at the end
holdout_with_last_row <- list()

# Iterate through each dep_var_name and append quarter_four at the end
for(dep_var_name in names(hold_out_dataset)) {
  
  # Ensure column names match between quarter_four and the data frame in hold_out_dataset
  if (!all(names(quarter_four[[dep_var_name]]) == names(hold_out_dataset[[dep_var_name]]))) {
    # Manually adjust the column names of quarter_four to match hold_out_dataset
    names(quarter_four[[dep_var_name]]) <- names(hold_out_dataset[[dep_var_name]])
  }
  
  # Combine the selected holdout (80-87 rows) and the corresponding last row from quarter_four
  holdout_with_last_row[[dep_var_name]] <- rbind(
    hold_out_dataset[[dep_var_name]],
    quarter_four[[dep_var_name]]
  )
}

# Update hold_out_dataset with the modified datasets
hold_out_dataset <- holdout_with_last_row

hold_out_dataset 

hold_out_period <- 80:86#period

# Function to exclude the hold-out period from the dataset
exclude_hold_out_1 <- function(df) {
  # Exclude the rows that are in the hold-out period (81-88)
  df_no_hold <- df[!rownames(df) %in% hold_out_period, ]
  return(df_no_hold)
}

#--------------------------------------------------------------
# This is the dataset without the hold out period 
Indice_testing_dataset <- lapply(dep_var_datasets, function(df) {
  # Exclude rows 81-88 from the dataset and return the modified dataset
  exclude_hold_out_1(df)
})

dep_var_datasets = Indice_testing_dataset
# Verify the result
Indice_testing_dataset
# Create rolling windows with consistent sizes (4-5 windows)
rolling_splits <- list()
for(dep_var_name in names(dep_var_datasets)) {
  current_data <- dep_var_datasets[[dep_var_name]]
  dataset_windows <- list()
  
  # Get total number of rows in the dataset
  total_rows <- nrow(current_data)
  
  # Fixed parameters
  hold_out_period <- 80:86#period
  train_size <- 90  # Fixed training window size
  window_step <- ceiling((total_rows - length(hold_out_period) - train_size) / 4)  # Step size to create 4-5 windows
  
  # Calculate number of windows possible
  max_windows <- 1 + floor((total_rows - length(hold_out_period) - train_size) / window_step)
  max_windows <- min(max_windows, 5)  # Cap at 5 windows
  
  # Available indices (excluding hold-out period)
  available_indices <- setdiff(1:total_rows, hold_out_period)
  
  # Create windows
  for(window_index in 1:max_windows) {
    # Calculate start position for this window
    start_pos <- 1 + (window_index - 1) * window_step
    
    # Get train indices (first train_size available indices from start_pos)
    # Make sure we don't go beyond available indices
    potential_train_indices <- available_indices[available_indices >= start_pos]
    if(length(potential_train_indices) < train_size) {
      cat("Not enough data for window", window_index, "in", dep_var_name, "\n")
      break
    }
    
    train_indices <- head(potential_train_indices, train_size)
    
    # All remaining indices are test indices
    test_indices <- setdiff(available_indices, train_indices)
    
    # Extract the data
    train_data <- current_data[train_indices, ]
    test_data <- current_data[test_indices, ]
    
    # Check for NA values in training data
    train_na_rows <- which(apply(train_data, 1, function(x) any(is.na(x))))
    if(length(train_na_rows) > 0) {
      cat("Removing", length(train_na_rows), "rows with NA values from", 
          dep_var_name, "window", window_index, "training data\n")
      
      # Remove rows with NA
      train_data <- train_data[-train_na_rows, ]
      train_indices <- train_indices[-train_na_rows]
    }
    
    # Check for NA values in test data
    test_na_rows <- which(apply(test_data, 1, function(x) any(is.na(x))))
    if(length(test_na_rows) > 0) {
      cat("Removing", length(test_na_rows), "rows with NA values from", 
          dep_var_name, "window", window_index, "test data\n")
      
      # Remove rows with NA
      test_data <- test_data[-test_na_rows, ]
      test_indices <- test_indices[-test_na_rows]
    }
    
    # Store the window
    dataset_windows[[window_index]] <- list(
      train = train_data, 
      test = test_data,
      train_indices = train_indices,
      test_indices = test_indices
    )
  }
  
  # Now ensure all windows have the same train and test size
  # Find the minimum sizes across all windows
  train_sizes <- sapply(dataset_windows, function(w) nrow(w$train))
  test_sizes <- sapply(dataset_windows, function(w) nrow(w$test))
  min_train_size <- min(train_sizes)
  min_test_size <- min(test_sizes)
  
  # Trim data in each window to match the minimum sizes
  for(i in 1:length(dataset_windows)) {
    # Trim training data if needed
    if(nrow(dataset_windows[[i]]$train) > min_train_size) {
      cat("Trimming window", i, "training data from", nrow(dataset_windows[[i]]$train), 
          "to", min_train_size, "rows\n")
      
      # Take the first min_train_size rows for consistency
      keep_indices <- head(1:nrow(dataset_windows[[i]]$train), min_train_size)
      dataset_windows[[i]]$train <- dataset_windows[[i]]$train[keep_indices, ]
      dataset_windows[[i]]$train_indices <- dataset_windows[[i]]$train_indices[keep_indices]
    }
    
    # Trim test data if needed
    if(nrow(dataset_windows[[i]]$test) > min_test_size) {
      cat("Trimming window", i, "test data from", nrow(dataset_windows[[i]]$test), 
          "to", min_test_size, "rows\n")
      
      # Take the first min_test_size rows for consistency
      keep_indices <- head(1:nrow(dataset_windows[[i]]$test), min_test_size)
      dataset_windows[[i]]$test <- dataset_windows[[i]]$test[keep_indices, ]
      dataset_windows[[i]]$test_indices <- dataset_windows[[i]]$test_indices[keep_indices]
    }
  }
  
  rolling_splits[[dep_var_name]] <- dataset_windows
}

```

```{r}
# Stepwise var selection with enforced minimum variables
# Load required library for BSTS (if not already loaded)
library(bsts)

# Custom LOOIC calculation function
calculate_looic <- function(model, train_data) {
  n_train <- nrow(train_data)
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  looic <- (-2 * sum(loo_liks)) / n_train
  return(looic)
}

# Revised stepwise_selection function with guaranteed minimum variables
stepwise_selection <- function(train_data, min_vars = 15, significance_threshold = 0.00004, max_iterations = 40) {
  set.seed(1234567)
  
  # Separate response (assumed in first column) and predictors.
  y <- train_data[, 1]
  X <- train_data[, -1]
  all_predictors <- colnames(X)
  
  # Check and ensure y is numeric
  if (!is.numeric(y)) {
    cat("Warning: Response variable is not numeric. Converting to numeric...\n")
    y <- as.numeric(as.character(y))
  }
  
  # Set up state space specification for the BSTS model.
  state_spec <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  state_spec <- AddLocalLinearTrend(state_spec, y)
  
  # Helper: Function to fit the BSTS model using a given set of predictors.
  fit_model <- function(predictors) {
    set.seed(12345678)
    if (length(predictors) == 0) {
      model_data <- data.frame(y = y)
      formula <- y ~ 1
    } else {
      model_data <- data.frame(y = y, X[, predictors, drop = FALSE])
      formula <- as.formula(paste("y ~", paste(predictors, collapse = " + ")))
    }
    bsts(formula, state.specification = state_spec, niter = 10000, data = model_data)
  }
  
  # Start with an empty set of predictors
  cat("Starting with an empty set of predictors\n")
  selected_vars <- character(0)
  current_model <- fit_model(selected_vars)
  current_looic <- calculate_looic(current_model, train_data)
  
  # Standard stepwise selection process (forward/backward)
  iteration <- 1
  improved_in_forward <- TRUE
  improved_in_backward <- TRUE
  
  while (iteration <= max_iterations && (improved_in_forward || improved_in_backward)) {
    cat("\nIteration", iteration, "\n")
    
    # ----- FORWARD SELECTION -----
    improved_in_forward <- FALSE
    available_predictors <- setdiff(all_predictors, selected_vars)
    if (length(available_predictors) > 0) {
      cat("Forward selection phase:\n")
      addition_candidates <- list()
      
      # Try adding each available predictor one at a time.
      for (var in available_predictors) {
        candidate_vars <- c(selected_vars, var)
        try({
          candidate_model <- fit_model(candidate_vars)
          candidate_looic <- calculate_looic(candidate_model, train_data)
          improvement_pct <- (current_looic - candidate_looic) / current_looic
          addition_candidates[[var]] <- list(var = var, model = candidate_model, 
                                             looic = candidate_looic, improvement_pct = improvement_pct)
          cat("  Adding", var, ": LOOIC =", candidate_looic, 
              ", improvement =", round(improvement_pct * 100, 4), "%\n")
        }, silent = TRUE)
      }
      
      if (length(addition_candidates) > 0) {
        valid_additions <- Filter(function(x) x$improvement_pct > significance_threshold, addition_candidates)
        if (length(valid_additions) > 0) {
          best_addition <- valid_additions[[which.min(sapply(valid_additions, function(x) x$looic))]]
          cat("Adding predictor:", best_addition$var, 
              "improves LOOIC from", current_looic, "to", best_addition$looic, 
              "with improvement =", round(best_addition$improvement_pct * 100, 4), "%\n")
          selected_vars <- c(selected_vars, best_addition$var)
          current_model <- best_addition$model
          current_looic <- best_addition$looic
          improved_in_forward <- TRUE
        } else {
          cat("No predictor addition yielded improvement greater than", significance_threshold * 100, "%\n")
        }
      }
    } else {
      cat("Forward selection skipped: no available predictors to add.\n")
    }
    
    # ----- BACKWARD SELECTION -----
    improved_in_backward <- FALSE
    if (length(selected_vars) > min_vars) {
      cat("Backward selection phase:\n")
      removal_candidates <- list()
      
      # Try removing each predictor one at a time.
      for (i in seq_along(selected_vars)) {
        candidate_vars <- selected_vars[-i]
        if (length(candidate_vars) < min_vars) next
        var_removed <- selected_vars[i]
        try({
          candidate_model <- fit_model(candidate_vars)
          candidate_looic <- calculate_looic(candidate_model, train_data)
          improvement_pct <- (current_looic - candidate_looic) / current_looic
          removal_candidates[[var_removed]] <- list(var = var_removed, model = candidate_model, 
                                                   looic = candidate_looic, improvement_pct = improvement_pct, 
                                                   index = i)
          cat("  Removing", var_removed, ": LOOIC =", candidate_looic, 
              ", improvement =", round(improvement_pct * 100, 4), "%\n")
        }, silent = TRUE)
      }
      
      if (length(removal_candidates) > 0) {
        valid_removals <- Filter(function(x) x$improvement_pct > significance_threshold, removal_candidates)
        if (length(valid_removals) > 0) {
          best_removal <- valid_removals[[which.min(sapply(valid_removals, function(x) x$looic))]]
          cat("Removing predictor:", best_removal$var, 
              "improves LOOIC from", current_looic, "to", best_removal$looic, 
              "with improvement =", round(best_removal$improvement_pct * 100, 4), "%\n")
          selected_vars <- selected_vars[-best_removal$index]
          current_model <- best_removal$model
          current_looic <- best_removal$looic
          improved_in_backward <- TRUE
        } else {
          cat("No predictor removal yielded improvement greater than", significance_threshold * 100, "%\n")
        }
      }
    } else {
      cat("Backward selection skipped: number of predictors equals or below minimum allowed.\n")
    }
    
    if (!improved_in_forward && !improved_in_backward) {
      cat("No further improvements in forward or backward phases. Terminating stepwise selection.\n")
      break
    }
    
    iteration <- iteration + 1
  }
  
  # ===== CRITICAL FIX: FORCE MINIMUM VARIABLE COUNT =====
  # This is the key part that was missing or not working in the original code
  if (length(selected_vars) < min_vars) {
    cat("\n===== ADDING VARIABLES TO REACH MINIMUM COUNT =====\n")
    cat("Current selected variables:", length(selected_vars), "Need:", min_vars, "\n")
    
    # Get unselected variables
    remaining_vars <- setdiff(all_predictors, selected_vars)
    
    # Calculate correlations
    correlations <- data.frame(variable = character(0), correlation = numeric(0))
    
    for (var in remaining_vars) {
      # Simple but robust correlation calculation
      var_values <- X[[var]]
      tryCatch({
        corr <- abs(cor(y, var_values, use = "pairwise.complete.obs"))
        if (!is.na(corr)) {
          correlations <- rbind(correlations, data.frame(variable = var, correlation = corr))
        }
      }, error = function(e) {
        # Skip variables that cause errors
      })
    }
    
    # If we successfully calculated any correlations
    if (nrow(correlations) > 0) {
      # Sort by correlation (highest first)
      correlations <- correlations[order(-correlations$correlation), ]
      
      # How many more variables do we need?
      needed <- min_vars - length(selected_vars)
      
      # Take as many as we need (or as many as available)
      to_add <- min(needed, nrow(correlations))
      additional_vars <- correlations$variable[1:to_add]
      
      cat("Adding", length(additional_vars), "variables based on correlation:\n")
      for (i in 1:length(additional_vars)) {
        var <- additional_vars[i]
        corr <- correlations$correlation[correlations$variable == var]
        cat("  ", i, ". ", var, " (correlation: ", round(corr, 4), ")\n", sep="")
      }
      
      # Add these variables to our selected set
      selected_vars <- c(selected_vars, additional_vars)
      
      # Refit the model with all variables
      cat("\nRefitting model with", length(selected_vars), "variables...\n")
      current_model <- fit_model(selected_vars)
      current_looic <- calculate_looic(current_model, train_data)
      cat("New LOOIC:", current_looic, "\n")
    } else {
      # If correlation calculation failed for all variables
      cat("Could not calculate valid correlations. Adding remaining variables directly.\n")
      needed <- min_vars - length(selected_vars)
      to_add <- min(needed, length(remaining_vars))
      selected_vars <- c(selected_vars, remaining_vars[1:to_add])
      
      # Refit the model
      current_model <- fit_model(selected_vars)
      current_looic <- calculate_looic(current_model, train_data)
    }
  }
  
  # Final verification
  cat("\nFinal model has", length(selected_vars), "variables. Minimum required:", min_vars, "\n")
  excluded_vars <- setdiff(all_predictors, selected_vars)
  
  return(list(
    model = current_model,
    selected_vars = selected_vars,
    looic = current_looic,
    excluded_vars = excluded_vars
  ))
}

# Function to Run Stepwise Selection for Each Dependent Variable
run_best_selection <- function(best_models, expanding_windows, min_vars = 15) {
  all_models_ewstep <- list()
  
  for (dep_var in names(best_models)) {
    cat("\n\n==== Variable Selection for", dep_var, "====\n")
    
    # Skip if this variable doesn't exist in expanding_windows
    if (!dep_var %in% names(expanding_windows)) {
      cat("ERROR: No expanding window data found for", dep_var, ". Skipping this variable.\n")
      next
    }
    
    # Use only the top model based on sMAPE
    model_info <- best_models[[dep_var]]
    if ("best_window_1" %in% names(model_info)) {
      best_window <- model_info$best_window_1
      best_method <- model_info$best_method_1
      best_smape <- model_info$best_smape_1
    } else if ("best_window" %in% names(model_info)) {
      best_window <- model_info$best_window
      best_method <- model_info$best_method
      best_smape <- model_info$best_smape
    } else {
      best_window <- 1
      best_method <- "unknown"
      best_smape <- NA
    }
    
    cat("\n--- Using best model: Window", best_window, "with", 
        ifelse(is.null(best_method), "unknown", best_method), 
        "method (sMAPE =", 
        ifelse(is.null(best_smape) || !is.numeric(best_smape), "NA", round(best_smape, 4)), 
        ") for", dep_var, "---\n")
    
    # Access by numeric index, making sure it's within range
    if (best_window < 1 || best_window > length(expanding_windows[[dep_var]])) {
      cat("Window index", best_window, "out of range for", dep_var, "\n")
      cat("Available indices: 1 to", length(expanding_windows[[dep_var]]), "\n")
      # Use first window as fallback
      best_window <- 1
      cat("Using window index 1 instead\n")
    }
    
    # Get the training data using numeric index
    train_data <- expanding_windows[[dep_var]][[best_window]]$train
    
    # Check if train_data exists and has the right structure
    if (is.null(train_data) || !is.data.frame(train_data) || ncol(train_data) < 2) {
      cat("Invalid training data for", dep_var, "at window index", best_window, "\n")
      next
    }
    
    # Run stepwise selection with explicit min_vars parameter
    cat("Running stepwise selection with min_vars =", min_vars, "\n")
    
    tryCatch({
      selection_result <- stepwise_selection(train_data, min_vars = min_vars)
      all_models_ewstep[[dep_var]] <- selection_result
      
      cat("Selected predictors for", dep_var, ":", paste(selection_result$selected_vars, collapse = ", "), "\n")
      cat("Number of selected predictors:", length(selection_result$selected_vars), "\n")
      cat("LOOIC:", selection_result$looic, "\n")
    }, error = function(e) {
      cat("Error in stepwise selection for", dep_var, ":", conditionMessage(e), "\n")
    })
  }
  
  return(all_models_ewstep)
}

# Example usage:
 best_model_rwST <- find_best_models(all_evaluations_rwall)
 final_models_rwST <- run_best_selection(best_model_rwST, rolling_splits, min_vars = 15)
```






```{r}
# Load library for Excel export
library(openxlsx)

# Create a new workbook
wb <- createWorkbook()

# Loop over each dependent variable model and export excluded predictors
for (dep_var in names(final_models_rwST)) {
  model_info <- final_models_rwST[[dep_var]]
  
  # Extract excluded predictors
  excluded_vars <- model_info$excluded_vars
  
  # Create a data frame
  excluded_df <- data.frame(
    ExcludedPredictors = excluded_vars,
    stringsAsFactors = FALSE
  )
  
  # Add a worksheet named after the dependent variable
  addWorksheet(wb, sheetName = dep_var)
  writeData(wb, sheet = dep_var, x = excluded_df)
}

# Save the workbook
saveWorkbook(wb, file = "Removed_pre_StepRW_2016.xlsx", overwrite = TRUE)

cat("Excluded predictors saved to 'excluded_predictors_stepwise.xlsx'\n")

```

## New dataset use removed colm number
```{r}
# Create datasets for each dependent variable
dep_var_datasets <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Store in list
  dep_var_datasets[[dep_var_name]] <- dataset
}


# Take lag of the missing value of 2024 Q4
dep_var_datasets_modified <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Repeat the first column (dep_var) and add it before the first column
  dataset <- cbind(dataset[, 1, drop = FALSE], dataset)  # Add first column as the first column again
  
  # Loop over columns starting from the second column
  for(col in 2:ncol(dataset)) {
    # Check if the last row value is missing
    if(is.na(dataset[nrow(dataset), col])) {
      # Apply lag: take the value from the previous row for all rows of the column
      dataset[, col] <- lag(dataset[, col], 1, default = NA)
      
      # Modify column name to indicate lag if it was modified
      new_col_name <- paste0(colnames(dataset)[col], "_lag1")
      colnames(dataset)[col] <- new_col_name
    }
  }
  
  # Store in list
  dep_var_datasets_modified[[dep_var_name]] <- dataset
}

# Delete 1996Q4
dep_var_datasets_modified <- lapply(dep_var_datasets_modified, function(x) {
  x[-(1:8), ]
})

# Extract last row from each dataset
last_row <- lapply(dep_var_datasets_modified, function(x) {
  x[nrow(x), ]
})

# Remove last row from each dataset
dep_var_datasets <- lapply(dep_var_datasets_modified, function(x) {
  x[-nrow(x), ]
})

```


```{r}
# Function to construct datasets based on selected variables from stepwise selection
construct_selected_datasets <- function(final_models, full_datasets) {
  selected_datasets <- list()
  
  for (dep_var in names(final_models)) {
    # Get the original dataset
    dataset <- full_datasets[[dep_var]]
    
    # Get selected variables from stepwise selection results
    selected_vars <- final_models[[dep_var]]$selected_vars
    
    # Find column indices of selected variables
    selected_indices <- c()
    for (var in selected_vars) {
      # Try exact match first
      exact_match <- which(colnames(dataset) == var)
      if (length(exact_match) > 0) {
        selected_indices <- c(selected_indices, exact_match)
      } else {
        # If exact match fails, try with potential lag suffix if it exists
        var_lag <- paste0(var, "_lag1")
        lag_match <- which(colnames(dataset) == var_lag)
        if (length(lag_match) > 0) {
          selected_indices <- c(selected_indices, lag_match)
          cat("Using lagged variable", var_lag, "for", var, "in", dep_var, "\n")
        } else {
          # Try partial match as last resort
          partial_matches <- grep(var, colnames(dataset), fixed=TRUE)
          if (length(partial_matches) > 0) {
            selected_indices <- c(selected_indices, partial_matches[1])  # Use first partial match
            cat("Warning: Using partial match for", var, "in", dep_var, ":", colnames(dataset)[partial_matches[1]], "\n")
          } else {
            cat("Warning: Variable", var, "not found in", dep_var, "dataset\n")
          }
        }
      }
    }
    
    # Ensure no duplicates in selected indices
    selected_indices <- unique(selected_indices)
    
    # Create new dataset with dependent variable (first column) and selected predictors
    if (length(selected_indices) > 0) {
      # First column is the dependent variable
      new_dataset <- dataset[, c(1, selected_indices)]
      
      cat("\nReconstructed", dep_var, "dataset:\n")
      cat("Original dimensions:", nrow(dataset), "rows,", ncol(dataset), "columns\n")
      cat("New dimensions:", nrow(new_dataset), "rows,", ncol(new_dataset), "columns\n")
      cat("Selected columns:", paste(colnames(new_dataset)[-1], collapse=", "), "\n")
      
      selected_datasets[[dep_var]] <- new_dataset
    } else {
      cat("Warning: No columns were selected for", dep_var, "\n")
      # Include only the dependent variable column
      selected_datasets[[dep_var]] <- dataset[, 1, drop=FALSE]
    }
  }
  
  return(selected_datasets)
}

 dep_var_datasets <- construct_selected_datasets(final_models_rwST , dep_var_datasets)
 quarter_four <- construct_selected_datasets(final_models_rwST , quarter_four)
```

```{r}
# Extracting rows 80-86 from each dataset in dep_var_datasets
hold_out_dataset <- lapply(dep_var_datasets, function(df) {
  df[80:86, ]#period
})

# Create a list to store the updated datasets with quarter_four at the end
holdout_with_last_row <- list()

# Iterate through each dep_var_name and append quarter_four at the end
for(dep_var_name in names(hold_out_dataset)) {
  
  # Ensure column names match between quarter_four and the data frame in hold_out_dataset
  if (!all(names(quarter_four[[dep_var_name]]) == names(hold_out_dataset[[dep_var_name]]))) {
    # Manually adjust the column names of quarter_four to match hold_out_dataset
    names(quarter_four[[dep_var_name]]) <- names(hold_out_dataset[[dep_var_name]])
  }
  
  # Combine the selected holdout (80-87 rows) and the corresponding last row from quarter_four
  holdout_with_last_row[[dep_var_name]] <- rbind(
    hold_out_dataset[[dep_var_name]],
    quarter_four[[dep_var_name]]
  )
}

# Update hold_out_dataset with the modified datasets
hold_out_dataset <- holdout_with_last_row
```

```{r}
# Define the hold-out period (rows 80-86)
hold_out_period <- 80:86#period

# Function to exclude the hold-out period from the dataset
exclude_hold_out_1 <- function(df) {
  # Exclude the rows that are in the hold-out period (80-86)
  df_no_hold <- df[!rownames(df) %in% hold_out_period, ]
  return(df_no_hold)
}

#--------------------------------------------------------------
# This is the dataset without the hold out period 
Indice_testing_dataset <- lapply(dep_var_datasets, function(df) {
  # Exclude rows 80-87 from the dataset and return the modified dataset
  exclude_hold_out_1(df)
})

# Verify the result
Indice_testing_dataset
```

## New rolling window 
```{r}
# Create rolling windows with consistent sizes (4-5 windows)
rolling_splits <- list()
for(dep_var_name in names(dep_var_datasets)) {
  current_data <- dep_var_datasets[[dep_var_name]]
  dataset_windows <- list()
  
  # Get total number of rows in the dataset
  total_rows <- nrow(current_data)
  
  # Fixed parameters
  hold_out_period <- 80:86 #period
  train_size <- 90  # Fixed training window size
  window_step <- ceiling((total_rows - length(hold_out_period) - train_size) / 4)  # Step size to create 4-5 windows
  
  # Calculate number of windows possible
  max_windows <- 1 + floor((total_rows - length(hold_out_period) - train_size) / window_step)
  max_windows <- min(max_windows, 5)  # Cap at 5 windows
  
  # Available indices (excluding hold-out period)
  available_indices <- setdiff(1:total_rows, hold_out_period)
  
  # Create windows
  for(window_index in 1:max_windows) {
    # Calculate start position for this window
    start_pos <- 1 + (window_index - 1) * window_step
    
    # Get train indices (first train_size available indices from start_pos)
    # Make sure we don't go beyond available indices
    potential_train_indices <- available_indices[available_indices >= start_pos]
    if(length(potential_train_indices) < train_size) {
      cat("Not enough data for window", window_index, "in", dep_var_name, "\n")
      break
    }
    
    train_indices <- head(potential_train_indices, train_size)
    
    # All remaining indices are test indices
    test_indices <- setdiff(available_indices, train_indices)
    
    # Extract the data
    train_data <- current_data[train_indices, ]
    test_data <- current_data[test_indices, ]
    
    # Check for NA values in training data
    train_na_rows <- which(apply(train_data, 1, function(x) any(is.na(x))))
    if(length(train_na_rows) > 0) {
      cat("Removing", length(train_na_rows), "rows with NA values from", 
          dep_var_name, "window", window_index, "training data\n")
      
      # Remove rows with NA
      train_data <- train_data[-train_na_rows, ]
      train_indices <- train_indices[-train_na_rows]
    }
    
    # Check for NA values in test data
    test_na_rows <- which(apply(test_data, 1, function(x) any(is.na(x))))
    if(length(test_na_rows) > 0) {
      cat("Removing", length(test_na_rows), "rows with NA values from", 
          dep_var_name, "window", window_index, "test data\n")
      
      # Remove rows with NA
      test_data <- test_data[-test_na_rows, ]
      test_indices <- test_indices[-test_na_rows]
    }
    
    # Store the window
    dataset_windows[[window_index]] <- list(
      train = train_data, 
      test = test_data,
      train_indices = train_indices,
      test_indices = test_indices
    )
  }
  
  # Now ensure all windows have the same train and test size
  # Find the minimum sizes across all windows
  train_sizes <- sapply(dataset_windows, function(w) nrow(w$train))
  test_sizes <- sapply(dataset_windows, function(w) nrow(w$test))
  min_train_size <- min(train_sizes)
  min_test_size <- min(test_sizes)
  
  # Trim data in each window to match the minimum sizes
  for(i in 1:length(dataset_windows)) {
    # Trim training data if needed
    if(nrow(dataset_windows[[i]]$train) > min_train_size) {
      cat("Trimming window", i, "training data from", nrow(dataset_windows[[i]]$train), 
          "to", min_train_size, "rows\n")
      
      # Take the first min_train_size rows for consistency
      keep_indices <- head(1:nrow(dataset_windows[[i]]$train), min_train_size)
      dataset_windows[[i]]$train <- dataset_windows[[i]]$train[keep_indices, ]
      dataset_windows[[i]]$train_indices <- dataset_windows[[i]]$train_indices[keep_indices]
    }
    
    # Trim test data if needed
    if(nrow(dataset_windows[[i]]$test) > min_test_size) {
      cat("Trimming window", i, "test data from", nrow(dataset_windows[[i]]$test), 
          "to", min_test_size, "rows\n")
      
      # Take the first min_test_size rows for consistency
      keep_indices <- head(1:nrow(dataset_windows[[i]]$test), min_test_size)
      dataset_windows[[i]]$test <- dataset_windows[[i]]$test[keep_indices, ]
      dataset_windows[[i]]$test_indices <- dataset_windows[[i]]$test_indices[keep_indices]
    }
  }
  
  rolling_splits[[dep_var_name]] <- dataset_windows
}

# Function to summarize window information
summarize_windows <- function(windows_list) {
  cat("\n======= Window Summary =======\n")
  
  for (dep_var_name in names(windows_list)) {
    cat("\nSummary for", dep_var_name, ":\n")
    windows <- windows_list[[dep_var_name]]
    
    # Create a table for sizes
    test_sizes <- sapply(windows, function(w) nrow(w$test))
    train_sizes <- sapply(windows, function(w) nrow(w$train))
    
    size_table <- data.frame(
      Window = 1:length(windows),
      Train_Size = train_sizes,
      Test_Size = test_sizes
    )
    
    print(size_table)
    
    # Check if all sizes are the same
    if(length(unique(train_sizes)) == 1) {
      cat("\nAll windows have uniform train size of", train_sizes[1], "rows\n")
    } else {
      cat("\nWarning: Train sizes vary across windows:", paste(train_sizes, collapse=", "), "\n")
    }
    
    if(length(unique(test_sizes)) == 1) {
      cat("All windows have uniform test size of", test_sizes[1], "rows\n")
    } else {
      cat("Warning: Test sizes vary across windows:", paste(test_sizes, collapse=", "), "\n")
    }
    
    # Function to find consecutive sequences for readability
    find_sequences <- function(indices) {
      if(length(indices) == 0) return(character(0))
      
      indices <- sort(indices)
      gaps <- diff(indices) > 1
      group_starts <- c(TRUE, gaps)
      group_ends <- c(gaps, TRUE)
      
      sequences <- character(0)
      start_idx <- 1
      
      for(i in 1:length(indices)) {
        if(group_starts[i]) start_idx <- i
        if(group_ends[i]) {
          end_idx <- i
          if(start_idx == end_idx) {
            sequences <- c(sequences, as.character(indices[start_idx]))
          } else {
            sequences <- c(sequences, paste(indices[start_idx], "-", indices[end_idx]))
          }
        }
      }
      
      return(sequences)
    }
    
    # Show detailed information for each window
    for (i in 1:length(windows)) {
      window <- windows[[i]]
      
      cat("\nWindow", i, ":\n")
      
      # Print train ranges in desired format
      train_sequences <- find_sequences(window$train_indices)
      train_display <- paste(train_sequences, collapse=", ")
      cat("  Training indices:", train_display, "\n")
      cat("  Training size:", nrow(window$train), "rows\n")
      
      # Print test ranges in desired format
      test_sequences <- find_sequences(window$test_indices)
      if(length(test_sequences) > 1) {
        test_display <- paste(test_sequences, collapse=" & ")
      } else {
        test_display <- test_sequences
      }
      cat("  Testing indices:", test_display, "\n")
      cat("  Testing size:", nrow(window$test), "rows\n")
      
      # Check hold-out period exclusion
      if(any(window$train_indices %in% hold_out_period)) {
        cat("  WARNING: Hold-out period found in training indices\n")
      }
      if(any(window$test_indices %in% hold_out_period)) {
        cat("  WARNING: Hold-out period found in test indices\n")
      }
    }
  }
}

# Run the summary function
summarize_windows(rolling_splits)

# Check for any remaining NA values in the final dataset
check_for_remaining_nas <- function(windows_list) {
  cat("\n======= Checking for remaining NA values =======\n")
  
  for(dep_var_name in names(windows_list)) {
    has_na <- FALSE
    
    for(i in 1:length(windows_list[[dep_var_name]])) {
      window <- windows_list[[dep_var_name]][[i]]
      
      # Check training data
      if(any(is.na(window$train))) {
        has_na <- TRUE
        cat("\nFound NA values in", dep_var_name, "window", i, "training data\n")
      }
      
      # Check test data
      if(any(is.na(window$test))) {
        has_na <- TRUE
        cat("\nFound NA values in", dep_var_name, "window", i, "test data\n")
      }
    }
    
    if(!has_na) {
      cat("\n", dep_var_name, ": No remaining NA values in any window\n")
    }
  }
}

# Check for NA values
check_for_remaining_nas(rolling_splits)
```

# Fit the model 
```{r}
# Fit models for all dependent variables
all_models_rwstep <- list()
# Net Interest Income
all_models_rwstep$net_interest_income <- fit_all_windows(rolling_splits$net_interest_income)
# Non-Interest Income
all_models_rwstep$non_interest_income <- fit_all_windows(rolling_splits$non_interest_income)
# Provision Credit Loss
all_models_rwstep$provision_credit_loss <- fit_all_windows(rolling_splits$provision_credit_loss)
# Non-Interest Expense
all_models_rwstep$non_interest_expense <- fit_all_windows(rolling_splits$non_interest_expense)
```


```{r}
all_evaluations_rwstep <- list()
 for(dep_var_name in names(dep_var_datasets)) {
   cat("\nEvaluating", dep_var_name, "\n")
   all_evaluations_rwstep[[dep_var_name]] <- evaluate_all_windows(all_models_rwstep[[dep_var_name]], dep_var_datasets[[dep_var_name]])
 }
 print_summary_tables(all_evaluations_rwstep)
```

```{r}
# Calculate model weights
model_weights_rwstep <- calculate_model_weights(all_evaluations_rwstep)
```

```{r}
smape_results_holdout_step <- predict_all_rolling_windows_holdout(all_models_rwstep, hold_out_dataset)
```

```{r}
aggregate_smape_results_step <- aggregate_and_calculate_holdout_smape(smape_results_holdout_step, all_models_rwstep, hold_out_dataset)

# Display aggregated results
cat("\n================================================================\n")
cat("SUMMARY OF AGGREGATED HOLD-OUT SMAPE RESULTS FOR ALL VARIABLES\n")
cat("================================================================\n")
for (var_name in names(aggregate_smape_results_step)) {
  cat("\n----------------------------------------------------------------\n")
  cat("AGGREGATED HOLD-OUT SMAPE SUMMARY FOR", toupper(var_name), "\n")
  cat("----------------------------------------------------------------\n")
  print(round(aggregate_smape_results_step[[var_name]], 4))
}
```

# Summary stat
```{r}
all_diagnostics_rwstep <- list()
for(dep_var in names(all_models_rwstep)) {
  all_diagnostics_rwstep[[dep_var]] <- get_model_diagnostics(all_models_rwstep[[dep_var]], rolling_splits[[dep_var]])
}

print_all_diagnostics(all_diagnostics_rwstep)
```

## Function of getting LOOIC 
```{r}
calculate_looic <- function(model, window_data) {
  # Get training sample size 
  n_train <- nrow(window_data$train)
  
  # Calculate LOOIC
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  # normalized LOOIC
  looic <- (-2 * sum(loo_liks)) / n_train
  
  return(looic)
}

# Function to apply for all windows
get_all_looic <- function(models, windows) {
  n_windows <- length(models)
  looic_results <- numeric(n_windows)
  
  for (i in 1:n_windows) {
    looic_results[i] <- calculate_looic(models[[i]], windows[[i]])
  }
  
  return(looic_results)
}
all_looic_rwstep <- list()
for (dep_var in names(all_models_rwstep)) {
  all_looic_rwstep[[dep_var]] <- get_all_looic(
    all_models_rwstep[[dep_var]], 
    rolling_splits[[dep_var]]
  )
}
# Display results
for (dep_var in names(all_looic_rwstep)) {
  cat(sprintf("\nNormalized LOOIC Results for %s\n", dep_var))
  for (i in 1:length(all_looic_rwstep[[dep_var]])) {
    cat(sprintf("Window %d: %.4f\n", i, all_looic_rwstep[[dep_var]][i]))
  }
}
```

## 95% confi interval
```{r}
confidence_intervals_step <- calculate_confidence_intervals(all_models_rwstep, rolling_splits)
```

## mcmc size 
```{r}
mcmc_stats_step <- get_mcmc_stats(all_models_rwstep,  rolling_splits)
```

## Bayesian R²  (Gelman et al.)

```{r}
## Loop over each dependent variable and each window to compute Bayesian R²
bayesian_R2_results_step <- list()

for (dep_var_name in names(all_models_rwstep)) {
  models_list <- all_models_rwstep[[dep_var_name]]
  
  # Data frame to store summary metrics for each window
  bayes_R2_summary_step <- data.frame(
    Window = integer(),
    Mean_R2 = numeric(),
    Median_R2 = numeric(),
    Lower_R2 = numeric(),
    Upper_R2 = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Loop through each of the 7 expanding windows
  for (i in 1:length(models_list)) {
    # Get the training data for the current window
    train_data <- rolling_splits[[dep_var_name]][[i]]$train
    
    # Extract the fitted model for this window
    model <- models_list[[i]]
    
    # Compute the Bayesian R² draws for the training set
    R2_draws <- calculate_bayes_R2(model, train_data, burn = 100)
    
    # Summarize the draws (you can adjust the summary metrics as desired)
    bayes_R2_summary_step <- rbind(
      bayes_R2_summary_step,
      data.frame(
        Window = i,
        Mean_R2 = mean(R2_draws),
        Median_R2 = median(R2_draws),
        Lower_R2 = quantile(R2_draws, 0.025),
        Upper_R2 = quantile(R2_draws, 0.975)
      )
    )
  }
  
  # Store the summary for the current dependent variable
  bayesian_R2_results_step[[dep_var_name]] <- bayes_R2_summary
}

## Print the Bayesian R² summary for each dependent variable and window
for (dep_var_name in names(bayesian_R2_results_step)) {
  cat("\nBayesian R² for", dep_var_name, ":\n")
  print(round(bayesian_R2_results_step[[dep_var_name]], 4))
}

```
## Ljung Box 
```{r}
# Calculate the Ljung-Box diagnostics (with default lag = 10)
ljung_box_results_step <- get_ljung_box_diagnostics(all_models_rwstep, rolling_splits, lags = 10)

# Print the Ljung-Box test results for each dependent variable
for (dep_var in names(ljung_box_results_step)) {
  cat("\nLjung-Box Test Diagnostics for", dep_var, ":\n")
  print(ljung_box_results_step[[dep_var]])
}
```

## Posterior Prob
```{r}
pips_results_step <- analyze_pips(all_models_rwstep, rolling_splits)
```

## Geweke’s Diagnostic
```{r}
# Apply 
geweke_results <- calculate_geweke(all_models_rwstep, rolling_splits)
```

## Heidelberger-Welch
```{r}
for(dep_var in names(all_models_rwstep)) {
  cat("\n\n=== Results for", dep_var, "===\n")
  heidel_results_step = calculate_heidel_welch(all_models_rwstep[[dep_var]], rolling_splits[[dep_var]])
}
```


## Raftery_lewis
```{r}
# Run diagnostics
for(dep_var in names(all_models_rwstep)) {
  cat("\n\n=== Convergence Diagnostics for", dep_var, "===\n")
  calculate_convergence_diagnostics(all_models_rwstep[[dep_var]], rolling_splits[[dep_var]])
}
```

# Predicitive interval
```{r}

# Apply the function with rolling window splits
get_predictive_intervals(
  all_evaluations_rwstep, 
  all_models_rwstep, 
  extended_test_sets_rolling
)

```

## log predictive density
```{r}
calculate_lpd <- function(all_models, rolling_splits) {
 lpd_results <- list()
 
 for(dep_var in names(all_models)) {
   window_lpd <- data.frame()
   
   for(i in 1:4) {
     model <- all_models[[dep_var]][[i]]
     test_data <- rolling_splits[[dep_var]][[i]]$test
     actual <- test_data[, 1]
     
     # Get predictions
     pred <- predict.bsts(model, newdata = as.data.frame(test_data[, -1]), burn = 100)
     
     # Calculate LPD for each observation
     lpd_values <- numeric(length(actual))
     for(j in seq_along(actual)) {
       # Get density estimate of prediction distribution
       density_est <- density(pred$distribution[,j])
       # Find density at actual value
       actual_density <- approx(density_est$x, density_est$y, xout = actual[j])$y
       # Log density
       lpd_values[j] <- log(actual_density)
     }
     
     window_lpd <- rbind(window_lpd, data.frame(
       Window = i,
       LPD_Mean = mean(lpd_values, na.rm = TRUE),
       LPD_SD = sd(lpd_values, na.rm = TRUE)
     ))
   }
   lpd_results[[dep_var]] <- window_lpd
 }
 
 # Print results
 for(dep_var in names(lpd_results)) {
   cat("\n=====================================")
   cat(sprintf("\n%s - Log Predictive Density\n", dep_var))
   cat("=====================================\n")
   print(round(lpd_results[[dep_var]], 4))
 }
 
 return(lpd_results)
}

# Apply function
lpd_scores_step <- calculate_lpd(all_models_rwstep, rolling_splits)
```
## MCMC trace plot 
```{r}

## MCMC trace plot for Rolling Window Analysis
# Function to find top 2 best models based on evaluations (Rolling Window)
find_best_models_rolling <- function(all_evaluations) {
  best_models <- list()  # To store the best models for each dependent variable
  
  for(var_name in names(all_evaluations)) {
    cat("\nFor", var_name, ":\n")
    
    # Collect all sMAPE values
    all_smape_results <- data.frame(
      Window = integer(),
      Method = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Collect mean-based sMAPE values
    for(i in 1:4) {  
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "mean",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: mean_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Collect median-based sMAPE values
    for(i in 1:4) {  # Assuming 6 possible window sizes
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "median",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: median_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Sort by sMAPE (ascending) to find the best models overall
    all_smape_results <- all_smape_results[order(all_smape_results$SMAPE), ]
    
    if (nrow(all_smape_results) >= 2) {
      # Get top 2 models
      top_2_models <- all_smape_results[1:2, ]
      
      cat("Top 2 models for", var_name, "based on sMAPE:\n")
      for (i in 1:2) {
        cat("Rank", i, ": Window", top_2_models$Window[i], "using", top_2_models$Method[i], 
            "method with sMAPE =", round(top_2_models$SMAPE[i], 4), "\n")
      }
      
      # Store the top 2 models info
      best_models[[var_name]] <- list(
        best_window_1 = top_2_models$Window[1],
        best_method_1 = top_2_models$Method[1],
        best_smape_1 = top_2_models$SMAPE[1],
        best_window_2 = top_2_models$Window[2],
        best_method_2 = top_2_models$Method[2],
        best_smape_2 = top_2_models$SMAPE[2]
      )
    } else if (nrow(all_smape_results) == 1) {
      # Handle case where only one valid model is found
      cat("Only one valid model found for", var_name, ":\n")
      cat("Window:", all_smape_results$Window[1], "using", all_smape_results$Method[1], 
          "method with sMAPE =", round(all_smape_results$SMAPE[1], 4), "\n")
      
      best_models[[var_name]] <- list(
        best_window_1 = all_smape_results$Window[1],
        best_method_1 = all_smape_results$Method[1],
        best_smape_1 = all_smape_results$SMAPE[1]
      )
    } else {
      cat("No valid models found for", var_name, "\n")
    }
  }
  
  return(best_models)
}

# Analyze MCMC trace plots for Rolling Window
analyze_bsts_mcmc_trace_plots_rolling <- function(all_evaluations, all_models_rwstep, best_models) {
  # Create a PDF device to save all plots
  pdf("MCMC_trace_plot_RW_step_2016Q4.pdf", width=12, height=10)
  
  for(dep_var in names(best_models)) {
    # Get the info for the top models
    best_model_info <- best_models[[dep_var]]
    
    # Process Rank 1 model
    window_1 <- best_model_info$best_window_1
    method_1 <- best_model_info$best_method_1
    smape_1 <- best_model_info$best_smape_1
    
    cat("\nCreating MCMC trace plots for", dep_var, ":\n")
    cat("Rank 1: Window:", window_1, "using", method_1, "method, sMAPE =", round(smape_1, 4), "\n")
    
    # Fetch the model
    model_1 <- all_models_rwstep[[dep_var]][[window_1]]
    
    # Create trace plots for Rank 1 model
    create_trace_plots(model_1, dep_var, "Rank 1", window_1, method_1, smape_1)
    
    # Process Rank 2 model if available
    if(!is.null(best_model_info$best_window_2)) {
      window_2 <- best_model_info$best_window_2
      method_2 <- best_model_info$best_method_2
      smape_2 <- best_model_info$best_smape_2
      
      cat("Rank 2: Window:", window_2, "using", method_2, "method, sMAPE =", round(smape_2, 4), "\n")
      
      # Fetch the second-ranked model
      model_2 <- all_models_rwstep[[dep_var]][[window_2]]
      
      # Create trace plots for Rank 2 model
      create_trace_plots(model_2, dep_var, "Rank 2", window_2, method_2, smape_2)
    } else {
      cat("No second-best model available for", dep_var, "\n")
    }
  }
  
  dev.off()
}

# Helper function to create trace plots for a model (unchanged from previous version)
create_trace_plots <- function(model, dep_var, rank_label, window, method, smape) {
  tryCatch({
    # Set up plot for state variances
    par(mfrow=c(3,1), mar=c(4,4,3,1))
    
    # Plot observation variance
    if (!is.null(model$sigma.obs)) {
      plot(1:length(model$sigma.obs), model$sigma.obs, type="l", col="blue",
           main="Observation Variance (sigma.obs)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot seasonal variance if it exists
    if (!is.null(model$sigma.seasonal.4)) {
      plot(1:length(model$sigma.seasonal.4), model$sigma.seasonal.4, type="l", col="red",
           main="Seasonal Variance (sigma.seasonal.4)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot trend level variance if it exists
    if (!is.null(model$sigma.trend.level)) {
      plot(1:length(model$sigma.trend.level), model$sigma.trend.level, type="l", col="green",
           main="Trend Level Variance (sigma.trend.level)",
           xlab="Iteration", ylab="Value")
    }
    
    mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Variances"), 
          outer=TRUE, line=-1.5, cex=1.2)
    
    # If the model has regression coefficients, plot those on a new page
    if (!is.null(model$coefficients) && !is.null(model$coefficients.samples) && 
        ncol(model$coefficients.samples) > 0) {
      
      # Calculate number of coefficient plots needed
      num_coefs <- ncol(model$coefficients.samples)
      rows_needed <- min(4, num_coefs)  # Maximum 4 coefficients per page
      
      # Start a new page
      par(mfrow=c(rows_needed, 1), mar=c(4,4,3,1))
      
      # Plot each coefficient
      for (i in 1:rows_needed) {
        coef_name <- colnames(model$coefficients.samples)[i]
        if (is.null(coef_name)) coef_name <- paste("Coefficient", i)
        
        coef_samples <- model$coefficients.samples[,i]
        plot(1:length(coef_samples), coef_samples, type="l", col="purple",
             main=paste("Regression Coefficient:", coef_name),
             xlab="Iteration", ylab="Value")
      }
      
      mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Coefficients"), 
            outer=TRUE, line=-1.5, cex=1.2)
    }
    
  }, error = function(e) {
    # If error occurs, print the error and try the built-in plotting function
    cat("Error in custom trace plotting for", rank_label, ":", e$message, "\n")
    cat("Falling back to built-in plotting...\n")
    
    # Reset the plot area
    par(mfrow=c(1,1))
    
    # Try the built-in plotting method
    tryCatch({
      # Try plotting state components instead
      plot(model, "components")
      title(main=paste0(dep_var, " (", rank_label, "): Component Contributions"))
    }, error = function(e2) {
      cat("Error in fallback plotting:", e2$message, "\n")
      
      # Create an empty plot with error message
      plot(1, 1, type="n", axes=FALSE, xlab="", ylab="")
      text(1, 1, paste("Error plotting MCMC traces for", rank_label), col="red", cex=1.5)
    })
  })
}

# Run the analysis for rolling window setting
best_models_rolling <- find_best_models_rolling(all_evaluations_rwstep)
analyze_bsts_mcmc_trace_plots_rolling(all_evaluations_rwstep, all_models_rwstep, best_models_rolling)
```

## Distribution Plot
```{r}
create_prediction_plots <- function(all_evaluations, all_models, extended_test_sets_rolling, hold_out_dataset) {
  library(ggplot2)
  pdf("Prediction_Distribution_RW_Step_2016Q4.pdf", width = 12, height = 8)
  
  for(dep_var in names(all_evaluations)) {
    # Create a data frame of all SMAPE values (mean and median) per window
    all_results <- data.frame(
      Window = numeric(),
      Type = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    for(i in 1:length(all_evaluations[[dep_var]])) {
      # Add mean results
      mean_smape <- all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE
      if(!is.na(mean_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "mean", SMAPE = mean_smape, stringsAsFactors = FALSE))
      }
      # Add median results
      median_smape <- all_evaluations[[dep_var]][[i]]$errors_median$SMAPE
      if(!is.na(median_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "median", SMAPE = median_smape, stringsAsFactors = FALSE))
      }
    }
    
    # Sort by SMAPE (ascending) and select the top 4 rows
    top_4 <- all_results[order(all_results$SMAPE), ][1:4, ]
    
    plot_data <- data.frame()
    label_list <- c()  # to track labels and catch duplicates
    
    for(i in 1:nrow(top_4)) {
      window <- top_4$Window[i]
      model_type <- top_4$Type[i]
      smape_value <- top_4$SMAPE[i]
      
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(extended_test_sets_rolling[[dep_var]][[window]][nrow(extended_test_sets_rolling[[dep_var]][[window]]), -1, drop = FALSE]),
                           burn = 100)
      
      # Create a label that includes rank, window, type, and SMAPE.
      base_label <- paste("Rank", i, ": Window", window, "-", model_type, "(SMAPE:", round(smape_value, 4), ")")
      # If the same label already exists, append a suffix.
      duplicate_count <- sum(label_list == base_label)
      if(duplicate_count > 0) {
        label <- paste0(base_label, " - Copy", duplicate_count + 1)
      } else {
        label <- base_label
      }
      label_list <- c(label_list, label)
      
      plot_data <- rbind(plot_data, 
                         data.frame(Value = pred$distribution[,1],
                                    Model = factor(label),
                                    stringsAsFactors = FALSE))
    }
    
    # Get the actual observed value from the hold_out_dataset (assume it's the last value in column 1)
    actual_value <- tail(hold_out_dataset[[dep_var]][, 1], 1)
    
    p <- ggplot(plot_data, aes(x = Value, fill = Model)) +
      geom_density(alpha = 0.4) +
      theme_minimal() +
      labs(title = paste(dep_var, "- Top 4 Models Based on sMAPE"),
           x = "Predicted Value",
           y = "Density") +
      geom_vline(xintercept = actual_value, color = "red", linetype = "dashed", size = 1) +
      annotate("text", x = actual_value, y = Inf, label = paste("Actual:", actual_value),
               vjust = -0.5, color = "red", size = 3)
    
    print(p)
  }
  
  dev.off()
}


# Apply the function
create_prediction_plots(all_evaluations_rwstep, all_models_rwstep, extended_test_sets_rolling, hold_out_dataset)
```

## Continuous Ranked Probability Score (CRPS)
```{r}
crps_scores_custom_rolling_step <- get_crps_scores_custom_rolling(all_models_rwstep, Indice_testing_dataset)
```

## Sensitivity 
```{r}
# First, clean up any existing parallel connections
try(stopImplicitCluster(), silent = TRUE)
try(stopCluster(cl), silent = TRUE)
closeAllConnections()
gc()  # Force garbage collection

# Load required packages
library(bsts)
library(parallel)

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to identify the best models
get_best_models_safe <- function() {
  best_predictions <- list()
  
  for (dep_var in names(all_models_rwstep)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:6) {
      if (!dep_var %in% names(all_models_rwstep) || length(all_models_rwstep[[dep_var]]) < i) {
        next
      }
      
      tryCatch({
        model <- all_models_rwstep[[dep_var]][[i]]
        
        # Get the testing data for this window and variable
        test_data <- rolling_splits[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        mape_mean <- mean(abs((actual_values - pred_mean) / actual_values)) * 100
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                          (abs(actual_values) + abs(pred_mean))) * 100
        
        mape_median <- mean(abs((actual_values - pred_median) / actual_values)) * 100
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                            (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to results
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = mape_mean, SMAPE = smape_mean),
                       data.frame(Window = i, Method = "median", 
                                MAPE = mape_median, SMAPE = smape_median))
      }, error = function(e) {
        cat("Error processing", dep_var, "window", i, ":", e$message, "\n")
      })
    }
    
    # Select the best five models
    if (nrow(results) > 0) {
      best_five <- results[order(results$MAPE), ][1:min(5, nrow(results)), ]
      best_predictions[[dep_var]] <- best_five
    }
  }
  
  return(best_predictions)
}

# Function to analyze one configuration
analyze_config <- function(params) {
  dep_var <- params$dep_var
  window_size <- params$window_size
  method <- params$method
  sigma <- params$sigma
  slab_var <- params$slab_var
  
  cat("Processing", dep_var, "window", window_size, "method", method, 
      "sigma", sigma, "slab_var", slab_var, "\n")
  
  # Get training and testing data
  train_data <- rolling_splits[[dep_var]][[window_size]]$train
  test_data <- rolling_splits[[dep_var]][[window_size]]$test
  
  # Extract values
  y <- train_data[, 1]
  X <- as.data.frame(train_data[, -1])
  actual_values <- test_data[, 1]
  test_predictors <- as.data.frame(test_data[, -1])
  
  # Create formula
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Full data for model
  model_data <- as.data.frame(cbind(y = y, X))
  
  # Create state specification
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y, 
                           level.sigma.prior = SdPrior(sigma = sigma),
                           slope.sigma.prior = SdPrior(sigma = sigma))
  
  # Fit model
  result <- tryCatch({
    # Using the approach that worked in our test
    model <- bsts(formula, 
                 state.specification = ss,
                 niter = 10000,
                 data = model_data)
    
    # Make predictions
    all_predictions <- numeric(length(actual_values))
    
    for (t in 1:length(actual_values)) {
      # Prepare data for this period
      pred_data <- data.frame(y = NA)
      pred_data <- cbind(pred_data, test_predictors[t, , drop = FALSE])
      
      # Predict
      single_pred <- predict(model, newdata = pred_data, burn = 100)
      
      # Extract prediction
      if (method == "mean") {
        all_predictions[t] <- mean(single_pred$distribution[, 1])
      } else {
        all_predictions[t] <- median(single_pred$distribution[, 1])
      }
    }
    
    # Check if predictions vary
    has_varying_predictions <- length(unique(round(all_predictions, 2))) > 1
    
    # Calculate metrics
    mape <- mean(abs((actual_values - all_predictions) / actual_values)) * 100
    smape <- mean(2 * abs(actual_values - all_predictions) / 
                 (abs(actual_values) + abs(all_predictions))) * 100
    
    waic <- NA
    looic <- NA
    if (!is.null(model$log.likelihood)) {
      waic <- -2 * mean(model$log.likelihood)
      looic <- -2 * mean(model$log.likelihood)
    }
    
    return(list(
      success = TRUE,
      sigma = sigma,
      slab_var = slab_var,
      mape = mape,
      smape = smape,
      waic = waic,
      looic = looic,
      has_regression = has_varying_predictions
    ))
    
  }, error = function(e) {
    return(list(
      success = FALSE,
      error = e$message
    ))
  })
  
  return(result)
}

# Controlled parallel approach
run_sensitivity_analysis_parallel <- function() {
  # Define parameter combinations
  sigmas <- c(0.2, 0.4, 0.6, 0.8)
  slab_vars <- c(50, 100, 200)
  
  # Get best models
  best_models <- get_best_models_safe()
  
  # Initialize results
  sensitivity_results <- list()
  
  # For each dependent variable
  for (dep_var in names(best_models)) {
    cat("\n=== Starting analysis for", dep_var, "===\n")
    sensitivity_results[[dep_var]] <- list()
    models_info <- best_models[[dep_var]]
    
    # For each top model
    for (i in 1:min(5, nrow(models_info))) {
      model_info <- models_info[i, ]
      window_size <- model_info$Window
      method <- as.character(model_info$Method)
      model_name <- paste0("model_", i)
      
      cat("\n--- Processing", dep_var, "window", window_size, "method", method, "---\n")
      
      # Initialize results for this model
      model_results <- list()
      
      # Create parameter combinations
      param_list <- list()
      for (sigma in sigmas) {
        for (slab_var in slab_vars) {
          param_list[[length(param_list) + 1]] <- list(
            dep_var = dep_var,
            window_size = window_size,
            method = method,
            sigma = sigma,
            slab_var = slab_var
          )
        }
      }
      
      # Create a small cluster - use just 2 or 3 cores for stability
      num_cores <- min(3, detectCores() - 1)
      cat("Using", num_cores, "cores for parallel processing\n")
      cl <- makeCluster(num_cores)
      
      # Export required data and functions
      clusterExport(cl, c("rolling_splits"), envir = .GlobalEnv)
      clusterEvalQ(cl, {
        library(bsts)
      })
      
      # Run analysis in parallel
      config_results <- parLapply(cl, param_list, analyze_config)
      
      # Clean up
      stopCluster(cl)
      
      # Process results
      for (j in 1:length(param_list)) {
        params <- param_list[[j]]
        result <- config_results[[j]]
        
        config_name <- paste0("sigma_", params$sigma, "_slab_", params$slab_var)
        
        if (result$success) {
          # Store successful result
          model_results[[config_name]] <- list(
            sigma = params$sigma,
            slab_var = params$slab_var,
            mape = result$mape,
            smape = result$smape,
            waic = result$waic,
            looic = result$looic,
            has_regression = result$has_regression
          )
          
          cat("Configuration", config_name, "- MAPE:", round(result$mape, 4),
             "SMAPE:", round(result$smape, 4), "Has regression:", result$has_regression, "\n")
        } else {
          cat("Configuration", config_name, "failed:", result$error, "\n")
        }
      }
      
      # Store results for this model
      sensitivity_results[[dep_var]][[model_name]] <- list(
        window = window_size,
        method = method,
        results = model_results
      )
      
      # Find best configuration
      best_config <- NULL
      best_mape <- Inf
      best_config_name <- ""
      
      for (config_name in names(model_results)) {
        config <- model_results[[config_name]]
        if (!is.na(config$mape) && config$mape < best_mape) {
          best_mape <- config$mape
          best_config <- config
          best_config_name <- config_name
        }
      }
      
      if (is.null(best_config)) {
        cat("No valid configurations found.\n")
      } else {
        # Print best configuration
        cat("\nBest configuration:", best_config_name, "\n")
        cat("MAPE:", round(best_config$mape, 4), "\n")
        cat("SMAPE:", round(best_config$smape, 4), "\n")
        cat("Sigma:", best_config$sigma, "\n")
        cat("Slab variance:", best_config$slab_var, "\n")
        cat("Has regression:", best_config$has_regression, "\n\n")
        
        # Create summary table
        config_summary <- data.frame(
          Sigma = numeric(),
          SlabVar = numeric(),
          MAPE = numeric(),
          SMAPE = numeric(),
          WAIC = numeric(),
          LOOIC = numeric(),
          HasRegression = logical()
        )
        
        for (config_name in names(model_results)) {
          config <- model_results[[config_name]]
          config_summary <- rbind(config_summary, 
                                data.frame(
                                  Sigma = config$sigma,
                                  SlabVar = config$slab_var,
                                  MAPE = config$mape,
                                  SMAPE = config$smape,
                                  WAIC = config$waic,
                                  LOOIC = config$looic,
                                  HasRegression = config$has_regression
                                ))
        }
        
        # Set row names and sort
        rownames(config_summary) <- names(model_results)
        config_summary <- config_summary[order(config_summary$MAPE), ]
        print(round(config_summary, 4))
      }
      
      # Clean up after each model
      rm(model_results, config_results)
      gc()
    }
  }
  
  return(sensitivity_results)
}

# Run the analysis
start_time <- Sys.time()
cat("Starting sensitivity analysis at:", format(start_time), "\n")

sensitivity_results <- run_sensitivity_analysis_parallel()

end_time <- Sys.time()
execution_time <- end_time - start_time
cat("\nSensitivity analysis completed in:", format(execution_time), "\n")
```

```{r}
# Function to identify the top 5 models by SMAPE
find_top_models <- function(all_models_rwstep, sensitivity_results) {
  top_models_summary <- list()
  
  # Process each dependent variable
  for(dep_var in names(sensitivity_results)) {
    cat("\n================================================================\n")
    cat("TOP 5 MODELS FOR", toupper(dep_var), "BASED ON SMAPE\n")
    cat("================================================================\n")
    
    # 1. Collect all model results including window performance and sensitivity analysis
    all_models <- data.frame(
      Source = character(),
      Window = integer(),
      Method = character(),
      Sigma = numeric(),
      SlabVar = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Add window performance results (from best_models)
    for (i in 1:6) { # Assuming 6 windows
      if (dep_var %in% names(all_models_rwstep) && length(all_models_rwstep[[dep_var]]) >= i) {
        # Get the testing data for this window and variable
        test_data <- rolling_splits[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(all_models_rwstep[[dep_var]][[i]], newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate SMAPE for mean
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                         (abs(actual_values) + abs(pred_mean))) * 100
        
        # Calculate SMAPE for median
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                           (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to all_models dataframe
        all_models <- rbind(all_models, 
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "mean",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_mean,
                            stringsAsFactors = FALSE
                          ),
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "median",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_median,
                            stringsAsFactors = FALSE
                          ))
      }
    }
    
    # Add sensitivity analysis results
    if (dep_var %in% names(sensitivity_results)) {
      for (model_name in names(sensitivity_results[[dep_var]])) {
        model_data <- sensitivity_results[[dep_var]][[model_name]]
        window <- model_data$window
        method <- model_data$method
        
        for (config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_models <- rbind(all_models,
                            data.frame(
                              Source = "Sensitivity",
                              Window = window,
                              Method = method,
                              Sigma = config$sigma,
                              SlabVar = config$slab_var,
                              SMAPE = config$smape,
                              stringsAsFactors = FALSE
                            ))
        }
      }
    }
    
    # 2. Sort by SMAPE and select top 5 models
    top_models <- all_models[order(all_models$SMAPE), ][1:min(5, nrow(all_models)), ]
    
    # 3. Create a formatted output
    formatted_models <- data.frame(
      Rank = 1:nrow(top_models),
      Description = character(nrow(top_models)),
      SMAPE = top_models$SMAPE,
      stringsAsFactors = FALSE
    )
    
    for (i in 1:nrow(top_models)) {
      model <- top_models[i, ]
      
      if (model$Source == "Window") {
        desc <- sprintf("%s Window %d", model$Method, model$Window)
      } else {
        desc <- sprintf("%s Window %d (sigma=%.1f, slab=%.0f)", 
                      model$Method, model$Window, model$Sigma, model$SlabVar)
      }
      
      formatted_models$Description[i] <- desc
    }
    
    # Display the top models
    print(formatted_models)
    
    # Store the results
    top_models_summary[[dep_var]] <- list(
      top_models = top_models,
      formatted_output = formatted_models
    )
  }
  
  return(top_models_summary)
}

# Example usage:
 top_model_results <- find_top_models(all_models_rwstep, sensitivity_results)
```

```{r}
# Function to predict holdout period using top 5 models and create weighted ensemble
predict_and_create_ensemble <- function(all_models_rwstep, top_model_results, hold_out_dataset) {
  # Clean up any existing connections
  try(stopImplicitCluster(), silent = TRUE)
  try(stopCluster(cl), silent = TRUE)
  closeAllConnections()
  gc()  # Force garbage collection
  
  # Load required packages
  library(parallel)
  
  # To store all results
  all_predictions <- list()
  
  # For each dependent variable
  for(dep_var in names(top_model_results)) {
    cat("\n================================================================\n")
    cat("HOLDOUT PREDICTIONS FOR", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Get top models for this variable
    top_models <- top_model_results[[dep_var]]$top_models
    
    # Get holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      cat("Holdout data not available for", dep_var, "\n")
      next
    }
    
    holdout_data <- hold_out_dataset[[dep_var]]
    
    # Prepare parameters for parallel execution
    param_list <- list()
    for(i in 1:nrow(top_models)) {
      param_list[[i]] <- list(
        dep_var = dep_var,
        model_info = top_models[i, ],
        model_index = i
      )
    }
    
    # Create a small cluster - use just 2 or 3 cores for stability
    num_cores <- min(3, detectCores() - 1)
    cat("Using", num_cores, "cores for parallel processing\n")
    cl <- makeCluster(num_cores)
    
    # Export required data and functions
    clusterExport(cl, c("all_models_rwstep", "hold_out_dataset", "rolling_splits"), envir = .GlobalEnv)
    clusterEvalQ(cl, {
      library(bsts)
    })
    
    # Worker function for parallel execution
    predict_model_worker <- function(params) {
      dep_var <- params$dep_var
      model_info <- params$model_info
      model_index <- params$model_index
      
      # Create result container
      result <- list(
        model_index = model_index,
        dep_var = dep_var,
        success = FALSE
      )
      
      # Get the holdout data
      holdout_data <- hold_out_dataset[[dep_var]]
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Create period labels
      period_labels <- c(as.character(80:86), "Q4")#period
      
      # Create model description
      if(model_info$Source == "Window") {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, ")")
      } else {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, 
                          ", sigma=", model_info$Sigma, 
                          ", slab=", model_info$SlabVar, ")")
      }
      
      tryCatch({
        # Get or create model based on source
        if(model_info$Source == "Window") {
          # Use existing model
          window <- model_info$Window
          method <- as.character(model_info$Method)
          
          if(!(dep_var %in% names(all_models_rwstep)) || length(all_models_rwstep[[dep_var]]) < window) {
            return(c(result, list(error = "Model not available")))
          }
          
          model <- all_models_rwstep[[dep_var]][[window]]
          
        } else {
          # Create model with sensitivity parameters
          window <- model_info$Window
          method <- as.character(model_info$Method)
          sigma <- model_info$Sigma
          slab_var <- model_info$SlabVar
          
          # Get original model
          if(!(dep_var %in% names(all_models_rwstep)) || length(all_models_rwstep[[dep_var]]) < window) {
            return(c(result, list(error = "Original model not available")))
          }
          
          original_model <- all_models_rwstep[[dep_var]][[window]]
          
          # Get training data
          train_data <- rolling_splits[[dep_var]][[window]]$train
          y <- train_data[, 1]
          X <- as.data.frame(train_data[, -1])
          
          # Create formula
          predictors <- colnames(X)
          formula_str <- paste("y ~", paste(predictors, collapse = " + "))
          formula <- as.formula(formula_str)
          
          # Create model data
          model_data <- as.data.frame(cbind(y = y, X))
          
          # Set up state specification
          ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
          ss <- AddLocalLinearTrend(ss, y, 
                                  level.sigma.prior = SdPrior(sigma = sigma),
                                  slope.sigma.prior = SdPrior(sigma = sigma))
          
          # Create model
          model <- bsts(formula, 
                       state.specification = ss,
                       niter = 10000,
                       data = model_data)
        }
        
        # Make predictions - using individual predictions for each period
        predictions <- numeric(nrow(holdout_data))
        
        for(t in 1:nrow(holdout_data)) {
          # Create data for this period
          if(model_info$Source == "Window") {
            # For window models
            this_period_data <- pred_data[t, , drop = FALSE]
            
            # Make prediction
            single_pred <- predict(model, newdata = this_period_data, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          } else {
            # For sensitivity models
            pred_row <- data.frame(y = NA)
            pred_row <- cbind(pred_row, pred_data[t, , drop = FALSE])
            
            # Make prediction
            single_pred <- predict(model, newdata = pred_row, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          }
        }
        
        # Calculate errors
        abs_errors <- abs(predictions - actual_values)
        avg_abs_error <- mean(abs_errors)
        
        # Return successful result
        return(list(
          model_index = model_index,
          dep_var = dep_var,
          success = TRUE,
          model_desc = model_desc,
          predictions = predictions,
          actual_values = actual_values,
          abs_errors = abs_errors,
          avg_abs_error = avg_abs_error,
          period_labels = period_labels,
          smape = model_info$SMAPE
        ))
        
      }, error = function(e) {
        return(c(result, list(error = e$message)))
      })
    }
    
    # Run predictions in parallel
    model_results <- parLapply(cl, param_list, predict_model_worker)
    
    # Stop cluster
    stopCluster(cl)
    
    # Process and display results
    var_predictions <- list()
    
    for(result in model_results) {
      if(result$success) {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, ":", result$model_desc, "\n")
        cat("----------------------------------------------------------------\n")
        
        # Print results in a tabular format
        cat("\nPeriod\tActual\t\tPredicted\tAbsError\n")
        for(p in 1:length(result$period_labels)) {
          cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                    result$period_labels[p], 
                    result$actual_values[p], 
                    result$predictions[p], 
                    result$abs_errors[p]))
        }
        
        cat("\nAverage Absolute Error:", round(result$avg_abs_error, 2), "\n")
        
        # Store successful predictions
        var_predictions[[result$model_index]] <- result
      } else {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, "failed:", result$error, "\n")
        cat("----------------------------------------------------------------\n")
      }
    }
    
    # Store predictions for this variable
    all_predictions[[dep_var]] <- var_predictions
    
    # Now create weighted ensemble
    successful_models <- model_results[sapply(model_results, function(r) r$success)]
    
    if(length(successful_models) > 0) {
      # Get the actual values and period labels from the first successful model
      actual_values <- successful_models[[1]]$actual_values
      period_labels <- successful_models[[1]]$period_labels
      
      # Create prediction matrix
      pred_matrix <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      smape_values <- numeric(length(successful_models))
      model_descriptions <- character(length(successful_models))
      
      # Fill the matrix with predictions
      for(i in 1:length(successful_models)) {
        pred_matrix[, i] <- successful_models[[i]]$predictions
        smape_values[i] <- successful_models[[i]]$smape
        model_descriptions[i] <- successful_models[[i]]$model_desc
      }
      
      # Calculate weights based on inverse SMAPE
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      
      # Display the weights
      cat("\n================================================================\n")
      cat("WEIGHTED ENSEMBLE FOR", toupper(dep_var), "\n")
      cat("================================================================\n")
      
      cat("\nModel Weights:\n")
      for(i in 1:length(weights)) {
        cat(sprintf("Model %d: %s - Weight: %.4f (SMAPE: %.4f)\n", 
                  i, model_descriptions[i], weights[i], smape_values[i]))
      }
      
      # Calculate weighted predictions
      weighted_predictions <- numeric(length(actual_values))
      for(i in 1:length(actual_values)) {
        weighted_predictions[i] <- sum(pred_matrix[i, ] * weights, na.rm = TRUE)
      }
      
      # Calculate errors
      abs_errors <- abs(weighted_predictions - actual_values)
      
      # Calculate ensemble SMAPE
      ensemble_smape <- mean(2 * abs(actual_values - weighted_predictions) / 
                           (abs(actual_values) + abs(weighted_predictions))) * 100
      
      # Display results
      cat("\n----------------------------------------------------------------\n")
      cat("WEIGHTED ENSEMBLE RESULTS\n")
      cat("----------------------------------------------------------------\n")
      cat("Ensemble SMAPE:", round(ensemble_smape, 4), "%\n\n")
      
      cat("Period\tActual\t\tPredicted\tAbsError\n")
      for(p in 1:length(period_labels)) {
        cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                  period_labels[p], 
                  actual_values[p], 
                  weighted_predictions[p], 
                  abs_errors[p]))
      }
      
      cat("\nAverage Absolute Error:", round(mean(abs_errors), 2), "\n")
    } else {
      cat("\nNo successful models for", dep_var, "- cannot create ensemble\n")
    }
  }
  
  # Return all predictions
  return(all_predictions)
}

# Run the function to predict and create weighted ensemble
all_results <- predict_and_create_ensemble(all_models_rwstep, top_model_results, hold_out_dataset)
```

```{r}
# This function replicates your 'print_ensemble_tables' logic but returns a named vector
# of Weighted Ensemble SMAPEs (one entry per dep_var).
get_ensemble_smapes <- function(all_results, top_model_results, hold_out_dataset) {
  ensemble_smapes <- numeric(0)  # named vector
  
  for(dep_var in names(top_model_results)) {
    # Skip if no holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      next
    }
    holdout_data  <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    
    # Filter out NULL models
    successful_models <- all_results[[dep_var]]
    successful_models <- successful_models[!sapply(successful_models, is.null)]
    
    if(length(successful_models) > 0) {
      smape_values <- numeric(length(successful_models))
      pred_matrix  <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      
      for(i in seq_along(successful_models)) {
        pred_matrix[, i]  <- successful_models[[i]]$predictions
        smape_values[i]   <- successful_models[[i]]$smape
      }
      
      # Weighted predictions
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      weighted_predictions <- rowSums(t(t(pred_matrix) * weights), na.rm = TRUE)
      
      # Weighted Ensemble SMAPE
      ensemble_smape <- mean(
        2 * abs(actual_values - weighted_predictions) /
        (abs(actual_values) + abs(weighted_predictions))
      ) * 100
      
      # Store in named vector
      ensemble_smapes[dep_var] <- ensemble_smape
    }
  }
  
  return(ensemble_smapes)
}
ensemble_smapes = get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
```

## Influence 
```{r}
if (!requireNamespace("foreach", quietly = TRUE)) {
  install.packages("foreach")
}
if (!requireNamespace("doParallel", quietly = TRUE)) {
  install.packages("doParallel")
}


# Register parallel backend
cores <- detectCores() - 1  # Leave one core free for system processes
registerDoParallel(cores)
cat("Using", cores, "cores for parallel processing\n")

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to get window data
get_window_data <- function(dep_var, window_size) {
  window_data <- rolling_splits[[dep_var]][[window_size]]$train
  return(window_data)
}

# Function to get the best models based on testing dataset performance (changed from holdout)
get_best_models_test <- function(all_models) {
  best_predictions <- list()
  
  for (dep_var in names(all_models)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:length(all_models[[dep_var]])) {
      model <- all_models[[dep_var]][[i]]
      
      # Get the testing data for this window and variable
      test_data <- rolling_splits[[dep_var]][[i]]$test
      actual_values <- test_data[, 1]
      test_predictors <- as.data.frame(test_data[, -1])
      
      # Make predictions for testing period
      tryCatch({
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        errors_mean <- calculate_errors(actual_values, pred_mean)
        errors_median <- calculate_errors(actual_values, pred_median)
        
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = errors_mean$MAPE, SMAPE = errors_mean$SMAPE),
                       data.frame(Window = i, Method = "median", 
                                MAPE = errors_median$MAPE, SMAPE = errors_median$SMAPE))
      }, error = function(e) {
        cat("Error predicting for", dep_var, "window", i, ":", conditionMessage(e), "\n")
      })
    }
    
    # If we have results, select the best two models based on MAPE
    if (nrow(results) > 0) {
      best_two <- results[order(results$MAPE), ][1:min(2, nrow(results)), ]
      best_predictions[[dep_var]] <- best_two
    } else {
      cat("No valid predictions for", dep_var, "- skipping\n")
    }
  }
  
  # Print best models
  cat("\n=== Best Models for Testing Period ===\n")
  for(dep_var in names(best_predictions)) {
    cat("\n", dep_var, ":\n")
    print(best_predictions[[dep_var]])
  }
  
  return(best_predictions)
}

# FIXED Function to calculate influence measures with LOOIC and LPD
calculate_influence_test <- function(model, train_data, method, dep_var, window_size) {
  # Get original training data
  y <- train_data[, 1]
  X <- train_data[, -1]
  n_obs <- length(y)
  
  # Create model matrix for X
  X_matrix <- model.matrix(~ ., data = as.data.frame(X))[, -1]
  
  # Create time labels starting from 1997 Q4
  start_year <- 1997
  time_labels <- paste0(rep(start_year:(start_year + floor(n_obs/4)), each=4)[1:n_obs], 
                       " Q", rep(1:4, length.out=n_obs))
  
  # Results storage
  results <- data.frame(
    time = time_labels,
    pointwise_lpd = numeric(n_obs),
    delta_looic = numeric(n_obs),
    delta_test_forecast = numeric(n_obs)
  )
  
  # Get test data - FIXED to use passed parameters
  test_data <- rolling_splits[[dep_var]][[window_size]]$test
  test_y <- test_data[, 1]
  test_X <- as.data.frame(test_data[, -1])
  
  # Make original predictions for test period
  original_pred <- predict(model, newdata = test_X, burn = 100)
  original_point_preds <- if(method == "median") {
    apply(original_pred$distribution, 2, median)
  } else {
    colMeans(original_pred$distribution)
  }
  
  # Calculate pointwise log predictive density 
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)  # Using number of samples from log-likelihood
  
  # Ensure we have enough samples (safety check)
  if (n_samples == 0) {
    stop("No log-likelihood values available.")
  }

  log_lik_matrix <- matrix(log_lik, nrow = n_samples, ncol = n_obs, byrow = FALSE)
  
  results$pointwise_lpd <- colMeans(log_lik_matrix, na.rm = TRUE)  # Average LPD
  
  # LOOIC Calculation and test prediction changes - Parallelized version
  looic_values <- rep(NA, n_obs)
  delta_forecasts <- rep(NA, n_obs)
  
  # Parallel loop over observations
  parallel_results <- foreach(i = 1:n_obs, 
                           .packages = c("bsts"),
                           .export = c("test_X", "original_point_preds", "method")) %dopar% {
    # Create leave-one-out dataset
    loo_data <- train_data[-i, ]
    y_loo <- loo_data[, 1]
    X_loo <- loo_data[, -1]
    
    # Fit model on leave-one-out data with reduced iterations for speed
    ss <- AddSeasonal(list(), y_loo, nseasons = 4, season.duration = 1)
    ss <- AddLocalLinearTrend(ss, y_loo)
    
    model_loo <- tryCatch({
      bsts(y_loo,
           X = X_loo,
           state.specification = ss,
           niter = 10000,  # Reduced from 10000 for speed
           ping = 0)
    }, 
    error = function(e) {
      return(NULL)  # Return NULL if fitting fails
    })
    
    if (is.null(model_loo)) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    # Calculate LOOIC for leave-out model
    log_lik_loo <- model_loo$log.likelihood
    if (length(log_lik_loo) == 0) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    loo_liks <- numeric(length(log_lik_loo))
    for (j in 1:length(log_lik_loo)) {
      loo_liks[j] <- mean(log_lik_loo[-j], na.rm = TRUE)
    }
    
    looic <- -2 * mean(loo_liks, na.rm = TRUE)
    
    # Calculate change in test forecasts
    # Predict for test period using the leave-one-out model
    pred_loo <- predict(model_loo, newdata = test_X, burn = 100)
    
    point_pred_loo <- if(method == "median") {
      apply(pred_loo$distribution, 2, median)
    } else {
      colMeans(pred_loo$distribution)
    }
    
    # Calculate average change across all test observations
    delta_forecast <- mean(point_pred_loo - original_point_preds)
    
    return(list(looic = looic, delta_forecast = delta_forecast))
  }
  
  # Collect the parallel results
  for (i in 1:n_obs) {
    if (!is.null(parallel_results[[i]])) {
      looic_values[i] <- parallel_results[[i]]$looic
      delta_forecasts[i] <- parallel_results[[i]]$delta_forecast
    }
  }
  
  # Calculate original LOOIC
  looic_original <- -2 * mean(colMeans(log_lik_matrix, na.rm = TRUE))
  
  # Assign results
  results$delta_looic <- looic_values - looic_original  # Calculate delta LOOIC
  results$delta_test_forecast <- delta_forecasts
  
  return(results)
}

# FIXED Function to analyze the top models with test data
analyze_top_models_parallel <- function(best_models, all_models) {
  all_influence_results <- list()
  
  for(dep_var in names(best_models)) {
    cat("\n\nAnalyzing dependent variable:", dep_var)
    dep_influence <- list()
    models <- best_models[[dep_var]]
    
    for(i in 1:nrow(models)) {
      cat("\n\nProcessing model", i, "for", dep_var)
      model_info <- models[i, ]
      
      # Get training data for this window
      window_data <- get_window_data(dep_var, model_info$Window)
      
      # Get the original model 
      original_model <- all_models[[dep_var]][[model_info$Window]]
      
      # Calculate influence measures - FIXED to pass dep_var and window_size
      influence_results <- calculate_influence_test(
        original_model, 
        window_data,
        model_info$Method,
        dep_var,
        model_info$Window
      )
      
      # Add model identifier
      identifier <- paste("Window", model_info$Window, "-", model_info$Method)
      dep_influence[[identifier]] <- influence_results
    }
    
    all_influence_results[[dep_var]] <- dep_influence
  }
  
  return(all_influence_results)
}

# Function to generate comprehensive results tables
generate_influence_summary <- function(influence_results) {
  for(dep_var in names(influence_results)) {
    cat("\n=== Results for", dep_var, "===\n")
    
    for(model_name in names(influence_results[[dep_var]])) {
      cat("\n--- Model:", model_name, "---\n")
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # Round numeric columns to 4 decimal places
      results_df[, 2:4] <- round(results_df[, 2:4], 4)
      
      # Sort by absolute delta_test_forecast to find most influential observations
      sorted_df <- results_df[order(abs(results_df$delta_test_forecast), decreasing = TRUE), ]
      
      cat("Top 25 most influential observations:\n")
      print(head(sorted_df, 25))
      
      cat("\nSummary statistics:\n")
      
      # Calculate summary statistics separately
      lpd_summary <- summary(results_df$pointwise_lpd)
      looic_summary <- summary(results_df$delta_looic)
      forecast_summary <- summary(results_df$delta_test_forecast)
      
      # Print each metric separately to avoid data frame mixing issues
      cat("\nPointwise LPD:\n")
      print(round(lpd_summary, 4))
      cat("Std Dev:", round(sd(results_df$pointwise_lpd, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta LOOIC:\n")
      print(round(looic_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_looic, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta Test Forecast:\n")
      print(round(forecast_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_test_forecast, na.rm = TRUE), 4), "\n")
    }
  }
}

# Run the analysis with parallel processing
run_influence_analysis_parallel <- function() {
  # Start timing
  start_time <- Sys.time()
  cat("Starting parallel influence analysis at:", format(start_time), "\n")
  
  # First get the best models based on test data performance
  best_models <- get_best_models_test(all_models_rwstep)
  
  # Run the influence analysis in parallel
  influence_results <- analyze_top_models_parallel(best_models, all_models_rwstep)
  
  # Generate summary tables
  generate_influence_summary(influence_results)
  
  # End timing
  end_time <- Sys.time()
  execution_time <- end_time - start_time
  cat("\nInfluence analysis completed in:", format(execution_time), "\n")
  
  return(influence_results)
}

# Run the analysis
influence_results_parallel <- run_influence_analysis_parallel()

# Clean up the parallel backend when done
stopImplicitCluster()
```
# Excel
```{r}
create_enhanced_prediction_tables <- function(all_evaluations, all_models, extended_test_sets_rolling, 
                                              crps_scores, lpd_scores, file_path, 
                                              aggregate_smape_results,
                                              ensemble_smapes) {
  dep_vars <- names(all_evaluations)
  wb <- createWorkbook()
  
  for(dep_var in dep_vars) {
    addWorksheet(wb, dep_var)
    
    mean_smape   <- aggregate_smape_results[[dep_var]]$Mean_SMAPE
    median_smape <- aggregate_smape_results[[dep_var]]$Median_SMAPE  # if your code uses Median_SMAPE
    # Use weighted ensemble SMAPE if available
    sensi_smape <- if(!is.null(ensemble_smapes[[dep_var]])) round(ensemble_smapes[[dep_var]], 2) else NA
    
    variable_data <- data.frame(
      Col1 = c(
        dep_var,
        "Holds out period",
        "Prediction based on testing SMAPE",
        "Use Mean",
        "Use Median",
        "Prediction based on Sensitivity"
      ),
      Col2 = c(
        "",
        "2016Q4 - 2018Q2 and 2024Q4",
        "SMAPE",
        round(mean_smape, 2),
        round(median_smape, 2),
        sensi_smape
      ),
      stringsAsFactors = FALSE
    )
    
    writeData(wb, dep_var, variable_data, startRow = 1, startCol = 1, colNames = FALSE)
    setColWidths(wb, dep_var, cols = 1:2, widths = c(25, 20))
    
    start_row <- 8  # row 7 is blank
    
    # Handle potential NULLs in evaluations
    smape_mean <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_mean) || is.null(x$errors_mean$SMAPE)) {
        return(NA)
      }
      return(x$errors_mean$SMAPE)
    })
    
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_median) || is.null(x$errors_median$SMAPE)) {
        return(NA)
      }
      return(x$errors_median$SMAPE)
    })
    
    # Ensure we only include valid values for ranking
    valid_mean_indices <- which(!is.na(smape_mean))
    valid_median_indices <- which(!is.na(smape_median))
    
    # Get top 5 from valid indices
    if(length(valid_mean_indices) >= 5) {
      top_5_mean <- valid_mean_indices[order(smape_mean[valid_mean_indices])[1:5]]
    } else if(length(valid_mean_indices) > 0) {
      top_5_mean <- valid_mean_indices[order(smape_mean[valid_mean_indices])[1:length(valid_mean_indices)]]
    } else {
      top_5_mean <- integer(0)
    }
    
    if(length(valid_median_indices) >= 5) {
      top_5_median <- valid_median_indices[order(smape_median[valid_median_indices])[1:5]]
    } else if(length(valid_median_indices) > 0) {
      top_5_median <- valid_median_indices[order(smape_median[valid_median_indices])[1:length(valid_median_indices)]]
    } else {
      top_5_median <- integer(0)
    }
    
    crps_mean <- numeric(length(all_evaluations[[dep_var]]))
    crps_sd   <- numeric(length(all_evaluations[[dep_var]]))
    for(i in seq_along(all_evaluations[[dep_var]])) {
      window_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        crps_mean[i] <- window_data$CRPS_Mean
        crps_sd[i]   <- window_data$CRPS_SD
      } else {
        crps_mean[i] <- NA
        crps_sd[i]   <- NA
      }
    }
    
    lpd_mean <- numeric(length(all_evaluations[[dep_var]]))
    lpd_sd   <- numeric(length(all_evaluations[[dep_var]]))
    for(i in seq_along(all_evaluations[[dep_var]])) {
      window_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        lpd_mean[i] <- window_data$LPD_Mean
        lpd_sd[i]   <- window_data$LPD_SD
      } else {
        lpd_mean[i] <- NA
        lpd_sd[i]   <- NA
      }
    }
    
    writeData(wb, dep_var, "Predictive interval for Top 5 models based on sMAPE", 
              startRow = start_row, startCol = 1)
    
    writeData(wb, dep_var, "Use Mean", startRow = start_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = start_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = start_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = start_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = start_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = start_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = start_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = start_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = start_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = start_row + 2, startCol = 9)
    
    current_row <- start_row + 3
    for(window in top_5_mean) {
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      
      # Check if the model exists and is a valid BSTS object
      if(is.null(all_models[[dep_var]][[window]]) || !inherits(all_models[[dep_var]][[window]], "bsts")) {
        # Model not available, write NA values
        writeData(wb, dep_var, "Model not available", startRow = current_row, startCol = 2)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      } else {
        # Check if the test data exists
        if(is.null(extended_test_sets_rolling[[dep_var]][[window]]) || 
           nrow(extended_test_sets_rolling[[dep_var]][[window]]) == 0) {
          # Test data not available
          writeData(wb, dep_var, "Test data not available", startRow = current_row, startCol = 2)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        } else {
          # Both model and test data are available
          tryCatch({
            pred_data <- extended_test_sets_rolling[[dep_var]][[window]]
            last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
            
            # Make predictions
            pred <- predict.bsts(all_models[[dep_var]][[window]], 
                                 newdata = as.data.frame(last_row_predictors),
                                 burn = 100)
            dist <- pred$distribution[,1]
            
            writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, mean(dist), startRow = current_row, startCol = 3)
            writeData(wb, dep_var, median(dist), startRow = current_row, startCol = 4)
            writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
          }, error = function(e) {
            # Handle prediction errors
            writeData(wb, dep_var, paste("Error:", e$message), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
          })
        }
        
        # Write performance metrics regardless of prediction success
        writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      }
      
      current_row <- current_row + 1
    }
    
    writeData(wb, dep_var, "Use Median", startRow = current_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = current_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = current_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = current_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = current_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = current_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = current_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = current_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = current_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = current_row + 2, startCol = 9)
    
    current_row <- current_row + 3
    for(window in top_5_median) {
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      
      # Check if the model exists and is a valid BSTS object
      if(is.null(all_models[[dep_var]][[window]]) || !inherits(all_models[[dep_var]][[window]], "bsts")) {
        # Model not available, write NA values
        writeData(wb, dep_var, "Model not available", startRow = current_row, startCol = 2)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
        writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      } else {
        # Check if the test data exists
        if(is.null(extended_test_sets_rolling[[dep_var]][[window]]) || 
           nrow(extended_test_sets_rolling[[dep_var]][[window]]) == 0) {
          # Test data not available
          writeData(wb, dep_var, "Test data not available", startRow = current_row, startCol = 2)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
          writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
        } else {
          # Both model and test data are available
          tryCatch({
            pred_data <- extended_test_sets_rolling[[dep_var]][[window]]
            last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
            
            # Make predictions
            pred <- predict.bsts(all_models[[dep_var]][[window]], 
                                 newdata = as.data.frame(last_row_predictors),
                                 burn = 100)
            dist <- pred$distribution[,1]
            
            writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, mean(dist), startRow = current_row, startCol = 3)
            writeData(wb, dep_var, median(dist), startRow = current_row, startCol = 4)
            writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
          }, error = function(e) {
            # Handle prediction errors
            writeData(wb, dep_var, paste("Error:", e$message), startRow = current_row, startCol = 2)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 3)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 4)
            writeData(wb, dep_var, NA, startRow = current_row, startCol = 5)
          })
        }
        
        # Write performance metrics regardless of prediction success
        writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      }
      
      current_row <- current_row + 1
    }
    
    setColWidths(wb, dep_var, cols = 1, width = 50)
    setColWidths(wb, dep_var, cols = 2:9, width = 15)
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}


###############################################################################
## 3) add_sensitivity_analysis
##    Moves the sensitivity analysis block down by 3 rows and removes any extra row (e.g. SMAPE row)
###############################################################################
add_sensitivity_analysis <- function(sensitivity_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for(dep_var in names(sensitivity_results)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    
    # Calculate the exact row to place the sensitivity analysis
    # Find the last non-empty row of the existing content
    last_content_row <- max(which(!is.na(sheet_data[,1]) & sheet_data[,1] != ""), na.rm = TRUE)
    
    # Add a small gap (3 rows) after the last content
    last_row <- last_content_row + 6
    
    # Create the results data frame
    results_data <- data.frame(
      Window   = integer(),
      Method   = character(),
      Sigma    = numeric(),
      Slab_Var = numeric(),
      WAIC     = numeric(),
      LOOIC    = numeric(),
      MAPE     = numeric(),
      SMAPE    = numeric(),
      stringsAsFactors = FALSE
    )
    
    for (model_name in names(sensitivity_results[[dep_var]])) {
      model_info <- sensitivity_results[[dep_var]][[model_name]]
      window <- model_info$window
      method <- model_info$method
      
      for (result_name in names(model_info$results)) {
        result <- model_info$results[[result_name]]
        results_data <- rbind(results_data, data.frame(
          Window   = window,
          Method   = method,
          Sigma    = if(!is.null(result$sigma)) result$sigma else NA,
          Slab_Var = if(!is.null(result$slab_var)) result$slab_var else NA,
          WAIC     = if(!is.null(result$waic)) result$waic else NA,
          LOOIC    = if(!is.null(result$looic)) result$looic else NA,
          MAPE     = if(!is.null(result$mape)) result$mape else NA,
          SMAPE    = if(!is.null(result$smape)) result$smape else NA,
          stringsAsFactors = FALSE
        ))
      }
    }
    
    # Write the title directly (no mysterious dep var title)
    writeData(wb, dep_var, "Sensitivity Analysis Results", startRow = last_row, startCol = 1)
    
    # Immediately write the table
    writeData(wb, dep_var, results_data, startRow = last_row + 1, startCol = 1, colNames = TRUE)
    
    # Set column widths
    setColWidths(wb, dep_var, cols = 1:8, widths = c(10,10,10,10,12,12,10,10))
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## Fixed add_influence_analysis_to_workbook function
## Fixes the spacing issue and handles delta_test_forecast/delta_holdout_forecast
###############################################################################
add_influence_analysis_to_workbook <- function(influence_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for (dep_var in names(influence_results)) {
    if (!dep_var %in% names(wb)) {
      addWorksheet(wb, dep_var)
    }
    
    sheet_data <- tryCatch({
      readWorkbook(wb, sheet = dep_var)
    }, error = function(e) {
      data.frame()
    })
    
    # Calculate exact start row by finding the last sensitivity analysis row
    # Find the last non-empty row in the sheet
    non_empty_rows <- which(!is.na(sheet_data[,1]) & sheet_data[,1] != "")
    
    if (length(non_empty_rows) > 0) {
      last_content_row <- max(non_empty_rows, na.rm = TRUE)
      # Look for "Sensitivity Analysis Results" in the sheet
      sensitivity_rows <- which(sheet_data[,1] == "Sensitivity Analysis Results")
      
      if (length(sensitivity_rows) > 0) {
        sensitivity_row <- max(sensitivity_rows)
        # Find the last row of the sensitivity table
        # Usually it's a block of data after the title
        for (i in (sensitivity_row + 1):nrow(sheet_data)) {
          if (is.na(sheet_data[i,1]) || sheet_data[i,1] == "") {
            last_sensitivity_row <- i - 1
            break
          }
          # If we reach the end of the dataframe
          if (i == nrow(sheet_data)) {
            last_sensitivity_row <- i
          }
        }
        # Start influence analysis 5 rows after the sensitivity table
        current_row <- last_sensitivity_row + 9
      } else {
        # If sensitivity analysis not found, start 5 rows after last content
        current_row <- last_content_row + 9
      }
    } else {
      # If sheet is empty
      current_row <- 1
    }
    
    processed_models <- c()
    
    for (model_name in names(influence_results[[dep_var]])) {
      if (model_name %in% processed_models) next
      processed_models <- c(processed_models, model_name)
      
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # CRITICAL FIX: Map delta_test_forecast to delta_holdout_forecast if needed
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        results_df$delta_holdout_forecast <- results_df$delta_test_forecast
      }
      
      # Process delta_looic if present
      if ("delta_looic" %in% names(results_df)) {
        results_df$delta_looic <- as.numeric(as.character(results_df$delta_looic))
      } else {
        results_df$delta_looic <- rep(NA, nrow(results_df))
      }
      
      # Round numeric columns to 4 decimal places
      numeric_cols <- c("pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      for (col in numeric_cols) {
        if (col %in% names(results_df) && is.numeric(results_df[[col]])) {
          results_df[[col]] <- round(results_df[[col]], 4)
        }
      }
      
      # Sort by absolute delta_holdout_forecast
      if (sum(!is.na(results_df$delta_holdout_forecast)) > 0) {
        sorted_df <- results_df[order(abs(results_df$delta_holdout_forecast), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df <- results_df
      }
      top_25_forecast <- head(sorted_df, 25)
      
      # Write the title
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta Holdout Forecast - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      # Get the correct column names
      if (all(c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      } else if (all(c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast")
        colnames(top_25_forecast)[colnames(top_25_forecast) == "delta_test_forecast"] <- "delta_holdout_forecast"
      } else {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      }
      
      # Make sure we only write columns that actually exist
      write_cols <- intersect(write_cols, colnames(top_25_forecast))
      
      # Write the data
      writeData(wb, dep_var, top_25_forecast[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_forecast)), 
                cols = col_idx)
      }
      
      # Proceed to next section - 5 rows after this table
      current_row <- current_row + nrow(top_25_forecast) + 5
      
      # Sort by absolute delta_looic
      if (sum(!is.na(results_df$delta_looic)) > 0) {
        sorted_df_looic <- results_df[order(abs(results_df$delta_looic), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df_looic <- results_df
      }
      top_25_looic <- head(sorted_df_looic, 25)
      
      # Write looic section
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta LOOIC - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      writeData(wb, dep_var, top_25_looic[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_looic)), 
                cols = col_idx)
      }
      
      # Proceed to next model section - 5 rows after this table
      current_row <- current_row + nrow(top_25_looic) + 5
    }
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  cat("Influence analysis results added to workbook:", file_path, "\n")
}
###############################################################################
## Parallelized version of create_excel_results
###############################################################################
create_excel_results_parallel <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets_rolling, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  rolling_splits, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  all_results,         
  top_model_results,   
  hold_out_dataset     
) {
  file_path <- "RW_ALL.xlsx"
  
  # This part is harder to parallelize as it builds on previous results
  ensemble_smapes <- get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
  
  # Create prediction tables
  wb <- create_enhanced_prediction_tables(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets_rolling = extended_test_sets_rolling, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_smapes = ensemble_smapes
  )
  
  # Now we'll parallelize these steps which can be run independently
  
  # Set up parallel processing
  no_cores <- detectCores() - 1  # Leave one core free
  cat("Using", no_cores, "cores for parallel Excel processing\n")
  
  # Register the parallel backend
  cl <- makeCluster(no_cores)
  registerDoParallel(cl)
  
  # Export necessary packages to all workers
  clusterEvalQ(cl, {
    library(openxlsx)
    library(bsts)
    library(coda)
  })
  
  # Process the different analyses in parallel
  # Each function will load, modify, and save the workbook independently
  
  # Run tasks in parallel
  results <- foreach(task_num = 1:3, .packages = c("openxlsx", "bsts")) %dopar% {
    result <- switch(task_num,
      # Task 1: Add sensitivity analysis
      {
        tryCatch({
          add_sensitivity_analysis(sensitivity_results, file_path)
          "Sensitivity analysis completed successfully"
        }, error = function(e) {
          paste("Error in sensitivity analysis:", e$message)
        })
      },
      
      # Task 2: Add influence analysis
      {
        tryCatch({
          # Use the fixed version of the function
          add_influence_analysis_to_workbook(influence_results, file_path)
          "Influence analysis completed successfully"
        }, error = function(e) {
          paste("Error in influence analysis:", e$message)
        })
      },
      
      # Task 3: Add diagnostics
      {
        tryCatch({
          add_diagnostics_to_workbook(
            all_models = all_models, 
            rolling_splits = rolling_splits, 
            all_diagnostics = all_diagnostics, 
            all_looic = all_looic,
            bayesian_R2_results = bayesian_R2_results, 
            crps_scores = crps_scores, 
            lpd_scores = lpd_scores, 
            ljung_box_results = ljung_box_results,
            file_path = file_path
          )
          "Diagnostics completed successfully"
        }, error = function(e) {
          paste("Error in diagnostics:", e$message)
        })
      }
    )
    return(result)
  }
  
  # Stop the cluster
  stopCluster(cl)
  registerDoSEQ()  # Reset to sequential processing
  
  # Clean up
  rm(cl)
  gc()  # Force garbage collection
  
  # Summarize results
  summary <- paste("Excel processing results:", 
                   paste(results, collapse = "; "))
  
  return(paste("Results successfully written to", file_path, "-", summary))
}
###############################################################################
## 5) add_diagnostics_to_workbook (unchanged)
###############################################################################
add_diagnostics_to_workbook <- function(all_models, rolling_splits, all_diagnostics, all_looic, 
                                        bayesian_R2_results, crps_scores, lpd_scores, 
                                        ljung_box_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  suppressWarnings({
    if (!requireNamespace("coda", quietly = TRUE)) {
      install.packages("coda")
    }
    library(coda)
  })
  
  for(dep_var in names(all_models)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    current_row <- nrow(sheet_data) + 22
    
    for(window in 1:length(all_models[[dep_var]])) {
      if (is.null(all_models[[dep_var]][[window]])) next
      
      train_size <- if (!is.null(rolling_splits[[dep_var]]) && 
                        !is.null(rolling_splits[[dep_var]][[window]]) && 
                        !is.null(rolling_splits[[dep_var]][[window]]$train)) {
        nrow(rolling_splits[[dep_var]][[window]]$train)
      } else {
        NA
      }
      
      test_size <- if (!is.null(rolling_splits[[dep_var]]) && 
                       !is.null(rolling_splits[[dep_var]][[window]]) && 
                       !is.null(rolling_splits[[dep_var]][[window]]$test)) {
        nrow(rolling_splits[[dep_var]][[window]]$test)
      } else {
        NA
      }
      
      n_predictors <- if (!is.null(rolling_splits[[dep_var]]) && 
                          !is.null(rolling_splits[[dep_var]][[window]]) && 
                          !is.null(rolling_splits[[dep_var]][[window]]$train)) {
        ncol(rolling_splits[[dep_var]][[window]]$train) - 1
      } else {
        NA
      }
      
      dic_value <- if (!is.null(all_diagnostics) && 
                       !is.null(all_diagnostics[[dep_var]]) && 
                       !is.null(all_diagnostics[[dep_var]]$DIC) &&
                       length(all_diagnostics[[dep_var]]$DIC) >= window) {
        all_diagnostics[[dep_var]]$DIC[window]
      } else {
        NA
      }
      
      waic_value <- if (!is.null(all_diagnostics) && 
                        !is.null(all_diagnostics[[dep_var]]) && 
                        !is.null(all_diagnostics[[dep_var]]$WAIC) &&
                        length(all_diagnostics[[dep_var]]$WAIC) >= window) {
        all_diagnostics[[dep_var]]$WAIC[window]
      } else {
        NA
      }
      
      looic_value <- if (!is.null(all_looic) && 
                         !is.null(all_looic[[dep_var]]) && 
                         length(all_looic[[dep_var]]) >= window) {
        all_looic[[dep_var]][window]
      } else {
        NA
      }
      
      r2_value <- if (!is.null(bayesian_R2_results) && 
                      !is.null(bayesian_R2_results[[dep_var]]) && 
                      !is.null(bayesian_R2_results[[dep_var]]$Mean_R2) &&
                      length(bayesian_R2_results[[dep_var]]$Mean_R2) >= window) {
        bayesian_R2_results[[dep_var]]$Mean_R2[window]
      } else {
        NA
      }
      
      crps_value <- if (!is.null(crps_scores) && 
                        !is.null(crps_scores[[dep_var]]) && 
                        any(crps_scores[[dep_var]]$Window == window)) {
        crps_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == window, ]
        if (nrow(crps_data) > 0 && !is.na(crps_data$CRPS_Mean) && !is.na(crps_data$CRPS_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", crps_data$CRPS_Mean, crps_data$CRPS_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lpd_value <- if (!is.null(lpd_scores) && 
                       !is.null(lpd_scores[[dep_var]]) && 
                       any(lpd_scores[[dep_var]]$Window == window)) {
        lpd_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == window, ]
        if (nrow(lpd_data) > 0 && !is.na(lpd_data$LPD_Mean) && !is.na(lpd_data$LPD_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", lpd_data$LPD_Mean, lpd_data$LPD_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lb_stat <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_Statistic) &&
                     length(ljung_box_results[[dep_var]]$LB_Statistic) >= window) {
        ljung_box_results[[dep_var]]$LB_Statistic[window]
      } else {
        NA
      }
      
      lb_pval <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_pvalue) &&
                     length(ljung_box_results[[dep_var]]$LB_pvalue) >= window) {
        ljung_box_results[[dep_var]]$LB_pvalue[window]
      } else {
        NA
      }
      
      metrics <- data.frame(
        Metric = c(
          "Training Sample Size",
          "Test Sample Size",
          "Number of Predictors",
          "DIC/number of training period",
          "WAIC/number of training period",
          "LOOIC",
          "Bayesian R²",
          "Number of iterations used in BSTS",
          "CRPS",
          "LPD",
          "LB_Statistic",
          "LB_pvalue"
        ),
        Value = c(
          train_size,
          test_size,
          n_predictors,
          dic_value,
          waic_value,
          looic_value,
          r2_value,
          10000,
          crps_value,
          lpd_value,
          lb_stat,
          lb_pval
        )
      )
      
      writeData(wb, dep_var, paste("Window", window), startRow = current_row)
      current_row <- current_row + 2
      writeData(wb, dep_var, metrics, startRow = current_row, startCol = 1, colNames = FALSE)
      current_row <- current_row + nrow(metrics) + 2
      
      tryCatch({
        coef_stats <- data.frame(
          Parameter     = character(),
          Mean          = numeric(),
          Median        = numeric(),
          CI_2.5        = numeric(),
          CI_97.5       = numeric(),
          ESS           = numeric(),
          Post_Prob     = numeric(),
          Geweke_Pval   = numeric(),
          HW_Pval       = numeric(),
          RL_Autocorr   = numeric(),
          RL_Iterations = numeric(),
          RL_Burnin     = numeric(),
          RL_Dependence = numeric(),
          stringsAsFactors = FALSE
        )
        
        model <- all_models[[dep_var]][[window]]
        if (!is.null(model) && !is.null(model$coefficients)) {
          coef_matrix <- as.matrix(model$coefficients)
          
          for(j in 1:ncol(coef_matrix)) {
            param_name <- colnames(coef_matrix)[j]
            chain <- as.vector(coef_matrix[,j])
            if (length(chain) < 2) next
            
            ci <- tryCatch({
              quantile(chain, probs = c(0.025, 0.975))
            }, error = function(e) c(NA, NA))
            
            mean_val   <- tryCatch(mean(chain), error = function(e) NA)
            median_val <- tryCatch(median(chain), error = function(e) NA)
            ess_val    <- tryCatch(as.numeric(effectiveSize(mcmc(chain))), error = function(e) NA)
            pip        <- tryCatch(mean(chain != 0), error = function(e) NA)
            
            geweke_pval <- tryCatch({
              geweke <- geweke.diag(mcmc(chain))
              2 * pnorm(-abs(geweke$z))
            }, error = function(e) NA)
            
            hw_pval <- tryCatch({
              hw <- heidel.diag(mcmc(chain))
              if (is.null(hw)) NA else hw[1, "pvalue"]
            }, error = function(e) NA)
            
            rl_stats <- tryCatch({
              if (length(unique(chain)) >= 10) {
                binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
                rl <- raftery.diag(binary_chain)
                c(cor(chain[-1], chain[-length(chain)]),
                  rl$resmatrix[1,"N"],
                  rl$resmatrix[1,"M"],
                  rl$resmatrix[1,"I"])
              } else {
                rep(NA, 4)
              }
            }, error = function(e) rep(NA, 4))
            
            coef_stats <- rbind(coef_stats, data.frame(
              Parameter     = param_name,
              Mean          = round(mean_val, 4),
              Median        = round(median_val, 4),
              CI_2.5        = round(ci[1], 4),
              CI_97.5       = round(ci[2], 4),
              ESS           = round(ess_val, 0),
              Post_Prob     = round(pip, 4),
              Geweke_Pval   = round(geweke_pval, 4),
              HW_Pval       = round(hw_pval, 4),
              RL_Autocorr   = round(rl_stats[1], 4),
              RL_Iterations = round(rl_stats[2], 0),
              RL_Burnin     = round(rl_stats[3], 0),
              RL_Dependence = round(rl_stats[4], 2),
              stringsAsFactors=FALSE
            ))
          }
        }
        
        if (nrow(coef_stats) > 0) {
          writeData(wb, dep_var, coef_stats, startRow = current_row)
          current_row <- current_row + nrow(coef_stats) + 3
        } else {
          writeData(wb, dep_var, "No coefficient data available", startRow = current_row)
          current_row <- current_row + 4
        }
        
      }, error = function(e) {
        writeData(wb, dep_var, paste("Error processing coefficients:", e$message), startRow = current_row)
        current_row <- current_row + 4
      })
    }
    
    setColWidths(wb, dep_var, cols = 1:13, widths = "auto")
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}


###############################################################################
## 6) format_window_range (unchanged)
###############################################################################
format_window_range <- function(window) {
  # Define the descriptive text for each rolling window's train and test sets
  # Updated to reflect the new window specifications and hold-out period (80-86)
  train_ranges <- c(
    "1 - 79 & 87 - 97",
    "5 - 79 & 87 - 101",
    "9 - 79 & 87 - 105",
    "13 - 79 & 87 - 109 "
  )
  
  test_ranges <- c(
    "98 - 111",
    "1 - 4 & 102 - 111",
    "1 - 8 & 106 - 111",
    "1 - 12 & 110 - 111"
  )
  
  paste(train_ranges[window], "as the training and the", test_ranges[window], "as the testing")
}

###############################################################################
## 7) create_excel_results
##    Computes ensemble SMAPEs then calls the above functions.
###############################################################################
create_excel_results <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets_rolling, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  rolling_splits, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  all_results,         
  top_model_results,   
  hold_out_dataset     
) {
  file_path <- "RW_Step_2016Q4.xlsx"
  
  # Map delta_test_forecast to delta_holdout_forecast in influence results
  for (dep_var in names(influence_results)) {
    for (model_name in names(influence_results[[dep_var]])) {
      results_df <- influence_results[[dep_var]][[model_name]]
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        influence_results[[dep_var]][[model_name]]$delta_holdout_forecast <- 
          results_df$delta_test_forecast
      }
    }
  }
  
  # Get ensemble SMAPEs
  ensemble_smapes <- get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
  
  # Create prediction tables
  cat("Creating prediction tables...\n")
  wb <- create_enhanced_prediction_tables(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets_rolling = extended_test_sets_rolling, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_smapes = ensemble_smapes
  )
  
  # Add sensitivity analysis with proper spacing
  cat("Adding sensitivity analysis...\n")
  add_sensitivity_analysis(sensitivity_results, file_path)
  
  # Add influence analysis with proper spacing
  cat("Adding influence analysis...\n")
  add_influence_analysis_to_workbook(influence_results, file_path)
  
  # Add diagnostics
  cat("Adding diagnostics...\n")
  add_diagnostics_to_workbook(
    all_models = all_models, 
    rolling_splits = rolling_splits, 
    all_diagnostics = all_diagnostics, 
    all_looic = all_looic,
    bayesian_R2_results = bayesian_R2_results, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    ljung_box_results = ljung_box_results,
    file_path = file_path
  )
  
  return(paste("Results successfully written to", file_path))
}
###############################################################################
## 8) Example usage
###############################################################################
 

# Example usage:
result <- create_excel_results(
  all_evaluations = all_evaluations_rwstep, 
  all_models = all_models_rwstep, 
  extended_test_sets_rolling = extended_test_sets_rolling, 
  crps_scores = crps_scores_custom_rolling_step, 
  lpd_scores = lpd_scores_step, 
  sensitivity_results = sensitivity_results, 
  influence_results = influence_results_parallel, 
  rolling_splits = rolling_splits,  
  all_diagnostics = all_diagnostics_rwstep, 
  all_looic = all_looic_rwstep, 
  bayesian_R2_results = bayesian_R2_results_step,
  ljung_box_results = ljung_box_results_step,
  aggregate_smape_results = aggregate_smape_results_step,
   all_results = all_results,
   top_model_results = top_model_results,
   hold_out_dataset = hold_out_dataset
)

print(result)
```