---
title: "EW2023"
output: html_document
date: "2025-04-22"
---


# Package

```{r}
# Package Installation and Library Loading
packages <- c("readxl", "janitor", "dplyr", "bsts", "forecast", "writexl", "broom", "openxlsx", "coda", "ggplot2")
install.packages(setdiff(packages, rownames(installed.packages())))

library(readxl)
library(janitor)
library(dplyr)
library(bsts)
library(forecast)
library(writexl)
library(broom)
library(openxlsx)
library(coda)
library(ggplot2)
library(openxlsx)
library(stats)
library(parallel)
library(foreach)
library(doParallel)
library(bsts)
```

# Data Import & Variable Definition

```{r}
file_path <- "Wells Fargo FS Q1-97 to Q4-24 Consolidation V3.0.xlsm"
mydata <- read_excel(
  path = filepath,
  sheet = "Consolidation Sheet",
  .name_repair = "unique"
) %>%
  clean_names() # for getting the whole dataset 


dep_var_sets <- list(
  net_interest_income = list(
    dep_var = "net_interest_income",
    ivs = c("efficiency_ratio", "liquid_assets_deposits_short_term_borrowing",
            "net_interest_income_after_llp_operating_revenue", "npl_total_equity",
            "pretax_operating_roa", "total_loans_logged",
            "aggregate_corporate_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_percentage",
            "aggregate_financial_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_in_billions_logged",
            "allowance_for_loan_reserves_loans_and_leases_large_domestically_chartered_commercial_banks_percentage",
            "borrowings_large_domestically_chartered_commercial_banks_in_billions_logged",
            "consumer_credit_quarterly_cumulative_change_non_annualized_seasonally_adjusted_in_billions",
            "employment_population_ratio_quarterly",
            "federal_funds_effective_rate_quarterly_average_percentage",
            "federal_receipts_in_billions_logged",
            "gz_spread", 
            "ice_bof_a_us_high_yield_index_total_return",
            "labor_force_total_15_64_years_old_logged",
            "m2_money_supply_to_real_gdp_2017_ratio_percentage",
            "net_interest_income_all_us_banks_in_billions_logged",
            "net_operating_income_margin_all_us_banks_percentage",
            "percentage_of_profitable_financial_institutions_with_yo_y_earnings_growth",
            "real_disposable_income_per_capita_2017_logged",
            "real_government_consumption_expenditures_and_gross_investment_2017_in_billions_logged",
            "real_gross_private_investment_2017_to_real_gdp_2007_percentage",
            "real_personal_consumption_expenditures_on_services_2017_to_real_personal_consumption_expenditures_2017_percentage",
            "s_p_core_logic_case_shiller_u_s_national_home_price_index",
            "sahm_rule_recession_indicator",
            "securities_to_loans_and_leases_after_allowance_for_loan_losses_large_domestically_chartered_commercial_banks_percentage",
            "smoothed_u_s_recession_probabilities_percent",
            "total_deposits_quarterly_percentage_change_seasonally_adjusted_percentage",
            "treasury_constant_maturity_tcm_10yr_2yr",
            "velocity_of_m2_money_stock_ratio")
  ),
  non_interest_income = list(
    dep_var = "non_interest_income",
    ivs = c("efficiency_ratio", "market_cap_ltm_ebt_excl_unusual_items",
            "liquid_assets_deposits_short_term_borrowing",
            "non_interest_operating_income_operating_revenue", 
            "npl_total_equity",
            "pretax_operating_roa", 
            "securities_total_assets",
            "aggregate_corporate_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_percentage",
            "aggregate_financial_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_in_billions_logged",
            "coincident_economic_activity_quarterly_index",
            "consumer_price_quarterly_index_for_all_urban_consumers",
            "employment_population_ratio_quarterly", 
            "equity_volatility_overall_quarterly",
            "federal_receipts_in_billions_logged", 
            "gz_spread",
            "ice_bof_a_us_high_yield_index_total_return",
            "labor_force_total_15_64_years_old_logged",
            "m2_money_supply_to_real_gdp_2017_ratio_percentage",
            "net_operating_income_margin_all_us_banks_percentage",
            "percentage_of_profitable_financial_institutions_with_yo_y_earnings_growth",
            "personal_savings_rate_percentage",
            "real_disposable_income_per_capita_2017_logged",
            "real_government_consumption_expenditures_and_gross_investment_2017_in_billions_logged",
            "real_gross_private_investment_2017_to_real_gdp_2007_percentage",
            "real_personal_consumption_expenditures_on_services_2017_to_real_personal_consumption_expenditures_2017_percentage",
            "reserve_balances_with_federal_reserve_banks_in_billions_logged",
            "s_p_core_logic_case_shiller_u_s_national_home_price_index",
            "sahm_rule_recession_indicator",
            "securities_to_loans_and_leases_after_allowance_for_loan_losses_large_domestically_chartered_commercial_banks_percentage",
            "total_deposits_quarterly_percentage_change_seasonally_adjusted_percentage",
            "total_non_interest_income_all_us_banks_in_billions_logged",
            "velocity_of_m2_money_stock_ratio")
  ),
  provision_credit_loss = list(
    dep_var = "provision_for_credit_losses",
    ivs = c("net_interest_income_after_llp_operating_revenue", 
            "npl_total_equity",
            "securities_total_equity", 
            "total_loans_logged",
            "aggregate_corporate_profits_per_unit_of_real_gross_value_add_with_iva_and_cca_percentage",
            "allowance_for_loan_reserves_loans_and_leases_large_domestically_chartered_commercial_banks_percentage",
            "coincident_economic_activity_quarterly_index",
            "consumer_credit_quarterly_cumulative_change_non_annualized_seasonally_adjusted_in_billions",
            "consumer_price_quarterly_index_for_all_urban_consumers",
            "delinquency_rates_on_loans_for_100_largest_us_banks_by_assets_percentage",
            "employment_population_ratio_quarterly",
            "federal_funds_effective_rate_quarterly_average_percentage",
            "federal_receipts_in_billions_logged", 
            "gz_spread",
            "ice_bof_a_us_high_yield_index_total_return",
            "net_charge_off_rate_of_total_loans_and_leases_percentage",
            "net_operating_income_margin_all_us_banks_percentage",
            "real_disposable_income_per_capita_2017_logged",
            "reserve_balances_with_federal_reserve_banks_in_billions_logged",
            "sahm_rule_recession_indicator",
            "treasury_constant_maturity_tcm_10yr_2yr",
            "treasury_constant_maturity_tcm_10yr_3mo",
            "unemployment_rate_percentage",
            "velocity_of_m2_money_stock_ratio")
  ),
  non_interest_expense = list(
    dep_var = "non_interest_expense",
    ivs = c("efficiency_ratio", 
            "gross_property_plant_equipment",
            "net_interest_income_after_llp_operating_revenue",
            "non_interest_operating_income_operating_revenue",
            "npl_total_equity", 
            "securities_total_equity",
            "total_loans_logged", 
            "total_deposits_logged",
            "consumer_price_quarterly_index_for_all_urban_consumers",
            "employment_population_ratio_quarterly",
            "labor_force_total_15_64_years_old_logged",
            "loans_and_leases_to_deposit_ratio_large_domestically_chartered_commercial_banks_percentage",
            "m2_money_supply_to_real_gdp_2017_ratio_percentage",
            "net_operating_income_margin_all_us_banks_percentage",
            "percentage_of_profitable_financial_institutions_with_yo_y_earnings_growth",
            "real_gross_private_investment_2017_to_real_gdp_2007_percentage",
            "real_personal_consumption_expenditures_on_services_2017_to_real_personal_consumption_expenditures_2017_percentage",
            "sahm_rule_recession_indicator",
            "total_noninterest_expense_all_us_banks_in_billions_logged",
            "treasury_constant_maturity_tcm_10yr_2yr",
            "treasury_constant_maturity_tcm_10yr_3mo")
  )
)
```

# original dataset lagged for 24NA （ create DATASET from 1997Q1 to 2024Q3）
```{r}
# Create datasets for each dependent variable
dep_var_datasets <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Store in list
  dep_var_datasets[[dep_var_name]] <- dataset
}


# Take lag of the missing value of the quarter we want to predict 
dep_var_datasets_modified <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  # Repeat the first column (dep_var) and add it before the first column
  dataset <- cbind(dataset[, 1, drop = FALSE], dataset)  # Add first column as the first column again
  # Loop over columns starting from the second column
  for(col in 2:ncol(dataset)) {
    # Check if the last row value is missing
    if(is.na(dataset[nrow(dataset), col])) {
      # Apply lag: take the value from the previous row for all rows of the column
      dataset[, col] <- lag(dataset[, col], 1, default = NA)
      # Modify column name to indicate lag if it was modified
      new_col_name <- paste0(colnames(dataset)[col], "_lag1")
      colnames(dataset)[col] <- new_col_name
    }
  }
  # Store in list
  dep_var_datasets_modified[[dep_var_name]] <- dataset
}

# Delete before 1997Q1
dep_var_datasets_modified <- lapply(dep_var_datasets_modified, function(x) {
  x[-(1:8), ]#period
})

# Extract last row from each dataset
last_row <- lapply(dep_var_datasets_modified, function(x) {
  x[nrow(x), ]
})

# Remove last row from each dataset
dep_var_datasets <- lapply(dep_var_datasets_modified, function(x) {
  x[-nrow(x), ]
})

# Verify dimensions
cat("Number of rows in main datasets:\n")
sapply(dep_var_datasets, nrow)
cat("\nNumber of columns in main datasets:\n")
sapply(dep_var_datasets, ncol)
print(dep_var_datasets[[1]][nrow(dep_var_datasets[[1]]), ])  # For the first dataset
```

# get 2024Q1
```{r}
#period
create_quarter_four_from_last_row <- function(dep_var_datasets, dep_var_sets) {
  quarter_four <- list()
  
  for (dep_var_name in names(dep_var_sets)) {
    # Get the original dep_var column name
    original_dep_var_name <- dep_var_sets[[dep_var_name]]$dep_var
    
    # Extract last row from dataset
    last_row_data <- dep_var_datasets[[dep_var_name]][nrow(dep_var_datasets[[dep_var_name]]), , drop = FALSE]
    
    # Separate and rename dep_var column
    dep_var_column <- last_row_data[, original_dep_var_name, drop = FALSE]
    colnames(dep_var_column) <- "dep_var"
    
    # Get the independent variables
    indep_vars_data <- last_row_data[, setdiff(colnames(last_row_data), original_dep_var_name), drop = FALSE]
    
    # Combine into final row format
    quarter_four[[dep_var_name]] <- cbind(dep_var_column, indep_vars_data)
    
    # Remove the last row from the dataset
    dep_var_datasets[[dep_var_name]] <- dep_var_datasets[[dep_var_name]][-nrow(dep_var_datasets[[dep_var_name]]), ]
  }
  
  return(list(quarter_four = quarter_four, dep_var_datasets = dep_var_datasets))
}

# Usage:
result <- create_quarter_four_from_last_row(dep_var_datasets, dep_var_sets)
quarter_four <- result$quarter_four
dep_var_datasets <- result$dep_var_datasets

```


# Extract the hold-out period + 2024 Q1

```{r}
# Extracting rows 80-87 from each dataset in dep_var_datasets
hold_out_dataset <- lapply(dep_var_datasets, function(df) {
  df[105:111, ]#period
})

# Create a list to store the updated datasets with quarter_four at the end
holdout_with_last_row <- list()

# Iterate through each dep_var_name and append quarter_four at the end
for(dep_var_name in names(hold_out_dataset)) {
  
  # Ensure column names match between quarter_four and the data frame in hold_out_dataset
  if (!all(names(quarter_four[[dep_var_name]]) == names(hold_out_dataset[[dep_var_name]]))) {
    # Manually adjust the column names of quarter_four to match hold_out_dataset
    names(quarter_four[[dep_var_name]]) <- names(hold_out_dataset[[dep_var_name]])
  }
  
  # Combine the selected holdout (80-87 rows) and the corresponding last row from quarter_four
  holdout_with_last_row[[dep_var_name]] <- rbind(
    hold_out_dataset[[dep_var_name]],
    quarter_four[[dep_var_name]]
  )
}

# Update hold_out_dataset with the modified datasets
hold_out_dataset <- holdout_with_last_row

hold_out_dataset 
```

# Data without hold out

```{r}
hold_out_period <- 105:111#period

# Function to exclude the hold-out period from the dataset
exclude_hold_out_1 <- function(df) {
  # Exclude the rows that are in the hold-out period (81-88)
  df_no_hold <- df[!rownames(df) %in% hold_out_period, ]
  return(df_no_hold)
}

#--------------------------------------------------------------
# This is the dataset without the hold out period 
Indice_testing_dataset <- lapply(dep_var_datasets, function(df) {
  # Exclude rows 81-88 from the dataset and return the modified dataset
  exclude_hold_out_1(df)
})

dep_var_datasets = Indice_testing_dataset
# Verify the result
Indice_testing_dataset
```

# Expanding Window
```{r}
# Create expanding windows with training starting from rows 1-70
expanding_windows <- list()

for(dep_var_name in names(dep_var_datasets)) {
  current_data <- dep_var_datasets[[dep_var_name]]
  dataset_windows <- list()
  
  # Get total number of rows in the dataset
  total_rows <- nrow(current_data)
  
  # Initial training window: rows 1-70
  train_end <- 70
  window_index <- 1
  
  # Maximum training window size is 95 rows
  max_train_rows <- 95
  
  # Create windows while:
  # 1. The training window doesn't exceed 95 rows
  # 2. There's enough data for testing (at least 10 rows for testing)
  while (train_end <= max_train_rows && train_end < total_rows - 9) {
    train_data <- current_data[1:train_end, ]
    test_data <- current_data[(train_end + 1):total_rows, ]
    
    dataset_windows[[window_index]] <- list(train = train_data, test = test_data)
    
    # Expand training window by 5 rows for next iteration
    train_end <- train_end + 5
    window_index <- window_index + 1
    
    # Break if we would exceed 95 rows on the next iteration
    if (train_end > max_train_rows) {
      break
    }
  }
  
  expanding_windows[[dep_var_name]] <- dataset_windows
}

# Check the number of windows created for each dependent variable
window_counts <- sapply(expanding_windows, length)
cat("Number of windows created for each dependent variable:\n")
print(window_counts)

# Print details of the first window for the first dependent variable
first_dep_var <- names(expanding_windows)[1]
first_window <- expanding_windows[[first_dep_var]][[1]]
cat("\nFirst window for", first_dep_var, ":\n")
cat("Training rows:", nrow(first_window$train), "\n")
cat("Training indices:", paste(rownames(first_window$train)[1], "to", 
                               rownames(first_window$train)[nrow(first_window$train)]), "\n")
cat("Testing rows:", nrow(first_window$test), "\n")
cat("Testing indices:", paste(rownames(first_window$test)[1], "to", 
                              rownames(first_window$test)[nrow(first_window$test)]), "\n")

# Print details of the last window for the first dependent variable
last_window_index <- length(expanding_windows[[first_dep_var]])
last_window <- expanding_windows[[first_dep_var]][[last_window_index]]
cat("\nLast window for", first_dep_var, ":\n")
cat("Training rows:", nrow(last_window$train), "\n")
cat("Training indices:", paste(rownames(last_window$train)[1], "to", 
                               rownames(last_window$train)[nrow(last_window$train)]), "\n")
cat("Testing rows:", nrow(last_window$test), "\n")
cat("Testing indices:", paste(rownames(last_window$test)[1], "to", 
                              rownames(last_window$test)[nrow(last_window$test)]), "\n")

# Function to summarize window information
summarize_windows <- function(windows_list) {
  summary_df <- data.frame(
    Window = integer(),
    Train_Start = integer(),
    Train_End = integer(),
    Test_Start = integer(),
    Test_End = integer(),
    Train_Size = integer(),
    Test_Size = integer(),
    stringsAsFactors = FALSE
  )
  
  for (dep_var_name in names(windows_list)) {
    cat("\nSummary for", dep_var_name, ":\n")
    windows <- windows_list[[dep_var_name]]
    
    for (i in 1:length(windows)) {
      window <- windows[[i]]
      train_start <- as.numeric(rownames(window$train)[1])
      train_end <- as.numeric(rownames(window$train)[nrow(window$train)])
      test_start <- as.numeric(rownames(window$test)[1])
      test_end <- as.numeric(rownames(window$test)[nrow(window$test)])
      
      summary_df <- rbind(summary_df, data.frame(
        Window = i,
        Train_Start = train_start,
        Train_End = train_end,
        Test_Start = test_start,
        Test_End = test_end,
        Train_Size = nrow(window$train),
        Test_Size = nrow(window$test),
        stringsAsFactors = FALSE
      ))
    }
    
    print(summary_df)
    summary_df <- summary_df[0, ] # Reset for next dependent variable
  }
}

# Run the summary function
summarize_windows(expanding_windows)

# Create a visual table for all windows
cat("\nVisual representation of all expanding windows:\n")
for (dep_var_name in names(expanding_windows)) {
  cat("\n", dep_var_name, ":\n")
  cat(paste(rep("-", 80), collapse = ""), "\n")
  cat(sprintf("%-10s %-20s %-20s %-15s %-15s\n", 
              "Window", "Training Range", "Testing Range", "Train Size", "Test Size"))
  cat(paste(rep("-", 80), collapse = ""), "\n")
  
  for (i in 1:length(expanding_windows[[dep_var_name]])) {
    window <- expanding_windows[[dep_var_name]][[i]]
    train_range <- sprintf("%s to %s", 
                          rownames(window$train)[1], 
                          rownames(window$train)[nrow(window$train)])
    test_range <- sprintf("%s to %s", 
                         rownames(window$test)[1], 
                         rownames(window$test)[nrow(window$test)])
    cat(sprintf("%-10d %-20s %-20s %-15d %-15d\n", 
                i, train_range, test_range, nrow(window$train), nrow(window$test)))
  }
  cat(paste(rep("-", 80), collapse = ""), "\n")
}
```

# EW_ALL
```{r}
set.seed(1234)
fit_bsts_model <- function(train_data) {
  set.seed(1234)
  # (first column): dep
  y <- train_data[, 1]
  # (all columns except first): IVs
  X <- train_data[, -1]
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y)
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Model
  model <- bsts(formula,
                state.specification = ss,
                niter = 10000,
                data = as.data.frame(cbind(y = y, X)))
  
  return(model)
}
```

## fit the model

```{r}
fit_all_windows <- function(dep_var_splits) {
  models <- list()
  for(i in 1:5) {
    models[[i]] <- fit_bsts_model(dep_var_splits[[i]]$train)
  }
  return(models)
}

# Fit models for all dependent variables
all_modelewall <- list()
# Net Interest Income
all_modelewall$net_interest_income <- fit_all_windows(expanding_windows$net_interest_income)
# Non-Interest Income
all_modelewall$non_interest_income <- fit_all_windows(expanding_windows$non_interest_income)
# Provision Credit Loss
all_modelewall$provision_credit_loss <- fit_all_windows(expanding_windows$provision_credit_loss)
# Non-Interest Expense
all_modelewall$non_interest_expense <- fit_all_windows(expanding_windows$non_interest_expense)
```



# Error of train and test
```{r}
# Function to calculate error metrics with NA handling
calculate_errors <- function(actual, predicted) {
  # Remove NA values in pairs
  valid_indices <- !is.na(actual) & !is.na(predicted)
  if (sum(valid_indices) == 0) {
    return(data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA))
  }
  
  actual <- actual[valid_indices]
  predicted <- predicted[valid_indices]
  
  # Handle case where there's not enough data for MASE
  if (length(actual) <= 1) {
    mase <- NA
  } else {
    # Check if mean(abs(diff(actual))) is zero or very close to zero
    denom <- mean(abs(diff(actual)))
    if (is.na(denom) || denom < 1e-10) {
      mase <- NA
    } else {
      mae <- mean(abs(actual - predicted))
      mase <- mae / denom
    }
  }
  
  # Calculate metrics
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  
  # Handle zero or close to zero values for MAPE
  mape_valid <- actual != 0 & abs(actual) > 1e-10
  if (sum(mape_valid) > 0) {
    mape <- mean(abs((actual[mape_valid] - predicted[mape_valid]) / actual[mape_valid])) * 100
  } else {
    mape <- NA
  }
  
  # Handle zero or close to zero denominators for SMAPE
  smape_denom <- abs(actual) + abs(predicted)
  smape_valid <- smape_denom > 1e-10
  if (sum(smape_valid) > 0) {
    smape <- mean(2 * abs(actual[smape_valid] - predicted[smape_valid]) / smape_denom[smape_valid]) * 100
  } else {
    smape <- NA
  }
  
  # Calculate OWA using available metrics
  if (!is.na(mase) && !is.na(mape)) {
    owa <- (mase + mape) / 2
  } else if (!is.na(mase)) {
    owa <- mase
  } else if (!is.na(mape)) {
    owa <- mape
  } else {
    owa <- NA
  }
  
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Updated evaluation function for all expanding windows
evaluate_predictions <- function(model, test_data) {
  if (is.null(model) || nrow(test_data) == 0) {
    return(list(
      actual = NA,
      predicted_mean = NA,
      predicted_median = NA,
      errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
      errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
    ))
  }
  
  # Extract actual values for testing
  actual <- test_data[, 1]
  
  # Prepare test data for prediction
  pred_data <- as.data.frame(test_data[, -1, drop = FALSE])
  
  # Attempt to make predictions with error handling
  tryCatch({
    # Make predictions
    pred <- predict(model, newdata = pred_data, burn = 100)
    
    # Calculate means and medians
    mean_predictions <- colMeans(pred$distribution)
    median_predictions <- apply(pred$distribution, 2, median)
    
    # Calculate errors
    errors_mean <- calculate_errors(actual, mean_predictions)
    errors_median <- calculate_errors(actual, median_predictions)
    
    return(list(
      actual = actual,
      predicted_mean = mean_predictions,
      predicted_median = median_predictions,
      errors_mean = errors_mean,
      errors_median = errors_median
    ))
  }, error = function(e) {
    cat("Error in prediction:", conditionMessage(e), "\n")
    return(list(
      actual = actual,
      predicted_mean = rep(NA, length(actual)),
      predicted_median = rep(NA, length(actual)),
      errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
      errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
    ))
  })
}

# Function to evaluate all windows for a specific dependent variable
evaluate_all_windows_for_var <- function(models, expanding_windows_var) {
  results <- list()
  
  for(i in 1:5) {
    if (i > length(models) || i > length(expanding_windows_var)) {
      results[[i]] <- list(
        actual = NA,
        predicted_mean = NA,
        predicted_median = NA,
        errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
        errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
      )
      next
    }
    
    # Get test data for this window
    test_data <- expanding_windows_var[[i]]$test
    
    # Evaluate predictions
    cat("Evaluating window", i, "...\n")
    results[[i]] <- evaluate_predictions(models[[i]], test_data)
  }
  
  return(results)
}

# Run evaluations for all dependent variables
all_evaluations_allew <- list()

# Net Interest Income
cat("\nEvaluating Net Interest Income models...\n")
all_evaluations_allew$net_interest_income <- evaluate_all_windows_for_var(
  all_modelewall$net_interest_income, 
  expanding_windows$net_interest_income
)

# Non-Interest Income
cat("\nEvaluating Non-Interest Income models...\n")
all_evaluations_allew$non_interest_income <- evaluate_all_windows_for_var(
  all_modelewall$non_interest_income, 
  expanding_windows$non_interest_income
)

# Provision Credit Loss
cat("\nEvaluating Provision Credit Loss models...\n")
all_evaluations_allew$provision_credit_loss <- evaluate_all_windows_for_var(
  all_modelewall$provision_credit_loss, 
  expanding_windows$provision_credit_loss
)

# Non-Interest Expense
cat("\nEvaluating Non-Interest Expense models...\n")
all_evaluations_allew$non_interest_expense <- evaluate_all_windows_for_var(
  all_modelewall$non_interest_expense, 
  expanding_windows$non_interest_expense
)

# Print summary tables
print_summary_tables <- function(all_evaluations, expanding_windows) {
  for(var_name in names(all_evaluations)) {
    cat("\nResults for", var_name, "\n")
    
    # Mean-based metrics
    cat("\nMean-based metrics:\n")
    window_results_mean <- data.frame()
    
    for(i in 1:5) {
      if (i > length(all_evaluations[[var_name]]) || 
          is.null(all_evaluations[[var_name]][[i]]) || 
          all(is.na(all_evaluations[[var_name]][[i]]$errors_mean))) {
        next
      }
      
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      window_errors$Window <- i
      
      # Add training and testing info
      if (i <= length(expanding_windows[[var_name]])) {
        window_errors$TrainSize <- nrow(expanding_windows[[var_name]][[i]]$train)
        window_errors$TestSize <- nrow(expanding_windows[[var_name]][[i]]$test)
        
        # Add range information
        train_start <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$train)[1])
        train_end <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$train)[nrow(expanding_windows[[var_name]][[i]]$train)])
        test_start <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$test)[1])
        test_end <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$test)[nrow(expanding_windows[[var_name]][[i]]$test)])
        
        window_errors$TrainRange <- paste(train_start, "-", train_end)
        window_errors$TestRange <- paste(test_start, "-", test_end)
      } else {
        window_errors$TrainSize <- NA
        window_errors$TestSize <- NA
        window_errors$TrainRange <- "Unknown"
        window_errors$TestRange <- "Unknown"
      }
      
      window_results_mean <- rbind(window_results_mean, window_errors)
    }
    
    if (nrow(window_results_mean) > 0) {
      # Round numeric columns
      numeric_cols <- sapply(window_results_mean, is.numeric)
      window_results_mean[, numeric_cols] <- round(window_results_mean[, numeric_cols], 4)
      
      # Print results
      print(window_results_mean[, c("Window", "TrainSize", "TestSize", "TrainRange", "TestRange", 
                                  "MSE", "MAE", "MAPE", "SMAPE", "MASE", "OWA")])
    } else {
      cat("No valid results available for mean-based metrics.\n")
    }
    
    # Median-based metrics
    cat("\nMedian-based metrics:\n")
    window_results_median <- data.frame()
    
    for(i in 1:5) {
      if (i > length(all_evaluations[[var_name]]) || 
          is.null(all_evaluations[[var_name]][[i]]) || 
          all(is.na(all_evaluations[[var_name]][[i]]$errors_median))) {
        next
      }
      
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      window_errors$Window <- i
      
      # Add training and testing info
      if (i <= length(expanding_windows[[var_name]])) {
        window_errors$TrainSize <- nrow(expanding_windows[[var_name]][[i]]$train)
        window_errors$TestSize <- nrow(expanding_windows[[var_name]][[i]]$test)
        
        # Add range information
        train_start <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$train)[1])
        train_end <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$train)[nrow(expanding_windows[[var_name]][[i]]$train)])
        test_start <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$test)[1])
        test_end <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$test)[nrow(expanding_windows[[var_name]][[i]]$test)])
        
        window_errors$TrainRange <- paste(train_start, "-", train_end)
        window_errors$TestRange <- paste(test_start, "-", test_end)
      } else {
        window_errors$TrainSize <- NA
        window_errors$TestSize <- NA
        window_errors$TrainRange <- "Unknown"
        window_errors$TestRange <- "Unknown"
      }
      
      window_results_median <- rbind(window_results_median, window_errors)
    }
    
    if (nrow(window_results_median) > 0) {
      # Round numeric columns
      numeric_cols <- sapply(window_results_median, is.numeric)
      window_results_median[, numeric_cols] <- round(window_results_median[, numeric_cols], 4)
      
      # Print results
      print(window_results_median[, c("Window", "TrainSize", "TestSize", "TrainRange", "TestRange", 
                                    "MSE", "MAE", "MAPE", "SMAPE", "MASE", "OWA")])
    } else {
      cat("No valid results available for median-based metrics.\n")
    }
  }
}

# Find the best models based on sMAPE
find_best_models <- function(all_evaluations) {
  best_models <- list()
  
  for (dep_var in names(all_evaluations)) {
    # Extract sMAPE values for each window
    smape_values <- numeric(5)
    
    for (i in 1:5) {
      if (i <= length(all_evaluations[[dep_var]]) && 
          !is.null(all_evaluations[[dep_var]][[i]]) && 
          !is.null(all_evaluations[[dep_var]][[i]]$errors_mean) && 
          !is.na(all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE)) {
        smape_values[i] <- all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE
      } else {
        smape_values[i] <- NA
      }
    }
    
    # Check if we have any valid smape values
    if (all(is.na(smape_values))) {
      cat("Warning: No valid sMAPE values for", dep_var, "\n")
      best_models[[dep_var]] <- list(
        best_window = NA,
        best_smape = NA
      )
      next
    }
    
    # Find the window with the minimum sMAPE
    best_window <- which.min(smape_values)
    best_smape <- min(smape_values, na.rm = TRUE)
    
    # Find the second best window (if available)
    if (sum(!is.na(smape_values)) > 1) {
      smape_values_without_best <- smape_values
      smape_values_without_best[best_window] <- Inf
      second_best_window <- which.min(smape_values_without_best)
      second_best_smape <- min(smape_values_without_best, na.rm = TRUE)
      
      best_models[[dep_var]] <- list(
        best_window_1 = best_window,
        best_smape_1 = best_smape,
        best_window_2 = second_best_window,
        best_smape_2 = second_best_smape
      )
    } else {
      best_models[[dep_var]] <- list(
        best_window = best_window,
        best_smape = best_smape
      )
    }
  }
  
  return(best_models)
}

# Print summary tables
cat("\n==== Printing Summary Tables ====\n")
print_summary_tables(all_evaluations_allew, expanding_windows)

# Find the best models for each dependent variable
cat("\n==== Finding Best Models ====\n")
best_models_ewall <- find_best_models(all_evaluations_allew)

# Print the best models
cat("\nBest Models Based on sMAPE:\n")
for (dep_var in names(best_models_ewall)) {
  cat("\n", dep_var, ":\n")
  if ("best_window_1" %in% names(best_models_ewall[[dep_var]])) {
    cat("  Best Window: ", best_models_ewall[[dep_var]]$best_window_1, 
        " (sMAPE = ", round(best_models_ewall[[dep_var]]$best_smape_1, 4), ")\n", sep="")
    cat("  Second Best Window: ", best_models_ewall[[dep_var]]$best_window_2, 
        " (sMAPE = ", round(best_models_ewall[[dep_var]]$best_smape_2, 4), ")\n", sep="")
  } else {
    window_val <- best_models_ewall[[dep_var]]$best_window
    if (is.na(window_val)) {
      cat("  No valid model found\n")
    } else {
      cat("  Best Window: ", window_val,
          " (sMAPE = ", round(best_models_ewall[[dep_var]]$best_smape, 4), ")\n", sep="")
    }
  }
}
```

## Calculate model weights based on test&train SMAPE

```{r}
calculate_model_weights <- function(all_evaluations) {
  weights_list <- list()
  for(var_name in names(all_evaluations)) {
    # Extract SMAPE for both mean and median
    smape_values_mean <- sapply(all_evaluations[[var_name]], function(x) x$errors_mean$SMAPE)
    smape_values_median <- sapply(all_evaluations[[var_name]], function(x) x$errors_median$SMAPE)
    
    epsilon <- 1e-10
    
    # Calculate weights for mean-based SMAPE
    inverse_weights_mean <- 1 / pmax(smape_values_mean, epsilon)
    normalized_weights_mean <- inverse_weights_mean / sum(inverse_weights_mean)
    
    # Calculate weights for median-based SMAPE
    inverse_weights_median <- 1 / pmax(smape_values_median, epsilon)
    normalized_weights_median <- inverse_weights_median / sum(inverse_weights_median)
    
    # Create dataframes for both
    weights_df_mean <- data.frame(
      Window = 1:length(smape_values_mean),
      SMAPE = smape_values_mean,
      Weight = normalized_weights_mean,
      Method = "Mean"
    )
    
    weights_df_median <- data.frame(
      Window = 1:length(smape_values_median),
      SMAPE = smape_values_median,
      Weight = normalized_weights_median,
      Method = "Median"
    )
    
    # Sort both by SMAPE
    weights_df_mean <- weights_df_mean[order(weights_df_mean$SMAPE), ]
    weights_df_median <- weights_df_median[order(weights_df_median$SMAPE), ]
    
    weights_list[[var_name]] <- list(
      mean = weights_df_mean,
      median = weights_df_median
    )
  }
  
  # Print results
  for(var_name in names(weights_list)) {
    cat("\nWeights for", var_name, "(Mean):\n")
    print(round(weights_list[[var_name]]$mean[, -4], 4))
    
    cat("\nWeights for", var_name, "(Median):\n")
    print(round(weights_list[[var_name]]$median[, -4], 4))
  }
  
  return(weights_list)
}
```

```{r}
# Calculate model weights
model_weights_allew <- calculate_model_weights(all_evaluations_allew)
```

# prediction to holds out period according to the weight given by test

```{r}
# Fixed function to generate prediction tables and SMAPE for all windows and all variables using holdout data
predict_all_windows_variables <- function(all_modelewall, hold_out_dataset) {
  # Initialize results storage
  smape_results <- list()
  
  # Loop through each window
  for(window_num in 1:6) {
    cat("\n================================================================\n")
    cat("WINDOW", window_num, "PREDICTIONS (HOLDOUT PERIOD)\n")
    cat("================================================================\n")
    
    # Loop through each variable
    for(var_name in names(all_modelewall)) {
      # Skip if model or holdout data doesn't exist
      if (!(var_name %in% names(all_modelewall)) || 
          length(all_modelewall[[var_name]]) < window_num || 
          !(var_name %in% names(hold_out_dataset))) {
        cat("Skipping", var_name, "for window", window_num, "(data not available)\n")
        next
      }
      
      # Get the model for this window and variable
      model <- all_modelewall[[var_name]][[window_num]]
      
      # Get the holdout dataset for this variable
      holdout_data <- hold_out_dataset[[var_name]]
      
      # Create period labels (80-87 plus Q1)
      period_labels <- c(as.character(105:111), "Q1")#period
      
      # Get actual values and predictors
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Make predictions
      pred <- predict(model, newdata = pred_data, burn = 100)
      mean_predictions <- colMeans(pred$distribution)
      median_predictions <- apply(pred$distribution, 2, median)
      
      # Calculate errors
      mean_errors <- mean_predictions - actual_values
      median_errors <- median_predictions - actual_values
      
      # Calculate SMAPE for mean predictions
      mean_smape <- mean(2 * abs(actual_values - mean_predictions) / 
                       (abs(actual_values) + abs(mean_predictions))) * 100
      
      # Calculate SMAPE for median predictions
      median_smape <- mean(2 * abs(actual_values - median_predictions) / 
                         (abs(actual_values) + abs(median_predictions))) * 100
      
      # Store SMAPE results
      if (is.null(smape_results[[var_name]])) {
        smape_results[[var_name]] <- data.frame(
          Window = numeric(),
          Mean_SMAPE = numeric(),
          Median_SMAPE = numeric()
        )
      }
      
      smape_results[[var_name]] <- rbind(smape_results[[var_name]], 
                                      data.frame(
                                        Window = window_num,
                                        Mean_SMAPE = mean_smape,
                                        Median_SMAPE = median_smape
                                      ))
      
      # Display SMAPE for this window and variable
      cat("\n----------------------------------------------------------------\n")
      cat("SMAPE FOR WINDOW", window_num, "PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      cat("Mean SMAPE:", round(mean_smape, 4), "%\n")
      cat("Median SMAPE:", round(median_smape, 4), "%\n")
      
      # Display mean table - FIX: Create numeric data first, then add Period as a separate column
      cat("\n----------------------------------------------------------------\n")
      cat("TABLE: WINDOW", window_num, "MEAN PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      
      # Create numeric results first
      numeric_results <- data.frame(
        Actual = actual_values,
        Predicted = mean_predictions,
        Error = mean_errors
      )
      
      # Then display with Period column
      cat("Period\tActual\tPredicted\tError\n")
      for (i in 1:length(period_labels)) {
        cat(period_labels[i], "\t", 
            round(numeric_results$Actual[i], 4), "\t", 
            round(numeric_results$Predicted[i], 4), "\t", 
            round(numeric_results$Error[i], 4), "\n")
      }
      
      # Display median table - Use same approach
      cat("\n----------------------------------------------------------------\n")
      cat("TABLE: WINDOW", window_num, "MEDIAN PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      
      # Create numeric results first
      numeric_results <- data.frame(
        Actual = actual_values,
        Predicted = median_predictions,
        Error = median_errors
      )
      
      # Then display with Period column
      cat("Period\tActual\tPredicted\tError\n")
      for (i in 1:length(period_labels)) {
        cat(period_labels[i], "\t", 
            round(numeric_results$Actual[i], 4), "\t", 
            round(numeric_results$Predicted[i], 4), "\t", 
            round(numeric_results$Error[i], 4), "\n")
      }
    }
  }
  
  # Display summary of SMAPE results for all windows and variables
  cat("\n================================================================\n")
  cat("SUMMARY OF SMAPE RESULTS FOR ALL WINDOWS AND VARIABLES (HOLDOUT PERIOD)\n")
  cat("================================================================\n")
  
  for(var_name in names(smape_results)) {
    cat("\n----------------------------------------------------------------\n")
    cat("SMAPE SUMMARY FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    print(round(smape_results[[var_name]], 4))
  }
  
  return(smape_results)
}
```

```{r}
# Fixed function to aggregate predictions for all windows
aggregate_and_calculate_smape <- function(smape_results, all_modelewall, hold_out_dataset) {
  # Initialize results storage for aggregated SMAPE
  aggregate_smape_results <- list()
  
  # Loop through each dependent variable
  for (var_name in names(all_modelewall)) {
    # Skip if holdout data doesn't exist
    if (!(var_name %in% names(hold_out_dataset))) {
      cat("Skipping", var_name, "(holdout data not available)\n")
      next
    }
    
    cat("\n================================================================\n")
    cat("AGGREGATED PREDICTIONS AND SMAPE FOR", toupper(var_name), "(HOLDOUT PERIOD)\n")
    cat("================================================================\n")
    
    # Create period labels (80-87 plus Q1)
    period_labels <- c(as.character(105:111), "Q1")#period
    
    # Get holdout data for this variable
    holdout_data <- hold_out_dataset[[var_name]]
    actual_values <- holdout_data[, 1]
    pred_data <- as.data.frame(holdout_data[, -1])
    
    # Initialize matrices to store predictions from all windows (each row is a window)
    all_mean_preds <- matrix(0, nrow = 6, ncol = length(actual_values))
    all_median_preds <- matrix(0, nrow = 6, ncol = length(actual_values))
    windows_available <- numeric()
    
    # Loop through each window to collect predictions
    for(window_num in 1:5) {
      # Skip if model doesn't exist
      if (!(var_name %in% names(all_modelewall)) || 
          length(all_modelewall[[var_name]]) < window_num) {
        next
      }
      
      # Get the model for this window and variable
      model <- all_modelewall[[var_name]][[window_num]]
      
      # Make predictions
      pred <- predict(model, newdata = pred_data, burn = 100)
      mean_predictions <- colMeans(pred$distribution)
      median_predictions <- apply(pred$distribution, 2, median)
      
      # Store in matrices
      all_mean_preds[window_num, ] <- mean_predictions
      all_median_preds[window_num, ] <- median_predictions
      windows_available <- c(windows_available, window_num)
    }
    
    # Filter matrices to only include available windows
    all_mean_preds <- all_mean_preds[windows_available, , drop = FALSE]
    all_median_preds <- all_median_preds[windows_available, , drop = FALSE]
    
    # Calculate aggregated predictions (average across windows)
    agg_mean_preds <- colMeans(all_mean_preds)
    agg_median_preds <- colMeans(all_median_preds)
    
    # Calculate errors
    mean_errors <- agg_mean_preds - actual_values
    median_errors <- agg_median_preds - actual_values
    
    # Calculate SMAPE for the aggregated predictions
    mean_smape <- mean(2 * abs(actual_values - agg_mean_preds) / 
                       (abs(actual_values) + abs(agg_mean_preds))) * 100
    
    median_smape <- mean(2 * abs(actual_values - agg_median_preds) / 
                         (abs(actual_values) + abs(agg_median_preds))) * 100
    
    # Store aggregated SMAPE results
    aggregate_smape_results[[var_name]] <- data.frame(
      Mean_SMAPE = mean_smape,
      Median_SMAPE = median_smape
    )
    
    # Display aggregated SMAPE results for this variable
    cat("\n----------------------------------------------------------------\n")
    cat("AGGREGATED SMAPE FOR", toupper(var_name), "(HOLDOUT PERIOD)\n")
    cat("----------------------------------------------------------------\n")
    cat("Mean SMAPE:", round(mean_smape, 4), "%\n")
    cat("Median SMAPE:", round(median_smape, 4), "%\n")
    
    # Display aggregated mean predictions table - FIX: Avoid mixing types in data frame
    cat("\n----------------------------------------------------------------\n")
    cat("TABLE: AGGREGATED MEAN PREDICTIONS FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    
    # Create numeric results first
    numeric_results <- data.frame(
      Actual = actual_values,
      Predicted = agg_mean_preds,
      Error = mean_errors
    )
    
    # Then display with Period column
    cat("Period\tActual\tPredicted\tError\n")
    for (i in 1:length(period_labels)) {
      cat(period_labels[i], "\t", 
          round(numeric_results$Actual[i], 4), "\t", 
          round(numeric_results$Predicted[i], 4), "\t", 
          round(numeric_results$Error[i], 4), "\n")
    }
    
    # Display aggregated median predictions table - Use same approach
    cat("\n----------------------------------------------------------------\n")
    cat("TABLE: AGGREGATED MEDIAN PREDICTIONS FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    
    # Create numeric results first
    numeric_results <- data.frame(
      Actual = actual_values,
      Predicted = agg_median_preds,
      Error = median_errors
    )
    
    # Then display with Period column
    cat("Period\tActual\tPredicted\tError\n")
    for (i in 1:length(period_labels)) {
      cat(period_labels[i], "\t", 
          round(numeric_results$Actual[i], 4), "\t", 
          round(numeric_results$Predicted[i], 4), "\t", 
          round(numeric_results$Error[i], 4), "\n")
    }
  }
  
  # Display summary of aggregated SMAPE results
  cat("\n================================================================\n")
  cat("SUMMARY OF AGGREGATED SMAPE RESULTS FOR ALL VARIABLES (HOLDOUT PERIOD)\n")
  cat("================================================================\n")
  
  for (var_name in names(aggregate_smape_results)) {
    cat("\n----------------------------------------------------------------\n")
    cat("AGGREGATED SMAPE SUMMARY FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    print(round(aggregate_smape_results[[var_name]], 4))
  }
  
  return(aggregate_smape_results)
}

# Run the functions with your models and holdout data
smape_results <- predict_all_windows_variables(all_modelewall, hold_out_dataset)
aggregate_smape_results <- aggregate_and_calculate_smape(smape_results, all_modelewall, hold_out_dataset)
```

# Summary stat

```{r}
get_model_diagnostics <- function(model, window_data) {
  # Numerically stable log-mean-exp function
  log_mean_exp <- function(x) {
    m <- max(x)
    m + log(mean(exp(x - m)))
  }
  
  window_stats <- lapply(seq_along(model), function(i) {
    ll <- model[[i]]$log.likelihood   # log-likelihood from the model
    n_train <- nrow(window_data[[i]]$train)
    
    # --- DIC Calculation ---
    # We always compute DIC on the full set of draws.
    # If ll is a matrix, use all elements; if it's a vector, use it directly.
    if (!is.null(dim(ll))) {
      # ll is already a matrix (n_iter x n_train)
      ll_vec <- as.vector(ll)
    } else {
      ll_vec <- ll
    }
    
    D_bar <- -2 * mean(ll_vec)
    D_theta_bar <- -2 * max(ll_vec)  # Using the maximum (posterior mode) as a plug‐in estimate
    p_D <- D_bar - D_theta_bar
    DIC <- (D_bar + p_D) / n_train  # reporting per-observation
    
    # --- WAIC Calculation ---
    # Check if we can reshape ll into a point-wise matrix.
    if (is.null(dim(ll)) && (length(ll) %% n_train != 0)) {
      # The log likelihood is aggregated (one value per iteration)
      # Compute overall lppd and p_waic, then scale per observation.
      lppd <- log_mean_exp(ll)
      p_waic <- var(ll)
      WAIC <- (-2 * (lppd - p_waic)) / n_train
    } else {
      # If ll is already a matrix, or if its length is divisible by n_train, reshape it.
      if (is.null(dim(ll))) {
        n_iter <- length(ll) / n_train
        ll_matrix <- matrix(ll, ncol = n_train, byrow = TRUE)
      } else {
        ll_matrix <- ll
      }
      # For each observation (i.e. each column), compute a stable log-mean-exp and variance.
      lppd <- sum(apply(ll_matrix, 2, log_mean_exp))
      p_waic <- sum(apply(ll_matrix, 2, var))
      WAIC <- (-2 * (lppd - p_waic)) / n_train
    }
    
    data.frame(
      Window = i,
      Training_Size = n_train,
      DIC = DIC,
      WAIC = WAIC
    )
  })
  
  do.call(rbind, window_stats)
}

print_all_diagnostics <- function(diagnostics) {
  for(dep_var in names(diagnostics)) {
    cat("\n===", dep_var, "===\n")
    print(diagnostics[[dep_var]])
  }
}
```

```{r}
all_diagnostics <- list()
for(dep_var in names(all_modelewall)) {
  all_diagnostics[[dep_var]] <- get_model_diagnostics(all_modelewall[[dep_var]], expanding_windows[[dep_var]])
}

print_all_diagnostics(all_diagnostics)
```

## Function of getting LOOIC

```{r}
calculate_looic <- function(model, window_data) {
  # Get training sample size 
  n_train <- nrow(window_data$train)
  
  # Calculate LOOIC
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  # normalized LOOIC
  looic <- (-2 * sum(loo_liks)) / n_train
  
  return(looic)
}

# Function to apply for all windows
get_all_looic <- function(models, windows) {
  n_windows <- length(models)
  looic_results <- numeric(n_windows)
  
  for (i in 1:n_windows) {
    looic_results[i] <- calculate_looic(models[[i]], windows[[i]])
  }
  
  return(looic_results)
}
```

### Apply the calculation to all dependent variables

```{r}
all_looic_ew <- list()
for (dep_var in names(all_modelewall)) {
  all_looic_ew[[dep_var]] <- get_all_looic(
    all_modelewall[[dep_var]], 
    expanding_windows[[dep_var]]
  )
}
# Display results
for (dep_var in names(all_looic_ew)) {
  cat(sprintf("\nNormalized LOOIC Results for %s\n", dep_var))
  for (i in 1:length(all_looic_ew[[dep_var]])) {
    cat(sprintf("Window %d: %.4f\n", i, all_looic_ew[[dep_var]][i]))
  }
}
```

## 95% confi interval

```{r}
calculate_confidence_intervals <- function(all_modelewall, expanding_windows) {
  for(dep_var in names(all_modelewall)) {
    cat("\n95% Confidence Intervals for", dep_var, "\n")
    for(window in seq_along(all_modelewall[[dep_var]])) {
      cat("\nWindow", window, ":\n")
      
      model <- all_modelewall[[dep_var]][[window]]
      coefficients <- model$coefficients
      
      # Calculate statistics
      stats <- apply(coefficients, 2, function(x) {
        quantiles <- quantile(x, probs = c(0.025, 0.975))
        c("2.5%" = quantiles[1],
          "Mean" = mean(x),
          "Median" = median(x),
          "97.5%" = quantiles[2])
      })
      
      stats_df <- as.data.frame(t(stats))
      stats_df <- round(stats_df, 4)
      print(stats_df)
    }
  }
}

confidence_intervals_ew <- calculate_confidence_intervals(all_modelewall, expanding_windows)
```

## mcmc size

```{r}
library(coda)
get_mcmc_stats <- function(all_modelewall, expanding_windows) {
  for(dep_var in names(all_modelewall)) {
    cat("\nEffective Sample Size for", dep_var, "\n")
    cat("===============================\n")
    
    n_windows <- length(all_modelewall[[dep_var]])
    
    for(window in 1:n_windows) {
      model <- all_modelewall[[dep_var]][[window]]
      
      # Get coefficients for this window
      coef_matrix <- model$coefficients
      param_names <- colnames(coef_matrix)
      
      cat(sprintf("\nWindow %d\n", window))
      cat("-------------------\n")
      
      for(param in param_names) {
        # Extract chain for this parameter
        chain <- mcmc(coef_matrix[, param])
        
        # Calculate ESS
        ess <- try(effectiveSize(chain), silent = TRUE)
        
        # Print results
        cat(sprintf("%s: %.1f\n", param, 
                    if(inherits(ess, "try-error")) NA else ess))
      }
    }
  }
}

# Apply the function
mcmc_stats_ew  <- get_mcmc_stats(all_modelewall, expanding_windows)
```

## Bayesian R² (Gelman et al.)

```{r}
## Function to compute Bayesian R² using posterior draws on the training set
calculate_bayes_R2 <- function(model, train_data, burn = 100) {
  # Extract the observed response (assumes the first column is the dependent variable)
  y_train <- train_data[, 1]
  
  # Get posterior draws for the fitted (training) values.
  # When newdata is provided, predict() returns a list with a 'distribution'
  # element that is a matrix with rows = MCMC iterations and columns = training observations.
  pred_train <- predict(model, newdata = as.data.frame(train_data[, -1]), burn = burn)
  
  # Compute Bayesian R²  the formula 
  #   R² = var(fitted values) / (var(fitted values) + var(residuals))
  R2_draws <- apply(pred_train$distribution, 1, function(fitted_values) {
    var_fitted <- var(fitted_values)
    var_resid  <- var(y_train - fitted_values)
    R2 <- var_fitted / (var_fitted + var_resid)
    return(R2)
  })
  
  return(R2_draws)
}

## Loop over each dependent variable and each window to compute Bayesian R²
bayesian_R2_results <- list()

for (dep_var_name in names(all_modelewall)) {
  models_list <- all_modelewall[[dep_var_name]]
  
  # Data frame to store summary metrics for each window
  bayes_R2_summary <- data.frame(
    Window = integer(),
    Mean_R2 = numeric(),
    Median_R2 = numeric(),
    Lower_R2 = numeric(),
    Upper_R2 = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Loop through each of the 7 expanding windows
  for (i in 1:length(models_list)) {
    # Get the training data for the current window
    train_data <- expanding_windows[[dep_var_name]][[i]]$train
    
    # Extract the fitted model for this window
    model <- models_list[[i]]
    
    # Compute the Bayesian R² draws for the training set
    R2_draws <- calculate_bayes_R2(model, train_data, burn = 100)
    
    # Summarize the draws (you can adjust the summary metrics as desired)
    bayes_R2_summary <- rbind(
      bayes_R2_summary,
      data.frame(
        Window = i,
        Mean_R2 = mean(R2_draws),
        Median_R2 = median(R2_draws),
        Lower_R2 = quantile(R2_draws, 0.025),
        Upper_R2 = quantile(R2_draws, 0.975)
      )
    )
  }
  
  # Store the summary for the current dependent variable
  bayesian_R2_results[[dep_var_name]] <- bayes_R2_summary
}

## Print the Bayesian R² summary for each dependent variable and window
for (dep_var_name in names(bayesian_R2_results)) {
  cat("\nBayesian R² for", dep_var_name, ":\n")
  print(round(bayesian_R2_results[[dep_var_name]], 4))
}

```

## Ljung Box

```{r}
# Function to compute Ljung-Box test diagnostics for each window
get_ljung_box_diagnostics <- function(models, window_data, lags = 10) {
  results <- list()
  
  # Loop over each dependent variable
  for (dep_var_name in names(models)) {
    window_results <- list()
    
    # Loop over each expanding window (for this dep var)
    for (i in seq_along(models[[dep_var_name]])) {
      # Extract the training data for the current window
      train_data <- window_data[[dep_var_name]][[i]]$train
      # Assume the first column is the dependent variable (actual values)
      y_train <- train_data[, 1]
      
      # Obtain one-step-ahead predictions for the training set
      # Here we use the same burn-in as in your evaluation (e.g., 100)
      pred <- predict(models[[dep_var_name]][[i]], 
                      newdata = as.data.frame(train_data[, -1]), 
                      burn = 100)
      
      # Compute the mean fitted value for each observation (across MCMC draws)
      fitted_values <- colMeans(pred$distribution)
      
      # Calculate residuals (actual minus fitted)
      residuals <- y_train - fitted_values
      
      # Perform the Ljung-Box test on the residuals using the specified number of lags
      lb_test <- Box.test(residuals, lag = lags, type = "Ljung-Box")
      
      # Save the results for the current window
      window_results[[i]] <- data.frame(
        Window = i,
        Training_Size = nrow(train_data),
        LB_Statistic = lb_test$statistic,
        LB_pvalue = lb_test$p.value
      )
    }
    
    # Combine the results for all windows for this dependent variable
    results[[dep_var_name]] <- do.call(rbind, window_results)
  }
  
  return(results)
}

# Calculate the Ljung-Box diagnostics (with default lag = 10)
ljung_box_results <- get_ljung_box_diagnostics(all_modelewall, expanding_windows, lags = 10)

# Print the Ljung-Box test results for each dependent variable
for (dep_var in names(ljung_box_results)) {
  cat("\nLjung-Box Test Diagnostics for", dep_var, ":\n")
  print(ljung_box_results[[dep_var]])
}

```

## Posterior Prob

```{r}
analyze_pips <- function(all_modelewall, expanding_windows) {
  for (dep_var in names(all_modelewall)) {
    cat(sprintf("\nPosterior Inclusion Probabilities for %s\n", dep_var))
    
    models <- all_modelewall[[dep_var]]
    for (window in seq_along(models)) {
      cat(sprintf("\nWindow %d:\n", window))
      model <- models[[window]]
      
      if (!is.null(model$coefficients)) {
        # Calculate average inclusion probabilities across MCMC iterations
        inclusion_matrix <- model$coefficients[,1:ncol(model$predictors)]
        pips <- colMeans(inclusion_matrix != 0) 
        names(pips) <- colnames(model$predictors)
        pips <- pips[-1] # Remove intercept
        pips <- sort(pips, decreasing = TRUE)
        print(pips)
      } else {
        cat("No coefficients available\n")
      }
    }
  }
}

pips_results <- analyze_pips(all_modelewall, expanding_windows)
```

## Geweke’s Diagnostic

check if your MCMC chain has converged p \< 0.05 means there's a statistically significant difference between the means of first and last parts (bad for significant)

```{r}
calculate_geweke <- function(all_modelewall, expanding_windows) {
  for(dep_var in names(all_modelewall)) {
    cat("\nGeweke's Diagnostic for", dep_var, "\n")
    
    n_windows <- length(all_modelewall[[dep_var]])
    
    for(window in 1:n_windows) {
      model <- all_modelewall[[dep_var]][[window]]
      # Get coefficients matrix
      coef_matrix <- as.matrix(model$coefficients)
      cat(sprintf("\nWindow %d\n", window))
      cat("-------------------\n")
      # Loop each parameter (column)
      for(j in 1:ncol(coef_matrix)) {
        param_name <- colnames(coef_matrix)[j]
        chain <- mcmc(coef_matrix[, j])
        # the default function in R 
        geweke_stats <- geweke.diag(chain, frac1=0.1, frac2=0.5)
        
        # Extract z-score
        z_score <- as.numeric(geweke_stats$z)
        
        # Calculate p-value only if z_score is not NA
        if(!is.na(z_score)) {
          p_value <- 2 * pnorm(-abs(z_score))
          cat(sprintf("%s:\n", param_name))
          cat(sprintf("  Z-statistic: %.4f\n", z_score))
          cat(sprintf("  p-value: %.4f\n", p_value))
          
          if(!is.na(p_value) && p_value < 0.05) {
            cat("  ** Possible convergence issue (p < 0.05)\n")
          }
        } else {
          cat(sprintf("%s: Unable to calculate Geweke diagnostic\n", param_name))
        }
      }
    }
  }
}

# Apply 
geweke_results <- calculate_geweke(all_modelewall, expanding_windows)
```

## Heidelberger-Welch

```{r}
calculate_heidel_welch <- function(model, window_data) {
  # Print diagnostic results for each parameter
  for(i in 1:length(model)) {
    cat("\nWindow", i, "\n")
    cat("-------------------\n")
    
    # Convert coefficients to mcmc object
    coef_matrix <- as.matrix(model[[i]]$coefficients)
    
    # Test each parameter
    for(j in 1:ncol(coef_matrix)) {
      param_name <- colnames(coef_matrix)[j]
      chain <- mcmc(coef_matrix[,j])
      
      # Run heidel.diag and capture output
      tryCatch({
        test <- heidel.diag(chain)
        
        if(!is.null(test)) {
          cat("\nParameter:", param_name, "\n")
          cat("Stationarity test:", ifelse(test[1,1] == 1, "PASSED", "FAILED"), "\n")
          cat("p-value:", format(test[1,3], digits=4), "\n")
          cat("Start iteration:", test[1,2], "\n")
          cat("Mean:", format(test[1,4], digits=4), "\n")
          cat("Halfwidth test:", ifelse(test[1,5] == 1, "PASSED", "FAILED"), "\n")
          cat("Halfwidth:", format(test[1,6], digits=4), "\n")
        } else {
          cat("\nParameter:", param_name, ": Test returned null result\n")
        }
      }, error = function(e) {
        cat("\nParameter:", param_name, ": Test failed -", e$message, "\n")
      })
    }
  }
}

# Apply to all dependent variables
for(dep_var in names(all_modelewall)) {
  cat("\n\n=== Results for", dep_var, "===\n")
  heidel_results = calculate_heidel_welch(all_modelewall[[dep_var]], expanding_windows[[dep_var]])
}
```

## Raftery_lewis

```{r}
calculate_convergence_diagnostics <- function(model, window_data) {
  for(i in 1:length(model)) {
    cat("\nWindow", i, "\n")
    cat("-------------------\n")
    
    coef_matrix <- as.matrix(model[[i]]$coefficients)
    
    for(j in 1:ncol(coef_matrix)) {
      param_name <- colnames(coef_matrix)[j]
      chain <- as.vector(coef_matrix[,j])
      
      cat("\nParameter:", param_name, "\n")
      
      # Basic diagnostics
      n_unique <- length(unique(chain))
      autocorr <- cor(chain[-1], chain[-length(chain)])
      
      cat("Chain diagnostics:\n")
      cat("Unique values:", n_unique, "\n")
      cat("Mean:", mean(chain), "\n")
      cat("SD:", sd(chain), "\n")
      cat("Autocorrelation:", autocorr, "\n")
      
      if(n_unique < 10) {
        cat("WARNING: Too few unique values for reliable convergence diagnostics\n")
        next
      }
      
      chain_mcmc <- as.mcmc(chain)
      
      # Both Raftery-Lewis and Geweke
      tryCatch({
        # Raftery-Lewis
        binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
        rl <- raftery.diag(binary_chain)
        
        cat("\nRaftery-Lewis Diagnostic:\n")
        cat("Iterations needed:", rl$resmatrix[1,"N"], "\n")
        cat("Burn-in needed:", rl$resmatrix[1,"M"], "\n")
        cat("Dependence factor:", rl$resmatrix[1,"I"], "\n")
        
        # Geweke
        gw <- geweke.diag(chain_mcmc)
        pvalue <- 2 * (1 - pnorm(abs(gw$z)))  # Two-tailed p-value from z-score
        
        cat("\nGeweke Test:\n")
        cat("Z-score:", gw$z, "\n")
        cat("P-value:", pvalue, "\n")
        
        # Combined assessment
        if(rl$resmatrix[1,"I"] > 5 || pvalue < 0.05) {
          cat("\nConvergence Issues Detected:\n")
          if(rl$resmatrix[1,"I"] > 5) {
            cat("- High dependence factor in Raftery-Lewis\n")
          }
          if(pvalue < 0.05) {
            cat("- Significant Geweke test (p < 0.05)\n")
          }
          if(autocorr > 0.7) {
            cat("- High autocorrelation detected\n")
          }
          if(length(chain) < rl$resmatrix[1,"N"]) {
            cat("- More iterations needed\n")
            cat(sprintf("  Current: %d, Recommended: %d\n", 
                       length(chain), rl$resmatrix[1,"N"]))
          }
        } else {
          cat("\nNo convergence issues detected\n")
        }
        
      }, error = function(e) {
        cat("\nDiagnostic calculation failed:\n")
        cat("- Error:", conditionMessage(e), "\n")
        if(autocorr > 0.7) {
          cat("- High autocorrelation might be causing issues\n")
        }
      })
    }
  }
}

# Run diagnostics
for(dep_var in names(all_modelewall)) {
  cat("\n\n=== Convergence Diagnostics for", dep_var, "===\n")
  calculate_convergence_diagnostics(all_modelewall[[dep_var]], expanding_windows[[dep_var]])
}
```

## predictive interval

The regular test sets in your expanding windows only include historical data that was set aside for testing model performance. The extended test sets add the final row of your data (the most recent quarter), which contains the predictors needed to generate a forecast for the next period.

```{r}
# Create extended test sets including the quarter_four data
create_extended_test_sets <- function(dep_var_datasets, expanding_windows, quarter_four) {
  extended_test_sets <- list()
  
  for(dep_var_name in names(dep_var_datasets)) {
    current_data <- dep_var_datasets[[dep_var_name]]
    dataset_windows <- list()
    
    # For each window in the expanding windows
    for(window_idx in 1:length(expanding_windows[[dep_var_name]])) {
      # Get the original test data from the expanding window
      test_data <- expanding_windows[[dep_var_name]][[window_idx]]$test
      
      # Add quarter_four data as the last row
      # Make sure to match column names
      if (!all(names(quarter_four[[dep_var_name]]) == names(test_data))) {
        names(quarter_four[[dep_var_name]]) <- names(test_data)
      }
      
      # Combine test data with quarter_four
      extended_test <- rbind(test_data, quarter_four[[dep_var_name]])
      
      # Add to windows
      dataset_windows[[window_idx]] <- extended_test
    }
    
    extended_test_sets[[dep_var_name]] <- dataset_windows
  }
  
  return(extended_test_sets)
}

# 95% Predictive Intervals using the fixed extended test sets
get_predictive_intervals <- function(all_evaluations, all_modelewall, extended_test_sets) {
  for(dep_var in names(all_evaluations)) {
    # Get SMAPE values and window numbers for mean-based
    smape_mean <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_mean) || is.na(x$errors_mean$SMAPE)) return(Inf)
      return(x$errors_mean$SMAPE)
    })
    
    # Sort and get indices of windows with valid SMAPE values
    valid_windows_mean <- which(is.finite(smape_mean))
    if(length(valid_windows_mean) > 0) {
      top_n_mean <- min(5, length(valid_windows_mean))
      ordered_windows_mean <- valid_windows_mean[order(smape_mean[valid_windows_mean])]
      top_5_mean <- ordered_windows_mean[1:top_n_mean]
    } else {
      top_5_mean <- integer(0)
    }
    
    # Get SMAPE values and window numbers for median-based
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_median) || is.na(x$errors_median$SMAPE)) return(Inf)
      return(x$errors_median$SMAPE)
    })
    
    # Sort and get indices of windows with valid SMAPE values
    valid_windows_median <- which(is.finite(smape_median))
    if(length(valid_windows_median) > 0) {
      top_n_median <- min(5, length(valid_windows_median))
      ordered_windows_median <- valid_windows_median[order(smape_median[valid_windows_median])]
      top_5_median <- ordered_windows_median[1:top_n_median]
    } else {
      top_5_median <- integer(0)
    }
    
    # Calculate intervals for mean-based top models
    cat("\n=====================================")
    cat(sprintf("\n%s - Mean-Based Top %d Models (by sMAPE)\n", dep_var, length(top_5_mean)))
    cat("=====================================\n")
    
    intervals_mean <- data.frame()
    for(window in top_5_mean) {
      # Skip if window or model doesn't exist
      if(is.null(extended_test_sets[[dep_var]][[window]]) || 
         is.null(all_modelewall[[dep_var]][[window]])) {
        cat("Skipping window", window, "- data or model not available\n")
        next
      }
      
      # Get the extended test data and extract the last row
      extended_data <- extended_test_sets[[dep_var]][[window]]
      last_row <- extended_data[nrow(extended_data), ]
      last_row_predictors <- last_row[-1, drop = FALSE]  # Remove first column (response)
      
      # Check for missing values and handle them
      if(any(is.na(last_row_predictors))) {
        cat("Warning: Missing values in predictors for window", window, "\n")
        # Try to impute missing values with column means from the test data
        # This is a simple approach - you might want to use more sophisticated imputation
        for(col in 1:ncol(last_row_predictors)) {
          if(is.na(last_row_predictors[1, col])) {
            # Use mean of non-NA values in this column from test data
            col_mean <- mean(extended_data[-nrow(extended_data), col+1], na.rm = TRUE)
            last_row_predictors[1, col] <- col_mean
          }
        }
      }
      
      # Check for invalid values after imputation
      if(any(is.na(last_row_predictors))) {
        cat("Error: Still have missing values after imputation in window", window, "\n")
        next
      }
      
      # Try to make prediction
      tryCatch({
        pred <- suppressWarnings(predict(all_modelewall[[dep_var]][[window]], 
                                       newdata = as.data.frame(last_row_predictors),
                                       burn = 100))
        
        # Check if prediction was successful
        if(is.null(pred) || is.null(pred$distribution) || ncol(pred$distribution) == 0) {
          cat("Error: Prediction failed for window", window, "\n")
          next
        }
        
        # Get the distribution
        dist <- pred$distribution[, 1]
        
        # Calculate intervals
        intervals_mean <- rbind(intervals_mean, data.frame(
          Window = window,
          SMAPE = smape_mean[window],
          Lower_95 = quantile(dist, 0.025),
          Mean = mean(dist),
          Median = median(dist),
          Upper_95 = quantile(dist, 0.975)
        ))
      }, error = function(e) {
        cat("Error in prediction for window", window, ":", conditionMessage(e), "\n")
      })
    }
    
    # Print results if any
    if(nrow(intervals_mean) > 0) {
      print(round(intervals_mean, 4))
    } else {
      cat("No valid predictions for mean-based models.\n")
    }
    
    # Calculate intervals for median-based top models
    cat("\n=====================================")
    cat(sprintf("\n%s - Median-Based Top %d Models (by sMAPE)\n", dep_var, length(top_5_median)))
    cat("=====================================\n")
    
    intervals_median <- data.frame()
    for(window in top_5_median) {
      # Skip if window or model doesn't exist
      if(is.null(extended_test_sets[[dep_var]][[window]]) || 
         is.null(all_modelewall[[dep_var]][[window]])) {
        cat("Skipping window", window, "- data or model not available\n")
        next
      }
      
      # Get the extended test data and extract the last row
      extended_data <- extended_test_sets[[dep_var]][[window]]
      last_row <- extended_data[nrow(extended_data), ]
      last_row_predictors <- last_row[-1, drop = FALSE]  # Remove first column (response)
      
      # Check for missing values and handle them
      if(any(is.na(last_row_predictors))) {
        cat("Warning: Missing values in predictors for window", window, "\n")
        # Try to impute missing values with column means from the test data
        for(col in 1:ncol(last_row_predictors)) {
          if(is.na(last_row_predictors[1, col])) {
            # Use mean of non-NA values in this column from test data
            col_mean <- mean(extended_data[-nrow(extended_data), col+1], na.rm = TRUE)
            last_row_predictors[1, col] <- col_mean
          }
        }
      }
      
      # Check for invalid values after imputation
      if(any(is.na(last_row_predictors))) {
        cat("Error: Still have missing values after imputation in window", window, "\n")
        next
      }
      
      # Try to make prediction
      tryCatch({
        pred <- suppressWarnings(predict(all_modelewall[[dep_var]][[window]], 
                                       newdata = as.data.frame(last_row_predictors),
                                       burn = 100))
        
        # Check if prediction was successful
        if(is.null(pred) || is.null(pred$distribution) || ncol(pred$distribution) == 0) {
          cat("Error: Prediction failed for window", window, "\n")
          next
        }
        
        # Get the distribution
        dist <- pred$distribution[, 1]
        
        # Calculate intervals
        intervals_median <- rbind(intervals_median, data.frame(
          Window = window,
          SMAPE = smape_median[window],
          Lower_95 = quantile(dist, 0.025),
          Mean = mean(dist),
          Median = median(dist),
          Upper_95 = quantile(dist, 0.975)
        ))
      }, error = function(e) {
        cat("Error in prediction for window", window, ":", conditionMessage(e), "\n")
      })
    }
    
    # Print results if any
    if(nrow(intervals_median) > 0) {
      print(round(intervals_median, 4))
    } else {
      cat("No valid predictions for median-based models.\n")
    }
  }
}

# Example usage:
# Create extended test sets with quarter_four data
extended_test_sets <- create_extended_test_sets(dep_var_datasets, expanding_windows, quarter_four)

# Apply the function
get_predictive_intervals(all_evaluations_allew, all_modelewall, extended_test_sets)
```

## MCMC trace plot

```{r}
# Function to find top 2 best models based on evaluations
find_best_models <- function(all_evaluations) {
  best_models <- list()  # To store the best models for each dep var
  
  for(var_name in names(all_evaluations)) {
    cat("\nFor", var_name, ":\n")
    
    # Collect all sMAPE values
    all_smape_results <- data.frame(
      Window = integer(),
      Method = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Collect mean-based sMAPE values
    for(i in 1:5) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "mean",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: mean_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Collect median-based sMAPE values
    for(i in 1:5) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "median",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: median_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Sort by sMAPE (ascending) to find the best models overall
    all_smape_results <- all_smape_results[order(all_smape_results$SMAPE), ]
    
    if (nrow(all_smape_results) >= 2) {
      # Get top 2 models
      top_2_models <- all_smape_results[1:2, ]
      
      cat("Top 2 models for", var_name, "based on sMAPE:\n")
      for (i in 1:2) {
        cat("Rank", i, ": Window", top_2_models$Window[i], "using", top_2_models$Method[i], 
            "method with sMAPE =", round(top_2_models$SMAPE[i], 4), "\n")
      }
      
      # Store the top 2 models info
      best_models[[var_name]] <- list(
        best_window_1 = top_2_models$Window[1],
        best_method_1 = top_2_models$Method[1],
        best_smape_1 = top_2_models$SMAPE[1],
        best_window_2 = top_2_models$Window[2],
        best_method_2 = top_2_models$Method[2],
        best_smape_2 = top_2_models$SMAPE[2]
      )
    } else if (nrow(all_smape_results) == 1) {
      # Handle case where only one valid model is found
      cat("Only one valid model found for", var_name, ":\n")
      cat("Window:", all_smape_results$Window[1], "using", all_smape_results$Method[1], 
          "method with sMAPE =", round(all_smape_results$SMAPE[1], 4), "\n")
      
      best_models[[var_name]] <- list(
        best_window_1 = all_smape_results$Window[1],
        best_method_1 = all_smape_results$Method[1],
        best_smape_1 = all_smape_results$SMAPE[1]
      )
    } else {
      cat("No valid models found for", var_name, "\n")
    }
  }
  
  return(best_models)
}

# Run find_best_models function to get best models based on sMAPE
best_models <- find_best_models(all_evaluations_allew)

```

```{r}
analyze_bsts_mcmc_trace_plots <- function(all_evaluations, all_models, best_models) {
  # Create a PDF device to save all plots
  pdf("MCMC_trace_plot_EW_ALL_2023Q1.pdf", width=12, height=10)
  
  for(dep_var in names(best_models)) {
    # Get the info for the top models
    best_model_info <- best_models[[dep_var]]
    
    # Process Rank 1 model
    window_1 <- best_model_info$best_window_1
    method_1 <- best_model_info$best_method_1
    smape_1 <- best_model_info$best_smape_1
    
    cat("\nCreating MCMC trace plots for", dep_var, ":\n")
    cat("Rank 1: Window:", window_1, "using", method_1, "method, sMAPE =", round(smape_1, 4), "\n")
    
    # Fetch the model
    model_1 <- all_models[[dep_var]][[window_1]]
    
    # Create trace plots for Rank 1 model
    create_trace_plots(model_1, dep_var, "Rank 1", window_1, method_1, smape_1)
    
    # Process Rank 2 model if available
    if(!is.null(best_model_info$best_window_2)) {
      window_2 <- best_model_info$best_window_2
      method_2 <- best_model_info$best_method_2
      smape_2 <- best_model_info$best_smape_2
      
      cat("Rank 2: Window:", window_2, "using", method_2, "method, sMAPE =", round(smape_2, 4), "\n")
      
      # Fetch the second-ranked model
      model_2 <- all_models[[dep_var]][[window_2]]
      
      # Create trace plots for Rank 2 model
      create_trace_plots(model_2, dep_var, "Rank 2", window_2, method_2, smape_2)
    } else {
      cat("No second-best model available for", dep_var, "\n")
    }
  }
  
  dev.off()
}

# Helper function to create trace plots for a model
create_trace_plots <- function(model, dep_var, rank_label, window, method, smape) {
  tryCatch({
    # Set up plot for state variances
    par(mfrow=c(3,1), mar=c(4,4,3,1))
    
    # Plot observation variance
    if (!is.null(model$sigma.obs)) {
      plot(1:length(model$sigma.obs), model$sigma.obs, type="l", col="blue",
           main="Observation Variance (sigma.obs)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot seasonal variance if it exists
    if (!is.null(model$sigma.seasonal.4)) {
      plot(1:length(model$sigma.seasonal.4), model$sigma.seasonal.4, type="l", col="red",
           main="Seasonal Variance (sigma.seasonal.4)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot trend level variance if it exists
    if (!is.null(model$sigma.trend.level)) {
      plot(1:length(model$sigma.trend.level), model$sigma.trend.level, type="l", col="green",
           main="Trend Level Variance (sigma.trend.level)",
           xlab="Iteration", ylab="Value")
    }
    
    mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Variances"), 
          outer=TRUE, line=-1.5, cex=1.2)
    
    # If the model has regression coefficients, plot those on a new page
    if (!is.null(model$coefficients) && !is.null(model$coefficients.samples) && 
        ncol(model$coefficients.samples) > 0) {
      
      # Calculate number of coefficient plots needed
      num_coefs <- ncol(model$coefficients.samples)
      rows_needed <- min(4, num_coefs)  # Maximum 4 coefficients per page
      
      # Start a new page
      par(mfrow=c(rows_needed, 1), mar=c(4,4,3,1))
      
      # Plot each coefficient
      for (i in 1:rows_needed) {
        coef_name <- colnames(model$coefficients.samples)[i]
        if (is.null(coef_name)) coef_name <- paste("Coefficient", i)
        
        coef_samples <- model$coefficients.samples[,i]
        plot(1:length(coef_samples), coef_samples, type="l", col="purple",
             main=paste("Regression Coefficient:", coef_name),
             xlab="Iteration", ylab="Value")
      }
      
      mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Coefficients"), 
            outer=TRUE, line=-1.5, cex=1.2)
    }
    
  }, error = function(e) {
    # If error occurs, print the error and try the built-in plotting function
    cat("Error in custom trace plotting for", rank_label, ":", e$message, "\n")
    cat("Falling back to built-in plotting...\n")
    
    # Reset the plot area
    par(mfrow=c(1,1))
    
    # Try the built-in plotting method
    tryCatch({
      # Try plotting state components instead
      plot(model, "components")
      title(main=paste0(dep_var, " (", rank_label, "): Component Contributions"))
    }, error = function(e2) {
      cat("Error in fallback plotting:", e2$message, "\n")
      
      # Create an empty plot with error message
      plot(1, 1, type="n", axes=FALSE, xlab="", ylab="")
      text(1, 1, paste("Error plotting MCMC traces for", rank_label), col="red", cex=1.5)
    })
  })
}

analyze_bsts_mcmc_trace_plots(all_evaluations_allew, all_modelewall, best_models)
```

## Distribution Plot

```{r}
create_prediction_plots <- function(all_evaluations, all_models, extended_test_sets, hold_out_dataset) {
  library(ggplot2)
  pdf("Prediction_Distribution_EW_ALL_2023Q1.pdf", width = 12, height = 8)
  
  for(dep_var in names(all_evaluations)) {
    # Create a data frame of all SMAPE values (mean and median) per window
    all_results <- data.frame(
      Window = numeric(),
      Type = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    for(i in 1:length(all_evaluations[[dep_var]])) {
      # Add mean results
      mean_smape <- all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE
      if(!is.na(mean_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "mean", SMAPE = mean_smape, stringsAsFactors = FALSE))
      }
      # Add median results
      median_smape <- all_evaluations[[dep_var]][[i]]$errors_median$SMAPE
      if(!is.na(median_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "median", SMAPE = median_smape, stringsAsFactors = FALSE))
      }
    }
    
    # Sort by SMAPE (ascending) and select the top 4 rows
    top_4 <- all_results[order(all_results$SMAPE), ][1:4, ]
    
    plot_data <- data.frame()
    label_list <- c()  # to track labels and catch duplicates
    
    for(i in 1:nrow(top_4)) {
      window <- top_4$Window[i]
      model_type <- top_4$Type[i]
      smape_value <- top_4$SMAPE[i]
      
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(extended_test_sets[[dep_var]][[window]][nrow(extended_test_sets[[dep_var]][[window]]), -1, drop = FALSE]),
                           burn = 100)
      
      # Create a label that includes rank, window, type, and SMAPE.
      base_label <- paste("Rank", i, ": Window", window, "-", model_type, "(SMAPE:", round(smape_value, 4), ")")
      # If the same label already exists, append a suffix.
      duplicate_count <- sum(label_list == base_label)
      if(duplicate_count > 0) {
        label <- paste0(base_label, " - Copy", duplicate_count + 1)
      } else {
        label <- base_label
      }
      label_list <- c(label_list, label)
      
      plot_data <- rbind(plot_data, 
                         data.frame(Value = pred$distribution[,1],
                                    Model = factor(label),
                                    stringsAsFactors = FALSE))
    }
    
    # Get the actual observed value from the hold_out_dataset (assume it's the last value in column 1)
    actual_value <- tail(hold_out_dataset[[dep_var]][, 1], 1)
    
    p <- ggplot(plot_data, aes(x = Value, fill = Model)) +
      geom_density(alpha = 0.4) +
      theme_minimal() +
      labs(title = paste(dep_var, "- Top 4 Models Based on sMAPE"),
           x = "Predicted Value",
           y = "Density") +
      geom_vline(xintercept = actual_value, color = "red", linetype = "dashed", size = 1) +
      annotate("text", x = actual_value, y = Inf, label = paste("Actual:", actual_value),
               vjust = -0.5, color = "red", size = 3)
    
    print(p)
  }
  
  dev.off()
}


# Apply the function
create_prediction_plots(all_evaluations_allew, all_modelewall, extended_test_sets, hold_out_dataset)
```

## Continuous Ranked Probability Score (CRPS)

```{r}
# Function to calculate CRPS
calculate_crps <- function(actual, pred_dist) {
  n <- length(pred_dist)
  sorted_pred <- sort(pred_dist)
  H <- function(x) ifelse(x >= 0, 1, 0)
  
  integral <- 0
  for(i in 1:(n-1)) {
    x <- sorted_pred[i]
    dx <- sorted_pred[i+1] - x
    F_x <- i/n
    integral <- integral + (F_x - H(x - actual))^2 * dx
  }
  return(integral)
}

# Fixed function to calculate CRPS for all windows and dependent variables
get_crps_scores <- function(all_models, expanding_windows) {
  crps_results <- list()
  
  for(dep_var in names(all_models)) {
    window_crps <- data.frame()
    
    for(i in 1:5) {  
      # Get the correct model (the argument order was incorrect before)
      model <- all_models[[dep_var]][[i]]
      
      # Check if the model is a bsts object
      if(!inherits(model, "bsts")) {
        cat("Warning: Model for", dep_var, "window", i, "is not a bsts object. Skipping.\n")
        next
      }
      
      # Get test data for this window
      test_data <- expanding_windows[[dep_var]][[i]]$test
      actual <- test_data[, 1]
      
      # Make predictions
      pred <- predict(model, newdata = as.data.frame(test_data[, -1]), burn = 100)
      
      # Calculate CRPS for each observation
      crps_values <- numeric(length(actual))
      for(j in seq_along(actual)) {
        crps_values[j] <- calculate_crps(actual[j], pred$distribution[,j])
      }
      
      # Store results
      window_crps <- rbind(window_crps, data.frame(
        Window = i,
        CRPS_Mean = mean(crps_values),
        CRPS_SD = sd(crps_values)
      ))
    }
    
    crps_results[[dep_var]] <- window_crps
  }
  
  # Print results
  for(dep_var in names(crps_results)) {
    cat("\n=====================================")
    cat(sprintf("\n%s - CRPS Scores\n", dep_var))
    cat("=====================================\n")
    print(round(crps_results[[dep_var]], 4))
  }
  
  return(crps_results)
}

# Apply the fixed function - with correct argument order
crps_scores <- get_crps_scores(all_modelewall, expanding_windows)
```

## log predictive density

```{r}
calculate_lpd <- function(all_models, expanding_windows) {
 lpd_results <- list()
 
 for(dep_var in names(all_models)) {
   window_lpd <- data.frame()
   
   for(i in 1:5) {
     model <- all_models[[dep_var]][[i]]
     test_data <- expanding_windows[[dep_var]][[i]]$test
     actual <- test_data[, 1]
     
     # Get predictions
     pred <- predict.bsts(model, newdata = as.data.frame(test_data[, -1]), burn = 100)
     
     # Calculate LPD for each observation
     lpd_values <- numeric(length(actual))
     for(j in seq_along(actual)) {
       # Get density estimate of prediction distribution
       density_est <- density(pred$distribution[,j])
       # Find density at actual value
       actual_density <- approx(density_est$x, density_est$y, xout = actual[j])$y
       # Log density
       lpd_values[j] <- log(actual_density)
     }
     
     window_lpd <- rbind(window_lpd, data.frame(
       Window = i,
       LPD_Mean = mean(lpd_values, na.rm = TRUE),
       LPD_SD = sd(lpd_values, na.rm = TRUE)
     ))
   }
   lpd_results[[dep_var]] <- window_lpd
 }
 
 # Print results
 for(dep_var in names(lpd_results)) {
   cat("\n=====================================")
   cat(sprintf("\n%s - Log Predictive Density\n", dep_var))
   cat("=====================================\n")
   print(round(lpd_results[[dep_var]], 4))
 }
 
 return(lpd_results)
}

# Apply function
lpd_scores <- calculate_lpd(all_modelewall, expanding_windows)
```

## Sensitivity

```{r}
# First, clean up any existing parallel connections
try(stopImplicitCluster(), silent = TRUE)
try(stopCluster(cl), silent = TRUE)
gc()  # Force garbage collection

# Load required packages
library(bsts)
library(parallel)

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to identify the best models
get_best_models_safe <- function() {
  best_predictions <- list()
  
  for (dep_var in names(all_modelewall )) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:6) {
      if (!dep_var %in% names(all_modelewall ) || length(all_modelewall [[dep_var]]) < i) {
        next
      }
      
      tryCatch({
        model <- all_modelewall [[dep_var]][[i]]
        
        # Get the testing data for this window and variable
        test_data <- expanding_windows[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        mape_mean <- mean(abs((actual_values - pred_mean) / actual_values)) * 100
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                          (abs(actual_values) + abs(pred_mean))) * 100
        
        mape_median <- mean(abs((actual_values - pred_median) / actual_values)) * 100
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                            (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to results
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = mape_mean, SMAPE = smape_mean),
                       data.frame(Window = i, Method = "median", 
                                MAPE = mape_median, SMAPE = smape_median))
      }, error = function(e) {
        cat("Error processing", dep_var, "window", i, ":", e$message, "\n")
      })
    }
    
    # Select the best five models
    if (nrow(results) > 0) {
      best_five <- results[order(results$MAPE), ][1:min(5, nrow(results)), ]
      best_predictions[[dep_var]] <- best_five
    }
  }
  
  return(best_predictions)
}

# Function to analyze one configuration
analyze_config <- function(params) {
  dep_var <- params$dep_var
  window_size <- params$window_size
  method <- params$method
  sigma <- params$sigma
  slab_var <- params$slab_var
  
  cat("Processing", dep_var, "window", window_size, "method", method, 
      "sigma", sigma, "slab_var", slab_var, "\n")
  
  # Get training and testing data
  train_data <- expanding_windows[[dep_var]][[window_size]]$train
  test_data <- expanding_windows[[dep_var]][[window_size]]$test
  
  # Extract values
  y <- train_data[, 1]
  X <- as.data.frame(train_data[, -1])
  actual_values <- test_data[, 1]
  test_predictors <- as.data.frame(test_data[, -1])
  
  # Create formula
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Full data for model
  model_data <- as.data.frame(cbind(y = y, X))
  
  # Create state specification
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y, 
                           level.sigma.prior = SdPrior(sigma = sigma),
                           slope.sigma.prior = SdPrior(sigma = sigma))
  
  # Fit model
  result <- tryCatch({
    # Using the approach that worked in our test
    model <- bsts(formula, 
                 state.specification = ss,
                 niter = 10000,
                 data = model_data)
    
    # Make predictions
    all_predictions <- numeric(length(actual_values))
    
    for (t in 1:length(actual_values)) {
      # Prepare data for this period
      pred_data <- data.frame(y = NA)
      pred_data <- cbind(pred_data, test_predictors[t, , drop = FALSE])
      
      # Predict
      single_pred <- predict(model, newdata = pred_data, burn = 100)
      
      # Extract prediction
      if (method == "mean") {
        all_predictions[t] <- mean(single_pred$distribution[, 1])
      } else {
        all_predictions[t] <- median(single_pred$distribution[, 1])
      }
    }
    
    # Check if predictions vary
    has_varying_predictions <- length(unique(round(all_predictions, 2))) > 1
    
    # Calculate metrics
    mape <- mean(abs((actual_values - all_predictions) / actual_values)) * 100
    smape <- mean(2 * abs(actual_values - all_predictions) / 
                 (abs(actual_values) + abs(all_predictions))) * 100
    
    waic <- NA
    looic <- NA
    if (!is.null(model$log.likelihood)) {
      waic <- -2 * mean(model$log.likelihood)
      looic <- -2 * mean(model$log.likelihood)
    }
    
    return(list(
      success = TRUE,
      sigma = sigma,
      slab_var = slab_var,
      mape = mape,
      smape = smape,
      waic = waic,
      looic = looic,
      has_regression = has_varying_predictions
    ))
    
  }, error = function(e) {
    return(list(
      success = FALSE,
      error = e$message
    ))
  })
  
  return(result)
}

# Controlled parallel approach
run_sensitivity_analysis_parallel <- function() {
  # Define parameter combinations
  sigmas <- c(0.2, 0.4, 0.6, 0.8)
  slab_vars <- c(50, 100, 200)
  
  # Get best models
  best_models <- get_best_models_safe()
  
  # Initialize results
  sensitivity_results <- list()
  
  # For each dependent variable
  for (dep_var in names(best_models)) {
    cat("\n=== Starting analysis for", dep_var, "===\n")
    sensitivity_results[[dep_var]] <- list()
    models_info <- best_models[[dep_var]]
    
    # For each top model
    for (i in 1:min(5, nrow(models_info))) {
      model_info <- models_info[i, ]
      window_size <- model_info$Window
      method <- as.character(model_info$Method)
      model_name <- paste0("model_", i)
      
      cat("\n--- Processing", dep_var, "window", window_size, "method", method, "---\n")
      
      # Initialize results for this model
      model_results <- list()
      
      # Create parameter combinations
      param_list <- list()
      for (sigma in sigmas) {
        for (slab_var in slab_vars) {
          param_list[[length(param_list) + 1]] <- list(
            dep_var = dep_var,
            window_size = window_size,
            method = method,
            sigma = sigma,
            slab_var = slab_var
          )
        }
      }
      
      # Create a small cluster - use just 2 or 3 cores for stability
      num_cores <- min(3, detectCores() - 1)
      cat("Using", num_cores, "cores for parallel processing\n")
      cl <- makeCluster(num_cores)
      
      # Export required data and functions
      clusterExport(cl, c("expanding_windows"), envir = .GlobalEnv)
      clusterEvalQ(cl, {
        library(bsts)
      })
      
      # Run analysis in parallel
      config_results <- parLapply(cl, param_list, analyze_config)
      
      # Clean up
      stopCluster(cl)
      
      # Process results
      for (j in 1:length(param_list)) {
        params <- param_list[[j]]
        result <- config_results[[j]]
        
        config_name <- paste0("sigma_", params$sigma, "_slab_", params$slab_var)
        
        if (result$success) {
          # Store successful result
          model_results[[config_name]] <- list(
            sigma = params$sigma,
            slab_var = params$slab_var,
            mape = result$mape,
            smape = result$smape,
            waic = result$waic,
            looic = result$looic,
            has_regression = result$has_regression
          )
          
          cat("Configuration", config_name, "- MAPE:", round(result$mape, 4),
             "SMAPE:", round(result$smape, 4), "Has regression:", result$has_regression, "\n")
        } else {
          cat("Configuration", config_name, "failed:", result$error, "\n")
        }
      }
      
      # Store results for this model
      sensitivity_results[[dep_var]][[model_name]] <- list(
        window = window_size,
        method = method,
        results = model_results
      )
      
      # Find best configuration
      best_config <- NULL
      best_mape <- Inf
      best_config_name <- ""
      
      for (config_name in names(model_results)) {
        config <- model_results[[config_name]]
        if (!is.na(config$mape) && config$mape < best_mape) {
          best_mape <- config$mape
          best_config <- config
          best_config_name <- config_name
        }
      }
      
      if (is.null(best_config)) {
        cat("No valid configurations found.\n")
      } else {
        # Print best configuration
        cat("\nBest configuration:", best_config_name, "\n")
        cat("MAPE:", round(best_config$mape, 4), "\n")
        cat("SMAPE:", round(best_config$smape, 4), "\n")
        cat("Sigma:", best_config$sigma, "\n")
        cat("Slab variance:", best_config$slab_var, "\n")
        cat("Has regression:", best_config$has_regression, "\n\n")
        
        # Create summary table
        config_summary <- data.frame(
          Sigma = numeric(),
          SlabVar = numeric(),
          MAPE = numeric(),
          SMAPE = numeric(),
          WAIC = numeric(),
          LOOIC = numeric(),
          HasRegression = logical()
        )
        
        for (config_name in names(model_results)) {
          config <- model_results[[config_name]]
          config_summary <- rbind(config_summary, 
                                data.frame(
                                  Sigma = config$sigma,
                                  SlabVar = config$slab_var,
                                  MAPE = config$mape,
                                  SMAPE = config$smape,
                                  WAIC = config$waic,
                                  LOOIC = config$looic,
                                  HasRegression = config$has_regression
                                ))
        }
        
        # Set row names and sort
        rownames(config_summary) <- names(model_results)
        config_summary <- config_summary[order(config_summary$MAPE), ]
        print(round(config_summary, 4))
      }
      
      # Clean up after each model
      rm(model_results, config_results)
      gc()
    }
  }
  
  return(sensitivity_results)
}

# Run the analysis
start_time <- Sys.time()
cat("Starting sensitivity analysis at:", format(start_time), "\n")

sensitivity_results <- run_sensitivity_analysis_parallel()

end_time <- Sys.time()
execution_time <- end_time - start_time
cat("\nSensitivity analysis completed in:", format(execution_time), "\n")
```


```{r}
# Function to identify the top 5 models by SMAPE
find_top_models <- function(all_modelewall , sensitivity_results) {
  top_models_summary <- list()
  
  # Process each dependent variable
  for(dep_var in names(sensitivity_results)) {
    cat("\n================================================================\n")
    cat("TOP 5 MODELS FOR", toupper(dep_var), "BASED ON SMAPE\n")
    cat("================================================================\n")
    
    # 1. Collect all model results including window performance and sensitivity analysis
    all_models <- data.frame(
      Source = character(),
      Window = integer(),
      Method = character(),
      Sigma = numeric(),
      SlabVar = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Add window performance results (from best_models)
    for (i in 1:5) { # Assuming 6 windows
      if (dep_var %in% names(all_modelewall ) && length(all_modelewall [[dep_var]]) >= i) {
        # Get the testing data for this window and variable
        test_data <- expanding_windows[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(all_modelewall [[dep_var]][[i]], newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate SMAPE for mean
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                         (abs(actual_values) + abs(pred_mean))) * 100
        
        # Calculate SMAPE for median
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                           (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to all_models dataframe
        all_models <- rbind(all_models, 
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "mean",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_mean,
                            stringsAsFactors = FALSE
                          ),
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "median",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_median,
                            stringsAsFactors = FALSE
                          ))
      }
    }
    
    # Add sensitivity analysis results
    if (dep_var %in% names(sensitivity_results)) {
      for (model_name in names(sensitivity_results[[dep_var]])) {
        model_data <- sensitivity_results[[dep_var]][[model_name]]
        window <- model_data$window
        method <- model_data$method
        
        for (config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_models <- rbind(all_models,
                            data.frame(
                              Source = "Sensitivity",
                              Window = window,
                              Method = method,
                              Sigma = config$sigma,
                              SlabVar = config$slab_var,
                              SMAPE = config$smape,
                              stringsAsFactors = FALSE
                            ))
        }
      }
    }
    
    # 2. Sort by SMAPE and select top 5 models
    top_models <- all_models[order(all_models$SMAPE), ][1:min(5, nrow(all_models)), ]
    
    # 3. Create a formatted output
    formatted_models <- data.frame(
      Rank = 1:nrow(top_models),
      Description = character(nrow(top_models)),
      SMAPE = top_models$SMAPE,
      stringsAsFactors = FALSE
    )
    
    for (i in 1:nrow(top_models)) {
      model <- top_models[i, ]
      
      if (model$Source == "Window") {
        desc <- sprintf("%s Window %d", model$Method, model$Window)
      } else {
        desc <- sprintf("%s Window %d (sigma=%.1f, slab=%.0f)", 
                      model$Method, model$Window, model$Sigma, model$SlabVar)
      }
      
      formatted_models$Description[i] <- desc
    }
    
    # Display the top models
    print(formatted_models)
    
    # Store the results
    top_models_summary[[dep_var]] <- list(
      top_models = top_models,
      formatted_output = formatted_models
    )
  }
  
  return(top_models_summary)
}

# Example usage:
 top_model_results <- find_top_models(all_modelewall , sensitivity_results)
```

```{r}
# Function to predict holdout period using top 5 models and create weighted ensemble
predict_and_create_ensemble <- function(all_modelewall , top_model_results, hold_out_dataset) {
  # Clean up any existing connections
  try(stopImplicitCluster(), silent = TRUE)
  try(stopCluster(cl), silent = TRUE)
  gc()  # Force garbage collection
  
  # Load required packages
  library(parallel)
  
  # To store all results
  all_predictions <- list()
  
  # For each dependent variable
  for(dep_var in names(top_model_results)) {
    cat("\n================================================================\n")
    cat("HOLDOUT PREDICTIONS FOR", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Get top models for this variable
    top_models <- top_model_results[[dep_var]]$top_models
    
    # Get holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      cat("Holdout data not available for", dep_var, "\n")
      next
    }
    
    holdout_data <- hold_out_dataset[[dep_var]]
    
    # Prepare parameters for parallel execution
    param_list <- list()
    for(i in 1:nrow(top_models)) {
      param_list[[i]] <- list(
        dep_var = dep_var,
        model_info = top_models[i, ],
        model_index = i
      )
    }
    
    # Create a small cluster - use just 2 or 3 cores for stability
    num_cores <- min(3, detectCores() - 1)
    cat("Using", num_cores, "cores for parallel processing\n")
    cl <- makeCluster(num_cores)
    
    # Export required data and functions
    clusterExport(cl, c("all_modelewall", "hold_out_dataset", "expanding_windows"), envir = .GlobalEnv)
    clusterEvalQ(cl, {
      library(bsts)
    })
    
    # Worker function for parallel execution
    predict_model_worker <- function(params) {
      dep_var <- params$dep_var
      model_info <- params$model_info
      model_index <- params$model_index
      
      # Create result container
      result <- list(
        model_index = model_index,
        dep_var = dep_var,
        success = FALSE
      )
      
      # Get the holdout data
      holdout_data <- hold_out_dataset[[dep_var]]
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Create period labels
      period_labels <- c(as.character(105:111), "Q1")#period
      
      # Create model description
      if(model_info$Source == "Window") {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, ")")
      } else {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, 
                          ", sigma=", model_info$Sigma, 
                          ", slab=", model_info$SlabVar, ")")
      }
      
      tryCatch({
        # Get or create model based on source
        if(model_info$Source == "Window") {
          # Use existing model
          window <- model_info$Window
          method <- as.character(model_info$Method)
          
          if(!(dep_var %in% names(all_modelewall )) || length(all_modelewall [[dep_var]]) < window) {
            return(c(result, list(error = "Model not available")))
          }
          
          model <- all_modelewall [[dep_var]][[window]]
          
        } else {
          # Create model with sensitivity parameters
          window <- model_info$Window
          method <- as.character(model_info$Method)
          sigma <- model_info$Sigma
          slab_var <- model_info$SlabVar
          
          # Get original model
          if(!(dep_var %in% names(all_modelewall )) || length(all_modelewall [[dep_var]]) < window) {
            return(c(result, list(error = "Original model not available")))
          }
          
          original_model <- all_modelewall [[dep_var]][[window]]
          
          # Get training data
          train_data <- expanding_windows[[dep_var]][[window]]$train
          y <- train_data[, 1]
          X <- as.data.frame(train_data[, -1])
          
          # Create formula
          predictors <- colnames(X)
          formula_str <- paste("y ~", paste(predictors, collapse = " + "))
          formula <- as.formula(formula_str)
          
          # Create model data
          model_data <- as.data.frame(cbind(y = y, X))
          
          # Set up state specification
          ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
          ss <- AddLocalLinearTrend(ss, y, 
                                  level.sigma.prior = SdPrior(sigma = sigma),
                                  slope.sigma.prior = SdPrior(sigma = sigma))
          
          # Create model
          model <- bsts(formula, 
                       state.specification = ss,
                       niter = 10000,
                       data = model_data)
        }
        
        # Make predictions - using individual predictions for each period
        predictions <- numeric(nrow(holdout_data))
        
        for(t in 1:nrow(holdout_data)) {
          # Create data for this period
          if(model_info$Source == "Window") {
            # For window models
            this_period_data <- pred_data[t, , drop = FALSE]
            
            # Make prediction
            single_pred <- predict(model, newdata = this_period_data, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          } else {
            # For sensitivity models
            pred_row <- data.frame(y = NA)
            pred_row <- cbind(pred_row, pred_data[t, , drop = FALSE])
            
            # Make prediction
            single_pred <- predict(model, newdata = pred_row, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          }
        }
        
        # Calculate errors
        abs_errors <- abs(predictions - actual_values)
        avg_abs_error <- mean(abs_errors)
        
        # Return successful result
        return(list(
          model_index = model_index,
          dep_var = dep_var,
          success = TRUE,
          model_desc = model_desc,
          predictions = predictions,
          actual_values = actual_values,
          abs_errors = abs_errors,
          avg_abs_error = avg_abs_error,
          period_labels = period_labels,
          smape = model_info$SMAPE
        ))
        
      }, error = function(e) {
        return(c(result, list(error = e$message)))
      })
    }
    
    # Run predictions in parallel
    model_results <- parLapply(cl, param_list, predict_model_worker)
    
    # Stop cluster
    stopCluster(cl)
    
    # Process and display results
    var_predictions <- list()
    
    for(result in model_results) {
      if(result$success) {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, ":", result$model_desc, "\n")
        cat("----------------------------------------------------------------\n")
        
        # Print results in a tabular format
        cat("\nPeriod\tActual\t\tPredicted\tAbsError\n")
        for(p in 1:length(result$period_labels)) {
          cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                    result$period_labels[p], 
                    result$actual_values[p], 
                    result$predictions[p], 
                    result$abs_errors[p]))
        }
        
        cat("\nAverage Absolute Error:", round(result$avg_abs_error, 2), "\n")
        
        # Store successful predictions
        var_predictions[[result$model_index]] <- result
      } else {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, "failed:", result$error, "\n")
        cat("----------------------------------------------------------------\n")
      }
    }
    
    # Store predictions for this variable
    all_predictions[[dep_var]] <- var_predictions
    
    # Now create weighted ensemble
    successful_models <- model_results[sapply(model_results, function(r) r$success)]
    
    if(length(successful_models) > 0) {
      # Get the actual values and period labels from the first successful model
      actual_values <- successful_models[[1]]$actual_values
      period_labels <- successful_models[[1]]$period_labels
      
      # Create prediction matrix
      pred_matrix <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      smape_values <- numeric(length(successful_models))
      model_descriptions <- character(length(successful_models))
      
      # Fill the matrix with predictions
      for(i in 1:length(successful_models)) {
        pred_matrix[, i] <- successful_models[[i]]$predictions
        smape_values[i] <- successful_models[[i]]$smape
        model_descriptions[i] <- successful_models[[i]]$model_desc
      }
      
      # Calculate weights based on inverse SMAPE
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      
      # Display the weights
      cat("\n================================================================\n")
      cat("WEIGHTED ENSEMBLE FOR", toupper(dep_var), "\n")
      cat("================================================================\n")
      
      cat("\nModel Weights:\n")
      for(i in 1:length(weights)) {
        cat(sprintf("Model %d: %s - Weight: %.4f (SMAPE: %.4f)\n", 
                  i, model_descriptions[i], weights[i], smape_values[i]))
      }
      
      # Calculate weighted predictions
      weighted_predictions <- numeric(length(actual_values))
      for(i in 1:length(actual_values)) {
        weighted_predictions[i] <- sum(pred_matrix[i, ] * weights, na.rm = TRUE)
      }
      
      # Calculate errors
      abs_errors <- abs(weighted_predictions - actual_values)
      
      # Calculate ensemble SMAPE
      ensemble_smape <- mean(2 * abs(actual_values - weighted_predictions) / 
                           (abs(actual_values) + abs(weighted_predictions))) * 100
      
      # Display results
      cat("\n----------------------------------------------------------------\n")
      cat("WEIGHTED ENSEMBLE RESULTS\n")
      cat("----------------------------------------------------------------\n")
      cat("Ensemble SMAPE:", round(ensemble_smape, 4), "%\n\n")
      
      cat("Period\tActual\t\tPredicted\tAbsError\n")
      for(p in 1:length(period_labels)) {
        cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                  period_labels[p], 
                  actual_values[p], 
                  weighted_predictions[p], 
                  abs_errors[p]))
      }
      
      cat("\nAverage Absolute Error:", round(mean(abs_errors), 2), "\n")
    } else {
      cat("\nNo successful models for", dep_var, "- cannot create ensemble\n")
    }
  }
  
  # Return all predictions
  return(all_predictions)
}

# Run the function to predict and create weighted ensemble
all_results <- predict_and_create_ensemble(all_modelewall, top_model_results, hold_out_dataset)
```

```{r}
# This function replicates your 'print_ensemble_tables' logic but returns a named vector
# of Weighted Ensemble SMAPEs (one entry per dep_var).
get_ensemble_smapes <- function(all_results, top_model_results, hold_out_dataset) {
  ensemble_smapes <- numeric(0)  # named vector
  
  for(dep_var in names(top_model_results)) {
    # Skip if no holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      next
    }
    holdout_data  <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    
    # Filter out NULL models
    successful_models <- all_results[[dep_var]]
    successful_models <- successful_models[!sapply(successful_models, is.null)]
    
    if(length(successful_models) > 0) {
      smape_values <- numeric(length(successful_models))
      pred_matrix  <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      
      for(i in seq_along(successful_models)) {
        pred_matrix[, i]  <- successful_models[[i]]$predictions
        smape_values[i]   <- successful_models[[i]]$smape
      }
      
      # Weighted predictions
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      weighted_predictions <- rowSums(t(t(pred_matrix) * weights), na.rm = TRUE)
      
      # Weighted Ensemble SMAPE
      ensemble_smape <- mean(
        2 * abs(actual_values - weighted_predictions) /
        (abs(actual_values) + abs(weighted_predictions))
      ) * 100
      
      # Store in named vector
      ensemble_smapes[dep_var] <- ensemble_smape
    }
  }
  
  return(ensemble_smapes)
}
ensemble_smapes = get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
```

## ensemble result 
```{r}
# Function to combine existing results and create ensemble SMAPE
combine_existing_results <- function(all_modelewall, sensitivity_results, hold_out_dataset) {
  
  final_results <- data.frame(
    Variable = character(),
    Prediction_Type = character(),
    Ensemble_SMAPE = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Process each dependent variable
  for(dep_var in names(hold_out_dataset)) {
    cat("\n================================================================\n")
    cat("PROCESSING", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Collect all SMAPE values and predictions from both methods
    all_configs <- data.frame(
      Method = character(),
      Window = integer(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Method 1: Original window models (from all_modelewall)
    for(window in 1:5) {
      if(dep_var %in% names(all_modelewall) && length(all_modelewall[[dep_var]]) >= window) {
        
        # Get test data
        test_data <- expanding_windows[[dep_var]][[window]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Get existing model and make predictions
        model <- all_modelewall[[dep_var]][[window]]
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        
        # Calculate SMAPE for this configuration
        smape <- mean(2 * abs(actual_values - pred_mean) / 
                     (abs(actual_values) + abs(pred_mean))) * 100
        
        all_configs <- rbind(all_configs, 
          data.frame(Method = "Original", Window = window, SMAPE = smape))
      }
    }
    
    # Method 2: Sensitivity analysis (from sensitivity_results)
    if(dep_var %in% names(sensitivity_results)) {
      for(model_name in names(sensitivity_results[[dep_var]])) {
        model_data <- sensitivity_results[[dep_var]][[model_name]]
        
        for(config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_configs <- rbind(all_configs, 
            data.frame(Method = "Sensitivity", 
                      Window = model_data$window, 
                      SMAPE = config$smape))
        }
      }
    }
    
    # Select top 10 configurations
    top_configs <- all_configs[order(all_configs$SMAPE), ][1:min(10, nrow(all_configs)), ]
    
    cat("Top 10 configurations selected:\n")
    print(top_configs)
    
    # Get holdout data
    holdout_data <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    pred_data <- as.data.frame(holdout_data[, -1])
    period_labels <- c(as.character(73:79), "Q4")
    
    # Generate holdout predictions for top 10 configurations
    pred_matrix_mean <- matrix(NA, nrow = length(actual_values), ncol = nrow(top_configs))
    pred_matrix_median <- matrix(NA, nrow = length(actual_values), ncol = nrow(top_configs))
    weights <- 1 / top_configs$SMAPE  # Inverse SMAPE weighting
    
    # For each top configuration, get holdout predictions
    for(i in 1:nrow(top_configs)) {
      config <- top_configs[i, ]
      
      if(config$Method == "Original") {
        # Use existing original model
        model <- all_modelewall[[dep_var]][[config$Window]]
        pred <- predict(model, newdata = pred_data, burn = 100)
        pred_matrix_mean[, i] <- colMeans(pred$distribution)
        pred_matrix_median[, i] <- apply(pred$distribution, 2, median)
        
      } else {
        # For sensitivity models, we already have the predictions in all_results
        # Find the corresponding predictions from your existing all_results
        found_pred <- FALSE
        
        if(exists("all_results") && dep_var %in% names(all_results)) {
          for(result in all_results[[dep_var]]) {
            if(!is.null(result) && "predictions" %in% names(result)) {
              # This is a simplified approach - you might need to match more precisely
              # based on your specific data structure
              pred_matrix_mean[, i] <- result$predictions
              pred_matrix_median[, i] <- result$predictions  # Assuming same for now
              found_pred <- TRUE
              break
            }
          }
        }
        
        if(!found_pred) {
          cat("Could not find predictions for sensitivity config", i, "\n")
          pred_matrix_mean[, i] <- NA
          pred_matrix_median[, i] <- NA
        }
      }
    }
    
    # Remove failed predictions
    valid_cols <- !is.na(colSums(pred_matrix_mean))
    pred_matrix_mean <- pred_matrix_mean[, valid_cols, drop = FALSE]
    pred_matrix_median <- pred_matrix_median[, valid_cols, drop = FALSE]
    weights <- weights[valid_cols]
    
    if(ncol(pred_matrix_mean) > 0) {
      # Normalize weights
      weights <- weights / sum(weights)
      
      # Calculate weighted ensemble predictions
      ensemble_preds_mean <- rowSums(t(t(pred_matrix_mean) * weights), na.rm = TRUE)
      ensemble_preds_median <- rowSums(t(t(pred_matrix_median) * weights), na.rm = TRUE)
      
      # Calculate ensemble SMAPE
      ensemble_smape_mean <- mean(2 * abs(actual_values - ensemble_preds_mean) / 
                                (abs(actual_values) + abs(ensemble_preds_mean))) * 100
      ensemble_smape_median <- mean(2 * abs(actual_values - ensemble_preds_median) / 
                                  (abs(actual_values) + abs(ensemble_preds_median))) * 100
      
      # Store results
      final_results <- rbind(final_results, 
        data.frame(Variable = dep_var, Prediction_Type = "Mean", Ensemble_SMAPE = ensemble_smape_mean),
        data.frame(Variable = dep_var, Prediction_Type = "Median", Ensemble_SMAPE = ensemble_smape_median))
      
      cat("\nResults for", dep_var, ":\n")
      cat("Mean Ensemble SMAPE:", round(ensemble_smape_mean, 4), "%\n")
      cat("Median Ensemble SMAPE:", round(ensemble_smape_median, 4), "%\n")
    }
  }
  
  return(final_results)
}

# Run the analysis using existing results only
ensemble_results <- combine_existing_results(all_modelewall, sensitivity_results, hold_out_dataset)

# Create the final table you requested
cat("\n================================================================\n")
cat("FINAL RESULTS TABLE\n")
cat("================================================================\n")

# Aggregate across all variables
mean_smape <- mean(ensemble_results[ensemble_results$Prediction_Type == "Mean", "Ensemble_SMAPE"])
median_smape <- mean(ensemble_results[ensemble_results$Prediction_Type == "Median", "Ensemble_SMAPE"])

final_table <- data.frame(
  Prediction_Type = c("Mean", "Median"),
  SMAPE = c(mean_smape, median_smape)
)

print(final_table)

cat("\nDetailed by variable:\n")
print(ensemble_results)
```
## Influence

```{r}
if (!requireNamespace("foreach", quietly = TRUE)) {
  install.packages("foreach")
}
if (!requireNamespace("doParallel", quietly = TRUE)) {
  install.packages("doParallel")
}


# Register parallel backend
cores <- detectCores() - 1  # Leave one core free for system processes
registerDoParallel(cores)
cat("Using", cores, "cores for parallel processing\n")

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to get window data
get_window_data <- function(dep_var, window_size) {
  window_data <- expanding_windows[[dep_var]][[window_size]]$train
  return(window_data)
}

# Function to get the best models based on testing dataset performance (changed from holdout)
get_best_models_test <- function(all_models) {
  best_predictions <- list()
  
  for (dep_var in names(all_models)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:length(all_models[[dep_var]])) {
      model <- all_models[[dep_var]][[i]]
      
      # Get the testing data for this window and variable
      test_data <- expanding_windows[[dep_var]][[i]]$test
      actual_values <- test_data[, 1]
      test_predictors <- as.data.frame(test_data[, -1])
      
      # Make predictions for testing period
      tryCatch({
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        errors_mean <- calculate_errors(actual_values, pred_mean)
        errors_median <- calculate_errors(actual_values, pred_median)
        
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = errors_mean$MAPE, SMAPE = errors_mean$SMAPE),
                       data.frame(Window = i, Method = "median", 
                                MAPE = errors_median$MAPE, SMAPE = errors_median$SMAPE))
      }, error = function(e) {
        cat("Error predicting for", dep_var, "window", i, ":", conditionMessage(e), "\n")
      })
    }
    
    # If we have results, select the best two models based on MAPE
    if (nrow(results) > 0) {
      best_two <- results[order(results$MAPE), ][1:min(2, nrow(results)), ]
      best_predictions[[dep_var]] <- best_two
    } else {
      cat("No valid predictions for", dep_var, "- skipping\n")
    }
  }
  
  # Print best models
  cat("\n=== Best Models for Testing Period ===\n")
  for(dep_var in names(best_predictions)) {
    cat("\n", dep_var, ":\n")
    print(best_predictions[[dep_var]])
  }
  
  return(best_predictions)
}

# FIXED Function to calculate influence measures with LOOIC and LPD
calculate_influence_test <- function(model, train_data, method, dep_var, window_size) {
  # Get original training data
  y <- train_data[, 1]
  X <- train_data[, -1]
  n_obs <- length(y)
  
  # Create model matrix for X
  X_matrix <- model.matrix(~ ., data = as.data.frame(X))[, -1]
  
  # Create time labels starting from 1997 Q1
  start_year <- 1997
  time_labels <- paste0(rep(start_year:(start_year + floor(n_obs/4)), each=4)[1:n_obs], 
                       " Q", rep(1:4, length.out=n_obs))
  
  # Results storage
  results <- data.frame(
    time = time_labels,
    pointwise_lpd = numeric(n_obs),
    delta_looic = numeric(n_obs),
    delta_test_forecast = numeric(n_obs)
  )
  
  # Get test data - FIXED to use passed parameters
  test_data <- expanding_windows[[dep_var]][[window_size]]$test
  test_y <- test_data[, 1]
  test_X <- as.data.frame(test_data[, -1])
  
  # Make original predictions for test period
  original_pred <- predict(model, newdata = test_X, burn = 100)
  original_point_preds <- if(method == "median") {
    apply(original_pred$distribution, 2, median)
  } else {
    colMeans(original_pred$distribution)
  }
  
  # Calculate pointwise log predictive density 
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)  # Using number of samples from log-likelihood
  
  # Ensure we have enough samples (safety check)
  if (n_samples == 0) {
    stop("No log-likelihood values available.")
  }

  log_lik_matrix <- matrix(log_lik, nrow = n_samples, ncol = n_obs, byrow = FALSE)
  
  results$pointwise_lpd <- colMeans(log_lik_matrix, na.rm = TRUE)  # Average LPD
  
  # LOOIC Calculation and test prediction changes - Parallelized version
  looic_values <- rep(NA, n_obs)
  delta_forecasts <- rep(NA, n_obs)
  
  # Parallel loop over observations
  parallel_results <- foreach(i = 1:n_obs, 
                           .packages = c("bsts"),
                           .export = c("test_X", "original_point_preds", "method")) %dopar% {
    # Create leave-one-out dataset
    loo_data <- train_data[-i, ]
    y_loo <- loo_data[, 1]
    X_loo <- loo_data[, -1]
    
    # Fit model on leave-one-out data with reduced iterations for speed
    ss <- AddSeasonal(list(), y_loo, nseasons = 4, season.duration = 1)
    ss <- AddLocalLinearTrend(ss, y_loo)
    
    model_loo <- tryCatch({
      bsts(y_loo,
           X = X_loo,
           state.specification = ss,
           niter = 10000,  # Reduced from 10000 for speed
           ping = 0)
    }, 
    error = function(e) {
      return(NULL)  # Return NULL if fitting fails
    })
    
    if (is.null(model_loo)) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    # Calculate LOOIC for leave-out model
    log_lik_loo <- model_loo$log.likelihood
    if (length(log_lik_loo) == 0) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    loo_liks <- numeric(length(log_lik_loo))
    for (j in 1:length(log_lik_loo)) {
      loo_liks[j] <- mean(log_lik_loo[-j], na.rm = TRUE)
    }
    
    looic <- -2 * mean(loo_liks, na.rm = TRUE)
    
    # Calculate change in test forecasts
    # Predict for test period using the leave-one-out model
    pred_loo <- predict(model_loo, newdata = test_X, burn = 100)
    
    point_pred_loo <- if(method == "median") {
      apply(pred_loo$distribution, 2, median)
    } else {
      colMeans(pred_loo$distribution)
    }
    
    # Calculate average change across all test observations
    delta_forecast <- mean(point_pred_loo - original_point_preds)
    
    return(list(looic = looic, delta_forecast = delta_forecast))
  }
  
  # Collect the parallel results
  for (i in 1:n_obs) {
    if (!is.null(parallel_results[[i]])) {
      looic_values[i] <- parallel_results[[i]]$looic
      delta_forecasts[i] <- parallel_results[[i]]$delta_forecast
    }
  }
  
  # Calculate original LOOIC
  looic_original <- -2 * mean(colMeans(log_lik_matrix, na.rm = TRUE))
  
  # Assign results
  results$delta_looic <- looic_values - looic_original  # Calculate delta LOOIC
  results$delta_test_forecast <- delta_forecasts
  
  return(results)
}

# FIXED Function to analyze the top models with test data
analyze_top_models_parallel <- function(best_models, all_models) {
  all_influence_results <- list()
  
  for(dep_var in names(best_models)) {
    cat("\n\nAnalyzing dependent variable:", dep_var)
    dep_influence <- list()
    models <- best_models[[dep_var]]
    
    for(i in 1:nrow(models)) {
      cat("\n\nProcessing model", i, "for", dep_var)
      model_info <- models[i, ]
      
      # Get training data for this window
      window_data <- get_window_data(dep_var, model_info$Window)
      
      # Get the original model 
      original_model <- all_models[[dep_var]][[model_info$Window]]
      
      # Calculate influence measures - FIXED to pass dep_var and window_size
      influence_results <- calculate_influence_test(
        original_model, 
        window_data,
        model_info$Method,
        dep_var,
        model_info$Window
      )
      
      # Add model identifier
      identifier <- paste("Window", model_info$Window, "-", model_info$Method)
      dep_influence[[identifier]] <- influence_results
    }
    
    all_influence_results[[dep_var]] <- dep_influence
  }
  
  return(all_influence_results)
}

# Function to generate comprehensive results tables
generate_influence_summary <- function(influence_results) {
  for(dep_var in names(influence_results)) {
    cat("\n=== Results for", dep_var, "===\n")
    
    for(model_name in names(influence_results[[dep_var]])) {
      cat("\n--- Model:", model_name, "---\n")
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # Round numeric columns to 4 decimal places
      results_df[, 2:4] <- round(results_df[, 2:4], 4)
      
      # Sort by absolute delta_test_forecast to find most influential observations
      sorted_df <- results_df[order(abs(results_df$delta_test_forecast), decreasing = TRUE), ]
      
      cat("Top 25 most influential observations:\n")
      print(head(sorted_df, 25))
      
      cat("\nSummary statistics:\n")
      
      # Calculate summary statistics separately
      lpd_summary <- summary(results_df$pointwise_lpd)
      looic_summary <- summary(results_df$delta_looic)
      forecast_summary <- summary(results_df$delta_test_forecast)
      
      # Print each metric separately to avoid data frame mixing issues
      cat("\nPointwise LPD:\n")
      print(round(lpd_summary, 4))
      cat("Std Dev:", round(sd(results_df$pointwise_lpd, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta LOOIC:\n")
      print(round(looic_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_looic, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta Test Forecast:\n")
      print(round(forecast_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_test_forecast, na.rm = TRUE), 4), "\n")
    }
  }
}

# Run the analysis with parallel processing
run_influence_analysis_parallel <- function() {
  # Start timing
  start_time <- Sys.time()
  cat("Starting parallel influence analysis at:", format(start_time), "\n")
  
  # First get the best models based on test data performance
  best_models <- get_best_models_test(all_modelewall)
  
  # Run the influence analysis in parallel
  influence_results <- analyze_top_models_parallel(best_models, all_modelewall)
  
  # Generate summary tables
  generate_influence_summary(influence_results)
  
  # End timing
  end_time <- Sys.time()
  execution_time <- end_time - start_time
  cat("\nInfluence analysis completed in:", format(execution_time), "\n")
  
  return(influence_results)
}

# Run the analysis
influence_results_parallel <- run_influence_analysis_parallel()

# Clean up the parallel backend when done
stopImplicitCluster()
```
# Excel
```{r}
# Modified create_enhanced_prediction_tables function - ONLY ENSEMBLE ROWS
create_enhanced_prediction_tables_revised <- function(all_evaluations, all_models, extended_test_sets, 
                                                      crps_scores, lpd_scores, file_path, 
                                                      aggregate_smape_results,
                                                      ensemble_results) {  # Changed parameter name
  dep_vars <- names(all_evaluations)
  wb <- createWorkbook()
  
  for(dep_var in dep_vars) {
    addWorksheet(wb, dep_var)
    
    # Get ensemble SMAPEs from the new ensemble analysis
    ensemble_mean_smape <- NA
    ensemble_median_smape <- NA
    
    if(!is.null(ensemble_results) && nrow(ensemble_results) > 0) {
      # Filter for this variable
      var_results <- ensemble_results[ensemble_results$Variable == dep_var, ]
      if(nrow(var_results) > 0) {
        mean_row <- var_results[var_results$Prediction_Type == "Mean", ]
        median_row <- var_results[var_results$Prediction_Type == "Median", ]
        
        if(nrow(mean_row) > 0) ensemble_mean_smape <- round(mean_row$Ensemble_SMAPE, 2)
        if(nrow(median_row) > 0) ensemble_median_smape <- round(median_row$Ensemble_SMAPE, 2)
      }
    }
    
    # ONLY ENSEMBLE ROWS - removed Use Mean and Use Median
    variable_data <- data.frame(
      Col1 = c(
        dep_var,
        "Holds out period",
        "Prediction based on testing SMAPE",
        "Ensemble Mean (Top 10)",      # Only these two rows
        "Ensemble Median (Top 10)"     
      ),
      Col2 = c(
        "",
        "2023Q1 - 2024Q4",
        "SMAPE",
        ensemble_mean_smape,           # Only these two values
        ensemble_median_smape          
      ),
      stringsAsFactors = FALSE
    )
    
    writeData(wb, dep_var, variable_data, startRow = 1, startCol = 1, colNames = FALSE)
    setColWidths(wb, dep_var, cols = 1:2, widths = c(25, 20))
    
    # Rest of the function remains the same...
    start_row <- 7  # Changed back to 7 since we removed 2 rows
    
    smape_mean   <- sapply(all_evaluations[[dep_var]], function(x) x$errors_mean$SMAPE)
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) x$errors_median$SMAPE)
    
    crps_mean <- numeric(7)
    crps_sd   <- numeric(7)
    for(i in 1:7) {
      window_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        crps_mean[i] <- window_data$CRPS_Mean
        crps_sd[i]   <- window_data$CRPS_SD
      } else {
        crps_mean[i] <- NA
        crps_sd[i]   <- NA
      }
    }
    
    lpd_mean <- numeric(7)
    lpd_sd   <- numeric(7)
    for(i in 1:7) {
      window_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        lpd_mean[i] <- window_data$LPD_Mean
        lpd_sd[i]   <- window_data$LPD_SD
      } else {
        lpd_mean[i] <- NA
        lpd_sd[i]   <- NA
      }
    }
    
    top_5_mean   <- order(smape_mean)[1:5]
    top_5_median <- order(smape_median)[1:5]
    
    writeData(wb, dep_var, "Predictive interval for Top 5 models based on sMAPE", 
              startRow = start_row, startCol = 1)
    
    writeData(wb, dep_var, "Use Mean", startRow = start_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = start_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = start_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = start_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = start_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = start_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = start_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = start_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = start_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = start_row + 2, startCol = 9)
    
    current_row <- start_row + 3
    for(window in top_5_mean) {
      pred_data <- extended_test_sets[[dep_var]][[window]]
      last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(last_row_predictors),
                           burn = 100)
      dist <- pred$distribution[,1]
      
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
      writeData(wb, dep_var, mean(dist),            startRow = current_row, startCol = 3)
      writeData(wb, dep_var, median(dist),          startRow = current_row, startCol = 4)
      writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
      
      writeData(wb, dep_var, smape_mean[window],  startRow = current_row, startCol = 6)
      writeData(wb, dep_var, crps_mean[window],   startRow = current_row, startCol = 7)
      writeData(wb, dep_var, lpd_mean[window],    startRow = current_row, startCol = 8)
      writeData(wb, dep_var, lpd_sd[window],      startRow = current_row, startCol = 9)
      
      current_row <- current_row + 1
    }
    
    writeData(wb, dep_var, "Use Median", startRow = current_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = current_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = current_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = current_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = current_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = current_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = current_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = current_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = current_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = current_row + 2, startCol = 9)
    
    current_row <- current_row + 3
    for(window in top_5_median) {
      pred_data <- extended_test_sets[[dep_var]][[window]]
      last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(last_row_predictors),
                           burn = 100)
      dist <- pred$distribution[,1]
      
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
      writeData(wb, dep_var, mean(dist),            startRow = current_row, startCol = 3)
      writeData(wb, dep_var, median(dist),          startRow = current_row, startCol = 4)
      writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
      
      writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
      writeData(wb, dep_var, crps_mean[window],    startRow = current_row, startCol = 7)
      writeData(wb, dep_var, lpd_mean[window],     startRow = current_row, startCol = 8)
      writeData(wb, dep_var, lpd_sd[window],       startRow = current_row, startCol = 9)
      
      current_row <- current_row + 1
    }
    
    setColWidths(wb, dep_var, cols = 1, width = 50)
    setColWidths(wb, dep_var, cols = 2:9, width = 15)
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}


###############################################################################
## 3) add_sensitivity_analysis
##    Moves the sensitivity analysis block down by 3 rows and removes any extra row (e.g. SMAPE row)
###############################################################################
add_sensitivity_analysis <- function(sensitivity_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for(dep_var in names(sensitivity_results)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    
    # Calculate the exact row to place the sensitivity analysis
    # Find the last non-empty row of the existing content
    last_content_row <- max(which(!is.na(sheet_data[,1]) & sheet_data[,1] != ""), na.rm = TRUE)
    
    # Add a small gap (3 rows) after the last content
    last_row <- last_content_row + 6
    
    # Create the results data frame
    results_data <- data.frame(
      Window   = integer(),
      Method   = character(),
      Sigma    = numeric(),
      Slab_Var = numeric(),
      WAIC     = numeric(),
      LOOIC    = numeric(),
      MAPE     = numeric(),
      SMAPE    = numeric(),
      stringsAsFactors = FALSE
    )
    
    for (model_name in names(sensitivity_results[[dep_var]])) {
      model_info <- sensitivity_results[[dep_var]][[model_name]]
      window <- model_info$window
      method <- model_info$method
      
      for (result_name in names(model_info$results)) {
        result <- model_info$results[[result_name]]
        results_data <- rbind(results_data, data.frame(
          Window   = window,
          Method   = method,
          Sigma    = if(!is.null(result$sigma)) result$sigma else NA,
          Slab_Var = if(!is.null(result$slab_var)) result$slab_var else NA,
          WAIC     = if(!is.null(result$waic)) result$waic else NA,
          LOOIC    = if(!is.null(result$looic)) result$looic else NA,
          MAPE     = if(!is.null(result$mape)) result$mape else NA,
          SMAPE    = if(!is.null(result$smape)) result$smape else NA,
          stringsAsFactors = FALSE
        ))
      }
    }
    
    # Write the title directly (no mysterious dep var title)
    writeData(wb, dep_var, "Sensitivity Analysis Results", startRow = last_row, startCol = 1)
    
    # Immediately write the table
    writeData(wb, dep_var, results_data, startRow = last_row + 1, startCol = 1, colNames = TRUE)
    
    # Set column widths
    setColWidths(wb, dep_var, cols = 1:8, widths = c(10,10,10,10,12,12,10,10))
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## Fixed add_influence_analysis_to_workbook function
## Fixes the spacing issue and handles delta_test_forecast/delta_holdout_forecast
###############################################################################
add_influence_analysis_to_workbook <- function(influence_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for (dep_var in names(influence_results)) {
    if (!dep_var %in% names(wb)) {
      addWorksheet(wb, dep_var)
    }
    
    sheet_data <- tryCatch({
      readWorkbook(wb, sheet = dep_var)
    }, error = function(e) {
      data.frame()
    })
    
    # Calculate exact start row by finding the last sensitivity analysis row
    # Find the last non-empty row in the sheet
    non_empty_rows <- which(!is.na(sheet_data[,1]) & sheet_data[,1] != "")
    
    if (length(non_empty_rows) > 0) {
      last_content_row <- max(non_empty_rows, na.rm = TRUE)
      # Look for "Sensitivity Analysis Results" in the sheet
      sensitivity_rows <- which(sheet_data[,1] == "Sensitivity Analysis Results")
      
      if (length(sensitivity_rows) > 0) {
        sensitivity_row <- max(sensitivity_rows)
        # Find the last row of the sensitivity table
        # Usually it's a block of data after the title
        for (i in (sensitivity_row + 1):nrow(sheet_data)) {
          if (is.na(sheet_data[i,1]) || sheet_data[i,1] == "") {
            last_sensitivity_row <- i - 1
            break
          }
          # If we reach the end of the dataframe
          if (i == nrow(sheet_data)) {
            last_sensitivity_row <- i
          }
        }
        # Start influence analysis 5 rows after the sensitivity table
        current_row <- last_sensitivity_row + 9
      } else {
        # If sensitivity analysis not found, start 5 rows after last content
        current_row <- last_content_row + 9
      }
    } else {
      # If sheet is empty
      current_row <- 1
    }
    
    processed_models <- c()
    
    for (model_name in names(influence_results[[dep_var]])) {
      if (model_name %in% processed_models) next
      processed_models <- c(processed_models, model_name)
      
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # CRITICAL FIX: Map delta_test_forecast to delta_holdout_forecast if needed
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        results_df$delta_holdout_forecast <- results_df$delta_test_forecast
      }
      
      # Process delta_looic if present
      if ("delta_looic" %in% names(results_df)) {
        results_df$delta_looic <- as.numeric(as.character(results_df$delta_looic))
      } else {
        results_df$delta_looic <- rep(NA, nrow(results_df))
      }
      
      # Round numeric columns to 4 decimal places
      numeric_cols <- c("pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      for (col in numeric_cols) {
        if (col %in% names(results_df) && is.numeric(results_df[[col]])) {
          results_df[[col]] <- round(results_df[[col]], 4)
        }
      }
      
      # Sort by absolute delta_holdout_forecast
      if (sum(!is.na(results_df$delta_holdout_forecast)) > 0) {
        sorted_df <- results_df[order(abs(results_df$delta_holdout_forecast), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df <- results_df
      }
      top_25_forecast <- head(sorted_df, 25)
      
      # Write the title
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta Holdout Forecast - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      # Get the correct column names
      if (all(c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      } else if (all(c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast")
        colnames(top_25_forecast)[colnames(top_25_forecast) == "delta_test_forecast"] <- "delta_holdout_forecast"
      } else {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      }
      
      # Make sure we only write columns that actually exist
      write_cols <- intersect(write_cols, colnames(top_25_forecast))
      
      # Write the data
      writeData(wb, dep_var, top_25_forecast[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                 createStyle(numFmt = "0.0000"), 
                 rows = (current_row + 3):(current_row + 3 + nrow(top_25_forecast)), 
                 cols = col_idx)
      }
      
      # Proceed to next section - 5 rows after this table
      current_row <- current_row + nrow(top_25_forecast) + 5
      
      # Sort by absolute delta_looic
      if (sum(!is.na(results_df$delta_looic)) > 0) {
        sorted_df_looic <- results_df[order(abs(results_df$delta_looic), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df_looic <- results_df
      }
      top_25_looic <- head(sorted_df_looic, 25)
      
      # Write looic section
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta LOOIC - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      writeData(wb, dep_var, top_25_looic[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                 createStyle(numFmt = "0.0000"), 
                 rows = (current_row + 3):(current_row + 3 + nrow(top_25_looic)), 
                 cols = col_idx)
      }
      
      # Proceed to next model section - 5 rows after this table
      current_row <- current_row + nrow(top_25_looic) + 5
    }
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  cat("Influence analysis results added to workbook:", file_path, "\n")
}

###############################################################################
## 5) add_diagnostics_to_workbook (unchanged)
###############################################################################
add_diagnostics_to_workbook <- function(all_models, expanding_windows, all_diagnostics, all_looic, 
                                        bayesian_R2_results, crps_scores, lpd_scores, 
                                        ljung_box_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  suppressWarnings({
    if (!requireNamespace("coda", quietly = TRUE)) {
      install.packages("coda")
    }
    library(coda)
  })
  
  for(dep_var in names(all_models)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    current_row <- nrow(sheet_data) + 22
    
    for(window in 1:length(all_models[[dep_var]])) {
      if (is.null(all_models[[dep_var]][[window]])) next
      
      train_size <- if (!is.null(expanding_windows[[dep_var]]) && 
                        !is.null(expanding_windows[[dep_var]][[window]]) && 
                        !is.null(expanding_windows[[dep_var]][[window]]$train)) {
        nrow(expanding_windows[[dep_var]][[window]]$train)
      } else {
        NA
      }
      
      test_size <- if (!is.null(expanding_windows[[dep_var]]) && 
                       !is.null(expanding_windows[[dep_var]][[window]]) && 
                       !is.null(expanding_windows[[dep_var]][[window]]$test)) {
        nrow(expanding_windows[[dep_var]][[window]]$test)
      } else {
        NA
      }
      
      n_predictors <- if (!is.null(expanding_windows[[dep_var]]) && 
                          !is.null(expanding_windows[[dep_var]][[window]]) && 
                          !is.null(expanding_windows[[dep_var]][[window]]$train)) {
        ncol(expanding_windows[[dep_var]][[window]]$train) - 1
      } else {
        NA
      }
      
      dic_value <- if (!is.null(all_diagnostics) && 
                       !is.null(all_diagnostics[[dep_var]]) && 
                       !is.null(all_diagnostics[[dep_var]]$DIC) &&
                       length(all_diagnostics[[dep_var]]$DIC) >= window) {
        all_diagnostics[[dep_var]]$DIC[window]
      } else {
        NA
      }
      
      waic_value <- if (!is.null(all_diagnostics) && 
                        !is.null(all_diagnostics[[dep_var]]) && 
                        !is.null(all_diagnostics[[dep_var]]$WAIC) &&
                        length(all_diagnostics[[dep_var]]$WAIC) >= window) {
        all_diagnostics[[dep_var]]$WAIC[window]
      } else {
        NA
      }
      
      looic_value <- if (!is.null(all_looic) && 
                         !is.null(all_looic[[dep_var]]) && 
                         length(all_looic[[dep_var]]) >= window) {
        all_looic[[dep_var]][window]
      } else {
        NA
      }
      
      r2_value <- if (!is.null(bayesian_R2_results) && 
                      !is.null(bayesian_R2_results[[dep_var]]) && 
                      !is.null(bayesian_R2_results[[dep_var]]$Mean_R2) &&
                      length(bayesian_R2_results[[dep_var]]$Mean_R2) >= window) {
        bayesian_R2_results[[dep_var]]$Mean_R2[window]
      } else {
        NA
      }
      
      crps_value <- if (!is.null(crps_scores) && 
                        !is.null(crps_scores[[dep_var]]) && 
                        any(crps_scores[[dep_var]]$Window == window)) {
        crps_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == window, ]
        if (nrow(crps_data) > 0 && !is.na(crps_data$CRPS_Mean) && !is.na(crps_data$CRPS_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", crps_data$CRPS_Mean, crps_data$CRPS_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lpd_value <- if (!is.null(lpd_scores) && 
                       !is.null(lpd_scores[[dep_var]]) && 
                       any(lpd_scores[[dep_var]]$Window == window)) {
        lpd_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == window, ]
        if (nrow(lpd_data) > 0 && !is.na(lpd_data$LPD_Mean) && !is.na(lpd_data$LPD_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", lpd_data$LPD_Mean, lpd_data$LPD_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lb_stat <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_Statistic) &&
                     length(ljung_box_results[[dep_var]]$LB_Statistic) >= window) {
        ljung_box_results[[dep_var]]$LB_Statistic[window]
      } else {
        NA
      }
      
      lb_pval <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_pvalue) &&
                     length(ljung_box_results[[dep_var]]$LB_pvalue) >= window) {
        ljung_box_results[[dep_var]]$LB_pvalue[window]
      } else {
        NA
      }
      
      metrics <- data.frame(
        Metric = c(
          "Training Sample Size",
          "Test Sample Size",
          "Number of Predictors",
          "DIC/number of training period",
          "WAIC/number of training period",
          "LOOIC",
          "Bayesian R²",
          "Number of iterations used in BSTS",
          "CRPS",
          "LPD",
          "LB_Statistic",
          "LB_pvalue"
        ),
        Value = c(
          train_size,
          test_size,
          n_predictors,
          dic_value,
          waic_value,
          looic_value,
          r2_value,
          10000,
          crps_value,
          lpd_value,
          lb_stat,
          lb_pval
        )
      )
      
      writeData(wb, dep_var, paste("Window", window), startRow = current_row)
      current_row <- current_row + 2
      writeData(wb, dep_var, metrics, startRow = current_row, startCol = 1, colNames = FALSE)
      current_row <- current_row + nrow(metrics) + 2
      
      tryCatch({
        coef_stats <- data.frame(
          Parameter     = character(),
          Mean          = numeric(),
          Median        = numeric(),
          CI_2.5        = numeric(),
          CI_97.5       = numeric(),
          ESS           = numeric(),
          Post_Prob     = numeric(),
          Geweke_Pval   = numeric(),
          HW_Pval       = numeric(),
          RL_Autocorr   = numeric(),
          RL_Iterations = numeric(),
          RL_Burnin     = numeric(),
          RL_Dependence = numeric(),
          stringsAsFactors = FALSE
        )
        
        model <- all_models[[dep_var]][[window]]
        if (!is.null(model) && !is.null(model$coefficients)) {
          coef_matrix <- as.matrix(model$coefficients)
          
          for(j in 1:ncol(coef_matrix)) {
            param_name <- colnames(coef_matrix)[j]
            chain <- as.vector(coef_matrix[,j])
            if (length(chain) < 2) next
            
            ci <- tryCatch({
              quantile(chain, probs = c(0.025, 0.975))
            }, error = function(e) c(NA, NA))
            
            mean_val   <- tryCatch(mean(chain), error = function(e) NA)
            median_val <- tryCatch(median(chain), error = function(e) NA)
            ess_val    <- tryCatch(as.numeric(effectiveSize(mcmc(chain))), error = function(e) NA)
            pip        <- tryCatch(mean(chain != 0), error = function(e) NA)
            
            geweke_pval <- tryCatch({
              geweke <- geweke.diag(mcmc(chain))
              2 * pnorm(-abs(geweke$z))
            }, error = function(e) NA)
            
            hw_pval <- tryCatch({
              hw <- heidel.diag(mcmc(chain))
              if (is.null(hw)) NA else hw[1, "pvalue"]
            }, error = function(e) NA)
            
            rl_stats <- tryCatch({
              if (length(unique(chain)) >= 10) {
                binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
                rl <- raftery.diag(binary_chain)
                c(cor(chain[-1], chain[-length(chain)]),
                  rl$resmatrix[1,"N"],
                  rl$resmatrix[1,"M"],
                  rl$resmatrix[1,"I"])
              } else {
                rep(NA, 4)
              }
            }, error = function(e) rep(NA, 4))
            
            coef_stats <- rbind(coef_stats, data.frame(
              Parameter     = param_name,
              Mean          = round(mean_val, 4),
              Median        = round(median_val, 4),
              CI_2.5        = round(ci[1], 4),
              CI_97.5       = round(ci[2], 4),
              ESS           = round(ess_val, 0),
              Post_Prob     = round(pip, 4),
              Geweke_Pval   = round(geweke_pval, 4),
              HW_Pval       = round(hw_pval, 4),
              RL_Autocorr   = round(rl_stats[1], 4),
              RL_Iterations = round(rl_stats[2], 0),
              RL_Burnin     = round(rl_stats[3], 0),
              RL_Dependence = round(rl_stats[4], 2),
              stringsAsFactors=FALSE
            ))
          }
        }
        
        if (nrow(coef_stats) > 0) {
          writeData(wb, dep_var, coef_stats, startRow = current_row)
          current_row <- current_row + nrow(coef_stats) + 3
        } else {
          writeData(wb, dep_var, "No coefficient data available", startRow = current_row)
          current_row <- current_row + 4
        }
        
      }, error = function(e) {
        writeData(wb, dep_var, paste("Error processing coefficients:", e$message), startRow = current_row)
        current_row <- current_row + 4
      })
    }
    
    setColWidths(wb, dep_var, cols = 1:13, widths = "auto")
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}


###############################################################################
## 6) format_window_range (unchanged)
###############################################################################
format_window_range <- function(window) {
  train_ranges <- c(
    "1-70", 
    "1-75",
    "1-80",
    "1-85",
    "1-90"
  )
  
  test_ranges <- c(
    "71-105",
    "76-105",
    "81-105",
    "86-105",
    "91-105"
  )
  
  paste(train_ranges[window], "as the training and the", test_ranges[window], "as the testing")
}


###############################################################################
## 7) create_excel_results_revised - MAIN FUNCTION
###############################################################################
create_excel_results_revised <- function(
    all_evaluations, 
    all_models, 
    extended_test_sets, 
    crps_scores, 
    lpd_scores, 
    sensitivity_results, 
    influence_results, 
    expanding_windows, 
    all_diagnostics, 
    all_looic, 
    bayesian_R2_results, 
    ljung_box_results,
    aggregate_smape_results,
    ensemble_results         # This should be the output from ensemble analysis
) {
  file_path <- "EW_ALL_2023Q1.xlsx"
  
  # Handle influence results if they exist
  if (!is.null(influence_results)) {
    for (dep_var in names(influence_results)) {
      for (model_name in names(influence_results[[dep_var]])) {
        results_df <- influence_results[[dep_var]][[model_name]]
        if ("delta_test_forecast" %in% names(results_df) && 
            !"delta_holdout_forecast" %in% names(results_df)) {
          influence_results[[dep_var]][[model_name]]$delta_holdout_forecast <- 
            results_df$delta_test_forecast
        }
      }
    }
  }
  
  # Create prediction tables with ensemble results ONLY
  cat("Creating prediction tables...\n")
  wb <- create_enhanced_prediction_tables_revised(  # Use revised function
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets = extended_test_sets, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_results = ensemble_results  # Pass ensemble results
  )
  
  # Add sensitivity analysis with proper spacing
  cat("Adding sensitivity analysis...\n")
  add_sensitivity_analysis(sensitivity_results, file_path)
  
  # Add influence analysis with proper spacing (only if data exists)
  if (!is.null(influence_results)) {
    cat("Adding influence analysis...\n")
    add_influence_analysis_to_workbook(influence_results, file_path)
  }
  
  # Add diagnostics
  cat("Adding diagnostics...\n")
  add_diagnostics_to_workbook(
    all_models = all_models, 
    expanding_windows = expanding_windows, 
    all_diagnostics = all_diagnostics, 
    all_looic = all_looic,
    bayesian_R2_results = bayesian_R2_results, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    ljung_box_results = ljung_box_results,
    file_path = file_path
  )
  
  return(paste("Results successfully written to", file_path))
}

###############################################################################
## 8) USAGE - Call this with your existing ensemble_results
###############################################################################
result <- create_excel_results_revised(
  all_evaluations = all_evaluations_allew, 
  all_models = all_modelewall, 
  extended_test_sets = extended_test_sets, 
  crps_scores = crps_scores, 
  lpd_scores = lpd_scores, 
  sensitivity_results = sensitivity_results, 
  influence_results = influence_results_parallel,  
  expanding_windows = expanding_windows, 
  all_diagnostics = all_diagnostics, 
  all_looic = all_looic_ew, 
  bayesian_R2_results = bayesian_R2_results,
  ljung_box_results = ljung_box_results,
  aggregate_smape_results = aggregate_smape_results,
  ensemble_results = ensemble_results  # Use your existing ensemble_results
)

print(result)
```



# BackW
```{r, results='hide', message=FALSE, warning=FALSE}
# Function to calculate LOOIC according to the specified method
calculate_looic <- function(model, train_data) {
  n_train <- nrow(train_data)
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  looic <- (-2 * sum(loo_liks)) / n_train
  return(looic)
}

# Backward selection function - SEQUENTIAL VERSION (no parallel processing)
backward_selection <- function(train_data) {
  set.seed(1234)
  y <- train_data[, 1]
  X <- train_data[, -1]
  all_predictors <- colnames(X)
  removed_vars <- c() # Track removed variables
  removed_indices <- c() # Track removed column indices
  
  # Set up state space components
  
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y)
  
  # Create mapping between predictors and their original column indices
  predictor_indices <- setNames(seq_along(all_predictors) + 2, all_predictors)  # +2 for the y and date columns
  
  current_predictors <- all_predictors
  
  fit_model <- function(predictors) {
    set.seed(1234)
    if (length(predictors) == 0) {
      model_data <- data.frame(y = y)
      model <- bsts(y ~ 1,
                    state.specification = ss,
                    niter = 10000,
                    data = model_data)
    } else {
      formula_str <- paste("y ~", paste(predictors, collapse = " + "))
      formula <- as.formula(formula_str)
      model_data <- as.data.frame(cbind(y = y, X[, predictors, drop = FALSE]))
      model <- bsts(formula,
                    state.specification = ss,
                    niter = 10000,
                    data = model_data)
    }
    return(model)
  }
  
  # Start with the full model
  cat("Fitting full model with", length(current_predictors), "predictors\n")
  full_model <- fit_model(current_predictors)
  current_looic <- calculate_looic(full_model, train_data)
  cat("Full model LOOIC:", current_looic, "\n")
  
  best_model <- full_model
  best_predictors <- current_predictors
  iteration <- 1
  
  # Define the threshold for LOOIC improvement (0.004%)
  looic_threshold <- current_looic * 0.00004
  
  while (length(current_predictors) > 1) {
    cat("\nIteration", iteration, "- Current predictors:", length(current_predictors), "\n")
    
    # Track the best improvement in this round
    best_looic_improvement <- 0
    var_to_remove <- NULL
    temp_best_model <- NULL
    
    # Test removing each predictor SEQUENTIALLY (no parallel)
    for (i in 1:length(current_predictors)) {
      var <- current_predictors[i]
      candidate_predictors <- current_predictors[-i]
      
      # Using tryCatch to handle errors
      result <- tryCatch({
        # Fit model without this predictor
        candidate_model <- fit_model(candidate_predictors)
        candidate_looic <- calculate_looic(candidate_model, train_data)
        
        # Calculate the LOOIC change
        looic_change <- current_looic - candidate_looic
        
        cat("  Testing removal of", var, "- LOOIC:", candidate_looic, 
            "(change:", looic_change, ")\n")
        
        # Return results
        list(
          var = var,
          var_index = i,
          looic = candidate_looic,
          looic_change = looic_change,
          model = candidate_model,
          error = NULL
        )
      }, error = function(e) {
        cat("    Error fitting model without", var, ":", e$message, "\n")
        # Return error info
        list(
          var = var,
          var_index = i,
          looic = NULL,
          looic_change = NULL,
          model = NULL,
          error = e$message
        )
      })
      
      # If no error and this removal gives better improvement
      if (is.null(result$error) && result$looic_change > best_looic_improvement) {
        best_looic_improvement <- result$looic_change
        var_to_remove <- i
        temp_best_model <- result$model
        cat("    Best improvement so far!\n")
      }
    }
    
    # Check if the best improvement exceeds the threshold
    if (!is.null(var_to_remove) && best_looic_improvement > looic_threshold) {
      removed_var <- current_predictors[var_to_remove]
      removed_vars <- c(removed_vars, removed_var) # Add to removed variables list
      removed_indices <- c(removed_indices, predictor_indices[removed_var]) # Add column index
      
      cat("Removing variable:", removed_var, "(column index:", predictor_indices[removed_var], ")\n")
      cat("LOOIC improved from", current_looic, "to", current_looic - best_looic_improvement, "\n")
      
      best_model <- temp_best_model
      current_looic <- current_looic - best_looic_improvement
      best_predictors <- current_predictors[-var_to_remove]
      current_predictors <- best_predictors
    } else {
      cat("No significant LOOIC improvement found (threshold:", looic_threshold, ")\n")
      break # Stop if no meaningful improvement
    }
    
    iteration <- iteration + 1
  }
  
  cat("\nBackward selection complete\n")
  cat("Selected", length(best_predictors), "predictors\n")
  
  return(list(
    model = best_model,
    selected_vars = best_predictors,
    looic = current_looic,
    removed_vars = removed_vars, # Include removed variables in the output
    removed_indices = removed_indices # Include removed column indices in the output
  ))
}

# Function to find best models based on evaluations 
# Find best models based on sMAPE from existing evaluation results
find_best_models <- function(all_evaluations) {
  best_models <- list()  # To store the best models for each dep var
  
  for(var_name in names(all_evaluations)) {
    cat("\nFor", var_name, ":\n")
    
    # Collect all sMAPE values
    all_smape_results <- data.frame(
      Window = integer(),
      Method = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Collect mean-based sMAPE values
    for(i in 1:5) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      all_smape_results <- rbind(all_smape_results, data.frame(
        Window = i,
        Method = "mean",
        SMAPE = window_errors$SMAPE,
        stringsAsFactors = FALSE
      ))
    }
    
    # Collect median-based sMAPE values
    for(i in 1:5) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      all_smape_results <- rbind(all_smape_results, data.frame(
        Window = i,
        Method = "median",
        SMAPE = window_errors$SMAPE,
        stringsAsFactors = FALSE
      ))
    }
    
    # Sort by sMAPE (ascending) to find the best model overall
    all_smape_results <- all_smape_results[order(all_smape_results$SMAPE), ]
    best_model <- all_smape_results[1, ]
    
    cat("Best model for", var_name, "based on sMAPE:\n")
    cat("Window:", best_model$Window, "using", best_model$Method, 
        "method with sMAPE =", round(best_model$SMAPE, 4), "\n")
    
    # Store the best model info
    best_models[[var_name]] <- list(
      best_window = best_model$Window,
      best_method = best_model$Method,
      best_smape = best_model$SMAPE
    )
  }
  
  return(best_models)
}

# Sequential version of run_best_selection (no parallel processing)
run_best_selection <- function(best_models, expanding_windows) {
  all_models <- list()
  
  # Process each dependent variable sequentially
  for(dep_var_name in names(best_models)) {
    cat("\n\n==== Variable Selection for", dep_var_name, "====\n")
    
    # Use the best window determined by sMAPE
    best_window <- best_models[[dep_var_name]]$best_window
    best_method <- best_models[[dep_var_name]]$best_method
    best_smape <- best_models[[dep_var_name]]$best_smape
    
    cat("\n--- Using best model: Window", best_window, "with", best_method, 
        "method (sMAPE =", round(best_smape, 4), ") for", dep_var_name, "---\n")
    
    # Retrieve training data for the best window
    train_data <- expanding_windows[[dep_var_name]][[best_window]]$train
    
    # Run backward selection on the training data
    selection_result <- backward_selection(train_data)
    
    # Store the result
    all_models[[dep_var_name]] <- selection_result
    
    # Print information about the selection
    cat("\nSelected variables for", dep_var_name, ":", 
        paste(selection_result$selected_vars, collapse=", "), "\n")
    
    # Print removed variables and their column indices
    cat("\nRemoved variables from original dataset for", dep_var_name, ":\n")
    for(i in 1:length(selection_result$removed_vars)) {
      cat(selection_result$removed_vars[i], "(Column", 
          selection_result$removed_indices[i], ")\n")
    }
    
    # Store removed variables in a dataframe or table if needed
    if (dep_var_name == "senate") {
      cat("\n=== REMOVED COLUMN INDICES IN SENATE ORIGINAL DATASET ===\n")
      cat(paste(selection_result$removed_indices, collapse="\n"), "\n")
      cat("=================================================\n")
    }
  }
  
  return(all_models)
}

# Wrapper function with error handling
run_backward_selection_wrapper <- function(all_evaluations_bwew, expanding_windows) {
  tryCatch({
    # Find the best models
    cat("Finding best models based on sMAPE...\n")
    best_models_EWBW <- find_best_models(all_evaluations_bwew)
    
    # Run the selection process (sequentially)
    cat("Running backward selection for best models...\n")
    final_models_EWBW <- run_best_selection(best_models_EWBW, expanding_windows)
    
    # Force garbage collection
    gc()
    
    return(final_models_EWBW)
  }, error = function(e) {
    cat("Error in backward selection process:", e$message, "\n")
    
    # Force garbage collection
    gc()
    
    return(NULL)
  })
}

# Usage:
 final_models_EWBW <- run_backward_selection_wrapper(all_evaluations_allew, expanding_windows)
```



```{r}
# Save removed variables to an Excel workbook
library(openxlsx)

# Create a new workbook
wb <- createWorkbook()

# Loop through each model and write the removed variables to a sheet
for(dep_var_name in names(final_models_EWBW)) {
  removed_vars <- final_models_EWBW[[dep_var_name]]$removed_vars
  
  # Create a dataframe
  removed_df <- data.frame(
    Variable = removed_vars,
    stringsAsFactors = FALSE
  )
  
  # Add a sheet with the name of the dependent variable
  addWorksheet(wb, sheetName = dep_var_name)
  writeData(wb, sheet = dep_var_name, x = removed_df)
}

# Save the workbook
saveWorkbook(wb, file = "Removed_pre_BWEW_2023.xlsx", overwrite = TRUE)
cat("Removed predictors written to 'removed_predictors.xlsx'\n")

```

## Find new IV list

```{r}
# Function to find the exact column indices for removed variables
find_exact_indices <- function(final_models, full_datasets) {
  for (dep_var in names(final_models)) {
    removed_vars <- final_models[[dep_var]]$removed_vars
    
    # Get the corresponding dataset
    dataset <- full_datasets[[dep_var]]
    
    cat("\nDependent variable:", dep_var, "\n")
    cat("Removed variables:", paste(removed_vars, collapse=", "), "\n")
    
    # Print all column names with their indices for debugging
    cat("Dataset columns:\n")
    for (i in 1:ncol(dataset)) {
      cat("Column", i, ":", colnames(dataset)[i], "\n")
    }
    
    # Try exact match first
    cat("\nMatching indices:\n")
    for (var in removed_vars) {
      exact_match <- which(colnames(dataset) == var)
      if (length(exact_match) > 0) {
        cat(var, "=> index:", exact_match, "\n")
      } else {
        # Try partial match if exact match fails
        partial_matches <- grep(var, colnames(dataset), fixed=TRUE)
        if (length(partial_matches) > 0) {
          cat(var, "=> potential indices (partial match):", 
              paste(partial_matches, collapse=", "), "\n")
        } else {
          cat(var, "=> NOT FOUND\n")
        }
      }
    }
    cat("\n----------------------------\n")
  }
}

# Usage:
 find_exact_indices(final_models_EWBW, Indice_testing_dataset)
```


## New dataset use removed colm number

```{r}
# Create datasets for each dependent variable
dep_var_datasets <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Store in list
  dep_var_datasets[[dep_var_name]] <- dataset
}


# Take lag of the missing value of 2024 Q1
dep_var_datasets_modified <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Repeat the first column (dep_var) and add it before the first column
  dataset <- cbind(dataset[, 1, drop = FALSE], dataset)  # Add first column as the first column again
  
  # Loop over columns starting from the second column
  for(col in 2:ncol(dataset)) {
    # Check if the last row value is missing
    if(is.na(dataset[nrow(dataset), col])) {
      # Apply lag: take the value from the previous row for all rows of the column
      dataset[, col] <- lag(dataset[, col], 1, default = NA)
      
      # Modify column name to indicate lag if it was modified
      new_col_name <- paste0(colnames(dataset)[col], "_lag1")
      colnames(dataset)[col] <- new_col_name
    }
  }
  
  # Store in list
  dep_var_datasets_modified[[dep_var_name]] <- dataset
}

# Delete 1996Q1
dep_var_datasets_modified <- lapply(dep_var_datasets_modified, function(x) {
  x[-(1:8), ]#period
})

# Extract last row from each dataset
last_row <- lapply(dep_var_datasets_modified, function(x) {
  x[nrow(x), ]
})

# Remove last row from each dataset
dep_var_datasets <- lapply(dep_var_datasets_modified, function(x) {
  x[-nrow(x), ]
})
```

```{r}
# Function to find and remove columns based on variable names in final_models
remove_by_found_indices <- function(final_models, dep_var_datasets) {
  # For each dependent variable
  for (dep_var in names(final_models)) {
    # Get the removed variable names
    removed_vars <- final_models[[dep_var]]$removed_vars
    
    # Get the dataset
    dataset <- dep_var_datasets[[dep_var]]
    
    # Find indices to remove
    indices_to_remove <- c()
    for (var in removed_vars) {
      # Try exact match first
      exact_match <- which(colnames(dataset) == var)
      if (length(exact_match) > 0) {
        indices_to_remove <- c(indices_to_remove, exact_match)
        cat("Found", var, "at index", exact_match, "\n")
      } else {
        # Try partial match if exact match fails
        partial_matches <- grep(var, colnames(dataset), fixed=TRUE)
        if (length(partial_matches) > 0) {
          indices_to_remove <- c(indices_to_remove, partial_matches)
          cat("Found", var, "at indices (partial match):", 
              paste(partial_matches, collapse=", "), "\n")
        } else {
          cat("Warning: Could not find", var, "in", dep_var, "dataset\n")
        }
      }
    }
    
    # Remove the columns if any were found
    if (length(indices_to_remove) > 0) {
      # Ensure unique indices (no duplicates)
      indices_to_remove <- unique(indices_to_remove)
      
      # Remove columns
      dep_var_datasets[[dep_var]] <- dataset[, -indices_to_remove, drop = FALSE]
      cat("Removed", length(indices_to_remove), "columns from", dep_var, "dataset\n")
      cat("Removed column indices:", paste(indices_to_remove, collapse=", "), "\n")
    } else {
      cat("No columns to remove from", dep_var, "dataset\n")
    }
  }
  
  return(dep_var_datasets)
}

# Apply the removal of columns
dep_var_datasets <- remove_by_found_indices(final_models_EWBW,dep_var_datasets)
quarter_four = remove_by_found_indices(final_models_EWBW,quarter_four)
dep_var_datasets
```

# Extract the hold-out period (rows 73-79) + 2024 Q1

```{r}
# Extracting rows 80-87 from each dataset in dep_var_datasets
hold_out_dataset <- lapply(dep_var_datasets, function(df) {
  df[105:111, ]#period
})

# Create a list to store the updated datasets with quarter_four at the end
holdout_with_last_row <- list()

# Iterate through each dep_var_name and append quarter_four at the end
for(dep_var_name in names(hold_out_dataset)) {
  
  # Ensure column names match between quarter_four and the data frame in hold_out_dataset
  if (!all(names(quarter_four[[dep_var_name]]) == names(hold_out_dataset[[dep_var_name]]))) {
    # Manually adjust the column names of quarter_four to match hold_out_dataset
    names(quarter_four[[dep_var_name]]) <- names(hold_out_dataset[[dep_var_name]])
  }
  
  # Combine the selected holdout (80-87 rows) and the corresponding last row from quarter_four
  holdout_with_last_row[[dep_var_name]] <- rbind(
    hold_out_dataset[[dep_var_name]],
    quarter_four[[dep_var_name]]
  )
}

# Update hold_out_dataset with the modified datasets
hold_out_dataset <- holdout_with_last_row

hold_out_dataset 
```

# Data without hold out
```{r}
hold_out_period <- 105:111#period

# Function to exclude the hold-out period from the dataset
exclude_hold_out_1 <- function(df) {
  # Exclude the rows that are in the hold-out period (81-88)
  df_no_hold <- df[!rownames(df) %in% hold_out_period, ]
  return(df_no_hold)
}

#--------------------------------------------------------------
# This is the dataset without the hold out period 
Indice_testing_dataset <- lapply(dep_var_datasets, function(df) {
  # Exclude rows 81-88 from the dataset and return the modified dataset
  exclude_hold_out_1(df)
})


dep_var_datasets = Indice_testing_dataset
# Verify the result
Indice_testing_dataset
```

```{r}
# Delete the last row from each dataframe in dep_var_datasets
dep_var_datasets <- lapply(dep_var_datasets, function(df) {
  # Get the number of rows in the dataframe
  n_rows <- nrow(df)
  
  # Return all rows except the last one
  return(df[1:(n_rows-1), ])
})

# Verify the result
dep_var_datasets
```

# Expanding Window
```{r}

# Create expanding windows with training starting from rows 1-70
expanding_windows <- list()

for(dep_var_name in names(dep_var_datasets)) {
  current_data <- dep_var_datasets[[dep_var_name]]
  dataset_windows <- list()
  
  # Get total number of rows in the dataset
  total_rows <- nrow(current_data)
  
  # Initial training window: rows 1-70
  train_end <- 70
  window_index <- 1
  
  # Maximum training window size is 95 rows
  max_train_rows <- 95
  
  # Create windows while:
  # 1. The training window doesn't exceed 95 rows
  # 2. There's enough data for testing (at least 10 rows for testing)
  while (train_end <= max_train_rows && train_end < total_rows - 9) {
    train_data <- current_data[1:train_end, ]
    test_data <- current_data[(train_end + 1):total_rows, ]
    
    dataset_windows[[window_index]] <- list(train = train_data, test = test_data)
    
    # Expand training window by 5 rows for next iteration
    train_end <- train_end + 5
    window_index <- window_index + 1
    
    # Break if we would exceed 95 rows on the next iteration
    if (train_end > max_train_rows) {
      break
    }
  }
  
  expanding_windows[[dep_var_name]] <- dataset_windows
}

# Check the number of windows created for each dependent variable
window_counts <- sapply(expanding_windows, length)
cat("Number of windows created for each dependent variable:\n")
print(window_counts)

# Print details of the first window for the first dependent variable
first_dep_var <- names(expanding_windows)[1]
first_window <- expanding_windows[[first_dep_var]][[1]]
cat("\nFirst window for", first_dep_var, ":\n")
cat("Training rows:", nrow(first_window$train), "\n")
cat("Training indices:", paste(rownames(first_window$train)[1], "to", 
                               rownames(first_window$train)[nrow(first_window$train)]), "\n")
cat("Testing rows:", nrow(first_window$test), "\n")
cat("Testing indices:", paste(rownames(first_window$test)[1], "to", 
                              rownames(first_window$test)[nrow(first_window$test)]), "\n")

# Print details of the last window for the first dependent variable
last_window_index <- length(expanding_windows[[first_dep_var]])
last_window <- expanding_windows[[first_dep_var]][[last_window_index]]
cat("\nLast window for", first_dep_var, ":\n")
cat("Training rows:", nrow(last_window$train), "\n")
cat("Training indices:", paste(rownames(last_window$train)[1], "to", 
                               rownames(last_window$train)[nrow(last_window$train)]), "\n")
cat("Testing rows:", nrow(last_window$test), "\n")
cat("Testing indices:", paste(rownames(last_window$test)[1], "to", 
                              rownames(last_window$test)[nrow(last_window$test)]), "\n")

# Function to summarize window information
summarize_windows <- function(windows_list) {
  summary_df <- data.frame(
    Window = integer(),
    Train_Start = integer(),
    Train_End = integer(),
    Test_Start = integer(),
    Test_End = integer(),
    Train_Size = integer(),
    Test_Size = integer(),
    stringsAsFactors = FALSE
  )
  
  for (dep_var_name in names(windows_list)) {
    cat("\nSummary for", dep_var_name, ":\n")
    windows <- windows_list[[dep_var_name]]
    
    for (i in 1:length(windows)) {
      window <- windows[[i]]
      train_start <- as.numeric(rownames(window$train)[1])
      train_end <- as.numeric(rownames(window$train)[nrow(window$train)])
      test_start <- as.numeric(rownames(window$test)[1])
      test_end <- as.numeric(rownames(window$test)[nrow(window$test)])
      
      summary_df <- rbind(summary_df, data.frame(
        Window = i,
        Train_Start = train_start,
        Train_End = train_end,
        Test_Start = test_start,
        Test_End = test_end,
        Train_Size = nrow(window$train),
        Test_Size = nrow(window$test),
        stringsAsFactors = FALSE
      ))
    }
    
    print(summary_df)
    summary_df <- summary_df[0, ] # Reset for next dependent variable
  }
}

# Run the summary function
summarize_windows(expanding_windows)

# Create a visual table for all windows
cat("\nVisual representation of all expanding windows:\n")
for (dep_var_name in names(expanding_windows)) {
  cat("\n", dep_var_name, ":\n")
  cat(paste(rep("-", 80), collapse = ""), "\n")
  cat(sprintf("%-10s %-20s %-20s %-15s %-15s\n", 
              "Window", "Training Range", "Testing Range", "Train Size", "Test Size"))
  cat(paste(rep("-", 80), collapse = ""), "\n")
  
  for (i in 1:length(expanding_windows[[dep_var_name]])) {
    window <- expanding_windows[[dep_var_name]][[i]]
    train_range <- sprintf("%s to %s", 
                          rownames(window$train)[1], 
                          rownames(window$train)[nrow(window$train)])
    test_range <- sprintf("%s to %s", 
                         rownames(window$test)[1], 
                         rownames(window$test)[nrow(window$test)])
    cat(sprintf("%-10d %-20s %-20s %-15d %-15d\n", 
                i, train_range, test_range, nrow(window$train), nrow(window$test)))
  }
  cat(paste(rep("-", 80), collapse = ""), "\n")
}
```

## fit the model
```{r}
set.seed(1234)
fit_bsts_model <- function(train_data) {
  set.seed(1234)
  # (first column): dep
  y <- train_data[, 1]
  # (all columns except first): IVs
  X <- train_data[, -1]
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y)
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Model
  model <- bsts(formula,
                state.specification = ss,
                niter = 10000,
                data = as.data.frame(cbind(y = y, X)))
  
  return(model)
}

fit_all_windows <- function(dep_var_splits) {
  models <- list()
  for(i in 1:5) {
    models[[i]] <- fit_bsts_model(dep_var_splits[[i]]$train)
  }
  return(models)
}

# Fit models for all dependent variables
all_modelewbw <- list()
# Net Interest Income
all_modelewbw$net_interest_income <- fit_all_windows(expanding_windows$net_interest_income)
# Non-Interest Income
all_modelewbw$non_interest_income <- fit_all_windows(expanding_windows$non_interest_income)
# Provision Credit Loss
all_modelewbw$provision_credit_loss <- fit_all_windows(expanding_windows$provision_credit_loss)
# Non-Interest Expense
all_modelewbw$non_interest_expense <- fit_all_windows(expanding_windows$non_interest_expense)
```



##error

```{r}
# Function to calculate error metrics with NA handling
calculate_errors <- function(actual, predicted) {
  # Remove NA values in pairs
  valid_indices <- !is.na(actual) & !is.na(predicted)
  if (sum(valid_indices) == 0) {
    return(data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA))
  }
  
  actual <- actual[valid_indices]
  predicted <- predicted[valid_indices]
  
  # Handle case where there's not enough data for MASE
  if (length(actual) <= 1) {
    mase <- NA
  } else {
    # Check if mean(abs(diff(actual))) is zero or very close to zero
    denom <- mean(abs(diff(actual)))
    if (is.na(denom) || denom < 1e-10) {
      mase <- NA
    } else {
      mae <- mean(abs(actual - predicted))
      mase <- mae / denom
    }
  }
  
  # Calculate metrics
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  
  # Handle zero or close to zero values for MAPE
  mape_valid <- actual != 0 & abs(actual) > 1e-10
  if (sum(mape_valid) > 0) {
    mape <- mean(abs((actual[mape_valid] - predicted[mape_valid]) / actual[mape_valid])) * 100
  } else {
    mape <- NA
  }
  
  # Handle zero or close to zero denominators for SMAPE
  smape_denom <- abs(actual) + abs(predicted)
  smape_valid <- smape_denom > 1e-10
  if (sum(smape_valid) > 0) {
    smape <- mean(2 * abs(actual[smape_valid] - predicted[smape_valid]) / smape_denom[smape_valid]) * 100
  } else {
    smape <- NA
  }
  
  # Calculate OWA using available metrics
  if (!is.na(mase) && !is.na(mape)) {
    owa <- (mase + mape) / 2
  } else if (!is.na(mase)) {
    owa <- mase
  } else if (!is.na(mape)) {
    owa <- mape
  } else {
    owa <- NA
  }
  
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Updated evaluation function for all expanding windows
evaluate_predictions <- function(model, test_data) {
  if (is.null(model) || nrow(test_data) == 0) {
    return(list(
      actual = NA,
      predicted_mean = NA,
      predicted_median = NA,
      errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
      errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
    ))
  }
  
  # Extract actual values for testing
  actual <- test_data[, 1]
  
  # Prepare test data for prediction
  pred_data <- as.data.frame(test_data[, -1, drop = FALSE])
  
  # Attempt to make predictions with error handling
  tryCatch({
    # Make predictions
    pred <- predict(model, newdata = pred_data, burn = 100)
    
    # Calculate means and medians
    mean_predictions <- colMeans(pred$distribution)
    median_predictions <- apply(pred$distribution, 2, median)
    
    # Calculate errors
    errors_mean <- calculate_errors(actual, mean_predictions)
    errors_median <- calculate_errors(actual, median_predictions)
    
    return(list(
      actual = actual,
      predicted_mean = mean_predictions,
      predicted_median = median_predictions,
      errors_mean = errors_mean,
      errors_median = errors_median
    ))
  }, error = function(e) {
    cat("Error in prediction:", conditionMessage(e), "\n")
    return(list(
      actual = actual,
      predicted_mean = rep(NA, length(actual)),
      predicted_median = rep(NA, length(actual)),
      errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
      errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
    ))
  })
}

# Function to evaluate all windows for a specific dependent variable
evaluate_all_windows_for_var <- function(models, expanding_windows_var) {
  results <- list()
  
  for(i in 1:5) {
    if (i > length(models) || i > length(expanding_windows_var)) {
      results[[i]] <- list(
        actual = NA,
        predicted_mean = NA,
        predicted_median = NA,
        errors_mean = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA),
        errors_median = data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
      )
      next
    }
    
    # Get test data for this window
    test_data <- expanding_windows_var[[i]]$test
    
    # Evaluate predictions
    cat("Evaluating window", i, "...\n")
    results[[i]] <- evaluate_predictions(models[[i]], test_data)
  }
  
  return(results)
}

# Run evaluations for all dependent variables
all_evaluations_bwew <- list()

# Net Interest Income
cat("\nEvaluating Net Interest Income models...\n")
all_evaluations_bwew$net_interest_income <- evaluate_all_windows_for_var(
  all_modelewbw$net_interest_income, 
  expanding_windows$net_interest_income
)

# Non-Interest Income
cat("\nEvaluating Non-Interest Income models...\n")
all_evaluations_bwew$non_interest_income <- evaluate_all_windows_for_var(
  all_modelewbw$non_interest_income, 
  expanding_windows$non_interest_income
)

# Provision Credit Loss
cat("\nEvaluating Provision Credit Loss models...\n")
all_evaluations_bwew$provision_credit_loss <- evaluate_all_windows_for_var(
  all_modelewbw$provision_credit_loss, 
  expanding_windows$provision_credit_loss
)

# Non-Interest Expense
cat("\nEvaluating Non-Interest Expense models...\n")
all_evaluations_bwew$non_interest_expense <- evaluate_all_windows_for_var(
  all_modelewbw$non_interest_expense, 
  expanding_windows$non_interest_expense
)

# Print summary tables
print_summary_tables <- function(all_evaluations, expanding_windows) {
  for(var_name in names(all_evaluations)) {
    cat("\nResults for", var_name, "\n")
    
    # Mean-based metrics
    cat("\nMean-based metrics:\n")
    window_results_mean <- data.frame()
    
    for(i in 1:5) {
      if (i > length(all_evaluations[[var_name]]) || 
          is.null(all_evaluations[[var_name]][[i]]) || 
          all(is.na(all_evaluations[[var_name]][[i]]$errors_mean))) {
        next
      }
      
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      window_errors$Window <- i
      
      # Add training and testing info
      if (i <= length(expanding_windows[[var_name]])) {
        window_errors$TrainSize <- nrow(expanding_windows[[var_name]][[i]]$train)
        window_errors$TestSize <- nrow(expanding_windows[[var_name]][[i]]$test)
        
        # Add range information
        train_start <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$train)[1])
        train_end <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$train)[nrow(expanding_windows[[var_name]][[i]]$train)])
        test_start <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$test)[1])
        test_end <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$test)[nrow(expanding_windows[[var_name]][[i]]$test)])
        
        window_errors$TrainRange <- paste(train_start, "-", train_end)
        window_errors$TestRange <- paste(test_start, "-", test_end)
      } else {
        window_errors$TrainSize <- NA
        window_errors$TestSize <- NA
        window_errors$TrainRange <- "Unknown"
        window_errors$TestRange <- "Unknown"
      }
      
      window_results_mean <- rbind(window_results_mean, window_errors)
    }
    
    if (nrow(window_results_mean) > 0) {
      # Round numeric columns
      numeric_cols <- sapply(window_results_mean, is.numeric)
      window_results_mean[, numeric_cols] <- round(window_results_mean[, numeric_cols], 4)
      
      # Print results
      print(window_results_mean[, c("Window", "TrainSize", "TestSize", "TrainRange", "TestRange", 
                                    "MSE", "MAE", "MAPE", "SMAPE", "MASE", "OWA")])
    } else {
      cat("No valid results available for mean-based metrics.\n")
    }
    
    # Median-based metrics
    cat("\nMedian-based metrics:\n")
    window_results_median <- data.frame()
    
    for(i in 1:5) {
      if (i > length(all_evaluations[[var_name]]) || 
          is.null(all_evaluations[[var_name]][[i]]) || 
          all(is.na(all_evaluations[[var_name]][[i]]$errors_median))) {
        next
      }
      
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      window_errors$Window <- i
      
      # Add training and testing info
      if (i <= length(expanding_windows[[var_name]])) {
        window_errors$TrainSize <- nrow(expanding_windows[[var_name]][[i]]$train)
        window_errors$TestSize <- nrow(expanding_windows[[var_name]][[i]]$test)
        
        # Add range information
        train_start <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$train)[1])
        train_end <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$train)[nrow(expanding_windows[[var_name]][[i]]$train)])
        test_start <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$test)[1])
        test_end <- as.numeric(rownames(expanding_windows[[var_name]][[i]]$test)[nrow(expanding_windows[[var_name]][[i]]$test)])
        
        window_errors$TrainRange <- paste(train_start, "-", train_end)
        window_errors$TestRange <- paste(test_start, "-", test_end)
      } else {
        window_errors$TrainSize <- NA
        window_errors$TestSize <- NA
        window_errors$TrainRange <- "Unknown"
        window_errors$TestRange <- "Unknown"
      }
      
      window_results_median <- rbind(window_results_median, window_errors)
    }
    
    if (nrow(window_results_median) > 0) {
      # Round numeric columns
      numeric_cols <- sapply(window_results_median, is.numeric)
      window_results_median[, numeric_cols] <- round(window_results_median[, numeric_cols], 4)
      
      # Print results
      print(window_results_median[, c("Window", "TrainSize", "TestSize", "TrainRange", "TestRange", 
                                      "MSE", "MAE", "MAPE", "SMAPE", "MASE", "OWA")])
    } else {
      cat("No valid results available for median-based metrics.\n")
    }
  }
}

# Find the best models based on sMAPE
find_best_models <- function(all_evaluations) {
  best_models <- list()
  
  for (dep_var in names(all_evaluations)) {
    # Extract sMAPE values for each window
    smape_values <- numeric(5)
    
    for (i in 1:5) {
      if (i <= length(all_evaluations[[dep_var]]) && 
          !is.null(all_evaluations[[dep_var]][[i]]) && 
          !is.null(all_evaluations[[dep_var]][[i]]$errors_mean) && 
          !is.na(all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE)) {
        smape_values[i] <- all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE
      } else {
        smape_values[i] <- NA
      }
    }
    
    # Check if we have any valid smape values
    if (all(is.na(smape_values))) {
      cat("Warning: No valid sMAPE values for", dep_var, "\n")
      best_models[[dep_var]] <- list(
        best_window = NA,
        best_smape = NA
      )
      next
    }
    
    # Find the window with the minimum sMAPE
    best_window <- which.min(smape_values)
    best_smape <- min(smape_values, na.rm = TRUE)
    
    # Find the second best window (if available)
    if (sum(!is.na(smape_values)) > 1) {
      smape_values_without_best <- smape_values
      smape_values_without_best[best_window] <- Inf
      second_best_window <- which.min(smape_values_without_best)
      second_best_smape <- min(smape_values_without_best, na.rm = TRUE)
      
      best_models[[dep_var]] <- list(
        best_window_1 = best_window,
        best_smape_1 = best_smape,
        best_window_2 = second_best_window,
        best_smape_2 = second_best_smape
      )
    } else {
      best_models[[dep_var]] <- list(
        best_window = best_window,
        best_smape = best_smape
      )
    }
  }
  
  return(best_models)
}

```

## Calculate model weights based on test&train SMAPE

```{r}
calculate_model_weights <- function(all_evaluations) {
  weights_list <- list()
  for(var_name in names(all_evaluations)) {
    # Extract SMAPE for both mean and median
    smape_values_mean <- sapply(all_evaluations[[var_name]], function(x) x$errors_mean$SMAPE)
    smape_values_median <- sapply(all_evaluations[[var_name]], function(x) x$errors_median$SMAPE)
    
    epsilon <- 1e-10
    
    # Calculate weights for mean-based SMAPE
    inverse_weights_mean <- 1 / pmax(smape_values_mean, epsilon)
    normalized_weights_mean <- inverse_weights_mean / sum(inverse_weights_mean)
    
    # Calculate weights for median-based SMAPE
    inverse_weights_median <- 1 / pmax(smape_values_median, epsilon)
    normalized_weights_median <- inverse_weights_median / sum(inverse_weights_median)
    
    # Create dataframes for both
    weights_df_mean <- data.frame(
      Window = 1:length(smape_values_mean),
      SMAPE = smape_values_mean,
      Weight = normalized_weights_mean,
      Method = "Mean"
    )
    
    weights_df_median <- data.frame(
      Window = 1:length(smape_values_median),
      SMAPE = smape_values_median,
      Weight = normalized_weights_median,
      Method = "Median"
    )
    
    # Sort both by SMAPE
    weights_df_mean <- weights_df_mean[order(weights_df_mean$SMAPE), ]
    weights_df_median <- weights_df_median[order(weights_df_median$SMAPE), ]
    
    weights_list[[var_name]] <- list(
      mean = weights_df_mean,
      median = weights_df_median
    )
  }
  
  # Print results
  for(var_name in names(weights_list)) {
    cat("\nWeights for", var_name, "(Mean):\n")
    print(round(weights_list[[var_name]]$mean[, -4], 4))
    
    cat("\nWeights for", var_name, "(Median):\n")
    print(round(weights_list[[var_name]]$median[, -4], 4))
  }
  
  return(weights_list)
}
```

```{r}
# Calculate model weights
model_weights_bwew  <- calculate_model_weights(all_evaluations_bwew)
```

# prediction to holds out period according to the

```{r}
# Fixed function to generate prediction tables and SMAPE for all windows and all variables using holdout data
predict_all_windows_variables <- function(all_modelewbw, hold_out_dataset) {
  # Initialize results storage
  smape_results <- list()
  
  # Loop through each window
  for(window_num in 1:6) {
    cat("\n================================================================\n")
    cat("WINDOW", window_num, "PREDICTIONS (HOLDOUT PERIOD)\n")
    cat("================================================================\n")
    
    # Loop through each variable
    for(var_name in names(all_modelewbw)) {
      # Skip if model or holdout data doesn't exist
      if (!(var_name %in% names(all_modelewbw)) || 
          length(all_modelewbw[[var_name]]) < window_num || 
          !(var_name %in% names(hold_out_dataset))) {
        cat("Skipping", var_name, "for window", window_num, "(data not available)\n")
        next
      }
      
      # Get the model for this window and variable
      model <- all_modelewbw[[var_name]][[window_num]]
      
      # Get the holdout dataset for this variable
      holdout_data <- hold_out_dataset[[var_name]]
      
      # Create period labels (80-87 plus Q1)
      period_labels <- c(as.character(105:111), "Q1")#period
      
      # Get actual values and predictors
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Make predictions
      pred <- predict(model, newdata = pred_data, burn = 100)
      mean_predictions <- colMeans(pred$distribution)
      median_predictions <- apply(pred$distribution, 2, median)
      
      # Calculate errors
      mean_errors <- mean_predictions - actual_values
      median_errors <- median_predictions - actual_values
      
      # Calculate SMAPE for mean predictions
      mean_smape <- mean(2 * abs(actual_values - mean_predictions) / 
                       (abs(actual_values) + abs(mean_predictions))) * 100
      
      # Calculate SMAPE for median predictions
      median_smape <- mean(2 * abs(actual_values - median_predictions) / 
                         (abs(actual_values) + abs(median_predictions))) * 100
      
      # Store SMAPE results
      if (is.null(smape_results[[var_name]])) {
        smape_results[[var_name]] <- data.frame(
          Window = numeric(),
          Mean_SMAPE = numeric(),
          Median_SMAPE = numeric()
        )
      }
      
      smape_results[[var_name]] <- rbind(smape_results[[var_name]], 
                                      data.frame(
                                        Window = window_num,
                                        Mean_SMAPE = mean_smape,
                                        Median_SMAPE = median_smape
                                      ))
      
      # Display SMAPE for this window and variable
      cat("\n----------------------------------------------------------------\n")
      cat("SMAPE FOR WINDOW", window_num, "PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      cat("Mean SMAPE:", round(mean_smape, 4), "%\n")
      cat("Median SMAPE:", round(median_smape, 4), "%\n")
      
      # Display mean table - FIX: Create numeric data first, then add Period as a separate column
      cat("\n----------------------------------------------------------------\n")
      cat("TABLE: WINDOW", window_num, "MEAN PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      
      # Create numeric results first
      numeric_results <- data.frame(
        Actual = actual_values,
        Predicted = mean_predictions,
        Error = mean_errors
      )
      
      # Then display with Period column
      cat("Period\tActual\tPredicted\tError\n")
      for (i in 1:length(period_labels)) {
        cat(period_labels[i], "\t", 
            round(numeric_results$Actual[i], 4), "\t", 
            round(numeric_results$Predicted[i], 4), "\t", 
            round(numeric_results$Error[i], 4), "\n")
      }
      
      # Display median table - Use same approach
      cat("\n----------------------------------------------------------------\n")
      cat("TABLE: WINDOW", window_num, "MEDIAN PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      
      # Create numeric results first
      numeric_results <- data.frame(
        Actual = actual_values,
        Predicted = median_predictions,
        Error = median_errors
      )
      
      # Then display with Period column
      cat("Period\tActual\tPredicted\tError\n")
      for (i in 1:length(period_labels)) {
        cat(period_labels[i], "\t", 
            round(numeric_results$Actual[i], 4), "\t", 
            round(numeric_results$Predicted[i], 4), "\t", 
            round(numeric_results$Error[i], 4), "\n")
      }
    }
  }
  
  # Display summary of SMAPE results for all windows and variables
  cat("\n================================================================\n")
  cat("SUMMARY OF SMAPE RESULTS FOR ALL WINDOWS AND VARIABLES (HOLDOUT PERIOD)\n")
  cat("================================================================\n")
  
  for(var_name in names(smape_results)) {
    cat("\n----------------------------------------------------------------\n")
    cat("SMAPE SUMMARY FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    print(round(smape_results[[var_name]], 4))
  }
  
  return(smape_results)
}

# Fixed function to aggregate predictions for all windows
aggregate_and_calculate_smape <- function(smape_results, all_modelewbw, hold_out_dataset) {
  # Initialize results storage for aggregated SMAPE
  aggregate_smape_results <- list()
  
  # Loop through each dependent variable
  for (var_name in names(all_modelewbw)) {
    # Skip if holdout data doesn't exist
    if (!(var_name %in% names(hold_out_dataset))) {
      cat("Skipping", var_name, "(holdout data not available)\n")
      next
    }
    
    cat("\n================================================================\n")
    cat("AGGREGATED PREDICTIONS AND SMAPE FOR", toupper(var_name), "(HOLDOUT PERIOD)\n")
    cat("================================================================\n")
    
    # Create period labels (80-87 plus Q1)
    period_labels <- c(as.character(105:111), "Q1")#period
    
    # Get holdout data for this variable
    holdout_data <- hold_out_dataset[[var_name]]
    actual_values <- holdout_data[, 1]
    pred_data <- as.data.frame(holdout_data[, -1])
    
    # Initialize matrices to store predictions from all windows (each row is a window)
    all_mean_preds <- matrix(0, nrow = 6, ncol = length(actual_values))
    all_median_preds <- matrix(0, nrow = 6, ncol = length(actual_values))
    windows_available <- numeric()
    
    # Loop through each window to collect predictions
    for(window_num in 1:6) {
      # Skip if model doesn't exist
      if (!(var_name %in% names(all_modelewbw)) || 
          length(all_modelewbw[[var_name]]) < window_num) {
        next
      }
      
      # Get the model for this window and variable
      model <- all_modelewbw[[var_name]][[window_num]]
      
      # Make predictions
      pred <- predict(model, newdata = pred_data, burn = 100)
      mean_predictions <- colMeans(pred$distribution)
      median_predictions <- apply(pred$distribution, 2, median)
      
      # Store in matrices
      all_mean_preds[window_num, ] <- mean_predictions
      all_median_preds[window_num, ] <- median_predictions
      windows_available <- c(windows_available, window_num)
    }
    
    # Filter matrices to only include available windows
    all_mean_preds <- all_mean_preds[windows_available, , drop = FALSE]
    all_median_preds <- all_median_preds[windows_available, , drop = FALSE]
    
    # Calculate aggregated predictions (average across windows)
    agg_mean_preds <- colMeans(all_mean_preds)
    agg_median_preds <- colMeans(all_median_preds)
    
    # Calculate errors
    mean_errors <- agg_mean_preds - actual_values
    median_errors <- agg_median_preds - actual_values
    
    # Calculate SMAPE for the aggregated predictions
    mean_smape <- mean(2 * abs(actual_values - agg_mean_preds) / 
                       (abs(actual_values) + abs(agg_mean_preds))) * 100
    
    median_smape <- mean(2 * abs(actual_values - agg_median_preds) / 
                         (abs(actual_values) + abs(agg_median_preds))) * 100
    
    # Store aggregated SMAPE results
    aggregate_smape_results[[var_name]] <- data.frame(
      Mean_SMAPE = mean_smape,
      Median_SMAPE = median_smape
    )
    
    # Display aggregated SMAPE results for this variable
    cat("\n----------------------------------------------------------------\n")
    cat("AGGREGATED SMAPE FOR", toupper(var_name), "(HOLDOUT PERIOD)\n")
    cat("----------------------------------------------------------------\n")
    cat("Mean SMAPE:", round(mean_smape, 4), "%\n")
    cat("Median SMAPE:", round(median_smape, 4), "%\n")
    
    # Display aggregated mean predictions table - FIX: Avoid mixing types in data frame
    cat("\n----------------------------------------------------------------\n")
    cat("TABLE: AGGREGATED MEAN PREDICTIONS FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    
    # Create numeric results first
    numeric_results <- data.frame(
      Actual = actual_values,
      Predicted = agg_mean_preds,
      Error = mean_errors
    )
    
    # Then display with Period column
    cat("Period\tActual\tPredicted\tError\n")
    for (i in 1:length(period_labels)) {
      cat(period_labels[i], "\t", 
          round(numeric_results$Actual[i], 4), "\t", 
          round(numeric_results$Predicted[i], 4), "\t", 
          round(numeric_results$Error[i], 4), "\n")
    }
    
    # Display aggregated median predictions table - Use same approach
    cat("\n----------------------------------------------------------------\n")
    cat("TABLE: AGGREGATED MEDIAN PREDICTIONS FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    
    # Create numeric results first
    numeric_results <- data.frame(
      Actual = actual_values,
      Predicted = agg_median_preds,
      Error = median_errors
    )
    
    # Then display with Period column
    cat("Period\tActual\tPredicted\tError\n")
    for (i in 1:length(period_labels)) {
      cat(period_labels[i], "\t", 
          round(numeric_results$Actual[i], 4), "\t", 
          round(numeric_results$Predicted[i], 4), "\t", 
          round(numeric_results$Error[i], 4), "\n")
    }
  }
  
  # Display summary of aggregated SMAPE results
  cat("\n================================================================\n")
  cat("SUMMARY OF AGGREGATED SMAPE RESULTS FOR ALL VARIABLES (HOLDOUT PERIOD)\n")
  cat("================================================================\n")
  
  for (var_name in names(aggregate_smape_results)) {
    cat("\n----------------------------------------------------------------\n")
    cat("AGGREGATED SMAPE SUMMARY FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    print(round(aggregate_smape_results[[var_name]], 4))
  }
  
  return(aggregate_smape_results)
}

# Run the functions with your models and holdout data
smape_results <- predict_all_windows_variables(all_modelewbw, hold_out_dataset)
aggregate_smape_results <- aggregate_and_calculate_smape(smape_results, all_modelewbw, hold_out_dataset)
```

# Summary stat

```{r}
get_model_diagnostics <- function(model, window_data) {
  # Numerically stable log-mean-exp function
  log_mean_exp <- function(x) {
    m <- max(x)
    m + log(mean(exp(x - m)))
  }
  
  window_stats <- lapply(seq_along(model), function(i) {
    ll <- model[[i]]$log.likelihood   # log-likelihood from the model
    n_train <- nrow(window_data[[i]]$train)
    
    # --- DIC Calculation ---
    # We always compute DIC on the full set of draws.
    # If ll is a matrix, use all elements; if it's a vector, use it directly.
    if (!is.null(dim(ll))) {
      # ll is already a matrix (n_iter x n_train)
      ll_vec <- as.vector(ll)
    } else {
      ll_vec <- ll
    }
    
    D_bar <- -2 * mean(ll_vec)
    D_theta_bar <- -2 * max(ll_vec)  # Using the maximum (posterior mode) as a plug‐in estimate
    p_D <- D_bar - D_theta_bar
    DIC <- (D_bar + p_D) / n_train  # reporting per-observation
    
    # --- WAIC Calculation ---
    # Check if we can reshape ll into a point-wise matrix.
    if (is.null(dim(ll)) && (length(ll) %% n_train != 0)) {
      # The log likelihood is aggregated (one value per iteration)
      # Compute overall lppd and p_waic, then scale per observation.
      lppd <- log_mean_exp(ll)
      p_waic <- var(ll)
      WAIC <- (-2 * (lppd - p_waic)) / n_train
    } else {
      # If ll is already a matrix, or if its length is divisible by n_train, reshape it.
      if (is.null(dim(ll))) {
        n_iter <- length(ll) / n_train
        ll_matrix <- matrix(ll, ncol = n_train, byrow = TRUE)
      } else {
        ll_matrix <- ll
      }
      # For each observation (i.e. each column), compute a stable log-mean-exp and variance.
      lppd <- sum(apply(ll_matrix, 2, log_mean_exp))
      p_waic <- sum(apply(ll_matrix, 2, var))
      WAIC <- (-2 * (lppd - p_waic)) / n_train
    }
    
    data.frame(
      Window = i,
      Training_Size = n_train,
      DIC = DIC,
      WAIC = WAIC
    )
  })
  
  do.call(rbind, window_stats)
}

print_all_diagnostics <- function(diagnostics) {
  for(dep_var in names(diagnostics)) {
    cat("\n===", dep_var, "===\n")
    print(diagnostics[[dep_var]])
  }
}
```

```{r}
all_diagnostics <- list()
for(dep_var in names(all_modelewbw)) {
  all_diagnostics[[dep_var]] <- get_model_diagnostics(all_modelewbw[[dep_var]], expanding_windows[[dep_var]])
}

print_all_diagnostics(all_diagnostics)
```

## Function of getting LOOIC

```{r}
calculate_looic <- function(model, window_data) {
  # Get training sample size 
  n_train <- nrow(window_data$train)
  
  # Calculate LOOIC
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  # normalized LOOIC
  looic <- (-2 * sum(loo_liks)) / n_train
  
  return(looic)
}

# Function to apply for all windows
get_all_looic <- function(models, windows) {
  n_windows <- length(models)
  looic_results <- numeric(n_windows)
  
  for (i in 1:n_windows) {
    looic_results[i] <- calculate_looic(models[[i]], windows[[i]])
  }
  
  return(looic_results)
}
```

### Apply the calculation to all dependent variables

```{r}
all_looic_ew <- list()
for (dep_var in names(all_modelewbw)) {
  all_looic_ew[[dep_var]] <- get_all_looic(
    all_modelewbw[[dep_var]], 
    expanding_windows[[dep_var]]
  )
}
# Display results
for (dep_var in names(all_looic_ew)) {
  cat(sprintf("\nNormalized LOOIC Results for %s\n", dep_var))
  for (i in 1:length(all_looic_ew[[dep_var]])) {
    cat(sprintf("Window %d: %.4f\n", i, all_looic_ew[[dep_var]][i]))
  }
}
```

## 95% confi interval

```{r}
calculate_confidence_intervals <- function(all_modelewbw, expanding_windows) {
  for(dep_var in names(all_modelewbw)) {
    cat("\n95% Confidence Intervals for", dep_var, "\n")
    for(window in seq_along(all_modelewbw[[dep_var]])) {
      cat("\nWindow", window, ":\n")
      
      model <- all_modelewbw[[dep_var]][[window]]
      coefficients <- model$coefficients
      
      # Calculate statistics
      stats <- apply(coefficients, 2, function(x) {
        quantiles <- quantile(x, probs = c(0.025, 0.975))
        c("2.5%" = quantiles[1],
          "Mean" = mean(x),
          "Median" = median(x),
          "97.5%" = quantiles[2])
      })
      
      stats_df <- as.data.frame(t(stats))
      stats_df <- round(stats_df, 4)
      print(stats_df)
    }
  }
}

confidence_intervals_ew <- calculate_confidence_intervals(all_modelewbw, expanding_windows)
```

## mcmc size

```{r}
library(coda)
get_mcmc_stats <- function(all_modelewbw, expanding_windows) {
  for(dep_var in names(all_modelewbw)) {
    cat("\nEffective Sample Size for", dep_var, "\n")
    cat("===============================\n")
    
    n_windows <- length(all_modelewbw[[dep_var]])
    
    for(window in 1:n_windows) {
      model <- all_modelewbw[[dep_var]][[window]]
      
      # Get coefficients for this window
      coef_matrix <- model$coefficients
      param_names <- colnames(coef_matrix)
      
      cat(sprintf("\nWindow %d\n", window))
      cat("-------------------\n")
      
      for(param in param_names) {
        # Extract chain for this parameter
        chain <- mcmc(coef_matrix[, param])
        
        # Calculate ESS
        ess <- try(effectiveSize(chain), silent = TRUE)
        
        # Print results
        cat(sprintf("%s: %.1f\n", param, 
                    if(inherits(ess, "try-error")) NA else ess))
      }
    }
  }
}

# Apply the function
mcmc_stats_ew  <- get_mcmc_stats(all_modelewbw, expanding_windows)
```

## Bayesian R² (Gelman et al.)

```{r}
## Function to compute Bayesian R² using posterior draws on the training set
calculate_bayes_R2 <- function(model, train_data, burn = 100) {
  # Extract the observed response (assumes the first column is the dependent variable)
  y_train <- train_data[, 1]
  
  # Get posterior draws for the fitted (training) values.
  # When newdata is provided, predict() returns a list with a 'distribution'
  # element that is a matrix with rows = MCMC iterations and columns = training observations.
  pred_train <- predict(model, newdata = as.data.frame(train_data[, -1]), burn = burn)
  
  # Compute Bayesian R² for each MCMC draw:
  # For each draw (each row of the prediction matrix) compute:
  #   R² = var(fitted values) / (var(fitted values) + var(residuals))
  R2_draws <- apply(pred_train$distribution, 1, function(fitted_values) {
    var_fitted <- var(fitted_values)
    var_resid  <- var(y_train - fitted_values)
    R2 <- var_fitted / (var_fitted + var_resid)
    return(R2)
  })
  
  return(R2_draws)
}

## Loop over each dependent variable and each window to compute Bayesian R²
bayesian_R2_results <- list()

for (dep_var_name in names(all_modelewbw)) {
  models_list <- all_modelewbw[[dep_var_name]]
  
  # Data frame to store summary metrics for each window
  bayes_R2_summary <- data.frame(
    Window = integer(),
    Mean_R2 = numeric(),
    Median_R2 = numeric(),
    Lower_R2 = numeric(),
    Upper_R2 = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Loop through each of the 7 expanding windows
  for (i in 1:length(models_list)) {
    # Get the training data for the current window
    train_data <- expanding_windows[[dep_var_name]][[i]]$train
    
    # Extract the fitted model for this window
    model <- models_list[[i]]
    
    # Compute the Bayesian R² draws for the training set
    R2_draws <- calculate_bayes_R2(model, train_data, burn = 100)
    
    # Summarize the draws (you can adjust the summary metrics as desired)
    bayes_R2_summary <- rbind(
      bayes_R2_summary,
      data.frame(
        Window = i,
        Mean_R2 = mean(R2_draws),
        Median_R2 = median(R2_draws),
        Lower_R2 = quantile(R2_draws, 0.025),
        Upper_R2 = quantile(R2_draws, 0.975)
      )
    )
  }
  
  # Store the summary for the current dependent variable
  bayesian_R2_results[[dep_var_name]] <- bayes_R2_summary
}

## Print the Bayesian R² summary for each dependent variable and window
for (dep_var_name in names(bayesian_R2_results)) {
  cat("\nBayesian R² for", dep_var_name, ":\n")
  print(round(bayesian_R2_results[[dep_var_name]], 4))
}

```

## Ljung Box

```{r}
# Load necessary package (Box.test is in stats, so this is optional)
library(stats)

# Function to compute Ljung-Box test diagnostics for each window
get_ljung_box_diagnostics <- function(models, window_data, lags = 10) {
  results <- list()
  
  # Loop over each dependent variable
  for (dep_var_name in names(models)) {
    window_results <- list()
    
    # Loop over each expanding window (for this dep var)
    for (i in seq_along(models[[dep_var_name]])) {
      # Extract the training data for the current window
      train_data <- window_data[[dep_var_name]][[i]]$train
      # Assume the first column is the dependent variable (actual values)
      y_train <- train_data[, 1]
      
      # Obtain one-step-ahead predictions for the training set
      # Here we use the same burn-in as in your evaluation (e.g., 100)
      pred <- predict(models[[dep_var_name]][[i]], 
                      newdata = as.data.frame(train_data[, -1]), 
                      burn = 100)
      
      # Compute the mean fitted value for each observation (across MCMC draws)
      fitted_values <- colMeans(pred$distribution)
      
      # Calculate residuals (actual minus fitted)
      residuals <- y_train - fitted_values
      
      # Perform the Ljung-Box test on the residuals using the specified number of lags
      lb_test <- Box.test(residuals, lag = lags, type = "Ljung-Box")
      
      # Save the results for the current window
      window_results[[i]] <- data.frame(
        Window = i,
        Training_Size = nrow(train_data),
        LB_Statistic = lb_test$statistic,
        LB_pvalue = lb_test$p.value
      )
    }
    
    # Combine the results for all windows for this dependent variable
    results[[dep_var_name]] <- do.call(rbind, window_results)
  }
  
  return(results)
}

# Calculate the Ljung-Box diagnostics (with default lag = 10)
ljung_box_results <- get_ljung_box_diagnostics(all_modelewbw, expanding_windows, lags = 10)

# Print the Ljung-Box test results for each dependent variable
for (dep_var in names(ljung_box_results)) {
  cat("\nLjung-Box Test Diagnostics for", dep_var, ":\n")
  print(ljung_box_results[[dep_var]])
}

```

## Posterior Prob

```{r}
analyze_pips <- function(all_modelewbw, expanding_windows) {
  for (dep_var in names(all_modelewbw)) {
    cat(sprintf("\nPosterior Inclusion Probabilities for %s\n", dep_var))
    
    models <- all_modelewbw[[dep_var]]
    for (window in seq_along(models)) {
      cat(sprintf("\nWindow %d:\n", window))
      model <- models[[window]]
      
      if (!is.null(model$coefficients)) {
        # Calculate average inclusion probabilities across MCMC iterations
        inclusion_matrix <- model$coefficients[,1:ncol(model$predictors)]
        pips <- colMeans(inclusion_matrix != 0) 
        names(pips) <- colnames(model$predictors)
        pips <- pips[-1] # Remove intercept
        pips <- sort(pips, decreasing = TRUE)
        print(pips)
      } else {
        cat("No coefficients available\n")
      }
    }
  }
}

pips_results <- analyze_pips(all_modelewbw, expanding_windows)
```

## Geweke’s Diagnostic

check if your MCMC chain has converged p \< 0.05 means there's a statistically significant difference between the means of first and last parts (bad for significant)

```{r}
calculate_geweke <- function(all_modelewbw, expanding_windows) {
  for(dep_var in names(all_modelewbw)) {
    cat("\nGeweke's Diagnostic for", dep_var, "\n")
    
    n_windows <- length(all_modelewbw[[dep_var]])
    
    for(window in 1:n_windows) {
      model <- all_modelewbw[[dep_var]][[window]]
      # Get coefficients matrix
      coef_matrix <- as.matrix(model$coefficients)
      cat(sprintf("\nWindow %d\n", window))
      cat("-------------------\n")
      # Loop each parameter (column)
      for(j in 1:ncol(coef_matrix)) {
        param_name <- colnames(coef_matrix)[j]
        chain <- mcmc(coef_matrix[, j])
        # the default function in R 
        geweke_stats <- geweke.diag(chain, frac1=0.1, frac2=0.5)
        
        # Extract z-score
        z_score <- as.numeric(geweke_stats$z)
        
        # Calculate p-value only if z_score is not NA
        if(!is.na(z_score)) {
          p_value <- 2 * pnorm(-abs(z_score))
          cat(sprintf("%s:\n", param_name))
          cat(sprintf("  Z-statistic: %.4f\n", z_score))
          cat(sprintf("  p-value: %.4f\n", p_value))
          
          if(!is.na(p_value) && p_value < 0.05) {
            cat("  ** Possible convergence issue (p < 0.05)\n")
          }
        } else {
          cat(sprintf("%s: Unable to calculate Geweke diagnostic\n", param_name))
        }
      }
    }
  }
}

# Apply 
geweke_results <- calculate_geweke(all_modelewbw, expanding_windows)
```

## Heidelberger-Welch

```{r}
calculate_heidel_welch <- function(model, window_data) {
  # Print diagnostic results for each parameter
  for(i in 1:length(model)) {
    cat("\nWindow", i, "\n")
    cat("-------------------\n")
    
    # Convert coefficients to mcmc object
    coef_matrix <- as.matrix(model[[i]]$coefficients)
    
    # Test each parameter
    for(j in 1:ncol(coef_matrix)) {
      param_name <- colnames(coef_matrix)[j]
      chain <- mcmc(coef_matrix[,j])
      
      # Run heidel.diag and capture output
      tryCatch({
        test <- heidel.diag(chain)
        
        if(!is.null(test)) {
          cat("\nParameter:", param_name, "\n")
          cat("Stationarity test:", ifelse(test[1,1] == 1, "PASSED", "FAILED"), "\n")
          cat("p-value:", format(test[1,3], digits=4), "\n")
          cat("Start iteration:", test[1,2], "\n")
          cat("Mean:", format(test[1,4], digits=4), "\n")
          cat("Halfwidth test:", ifelse(test[1,5] == 1, "PASSED", "FAILED"), "\n")
          cat("Halfwidth:", format(test[1,6], digits=4), "\n")
        } else {
          cat("\nParameter:", param_name, ": Test returned null result\n")
        }
      }, error = function(e) {
        cat("\nParameter:", param_name, ": Test failed -", e$message, "\n")
      })
    }
  }
}

# Apply to all dependent variables
for(dep_var in names(all_modelewbw)) {
  cat("\n\n=== Results for", dep_var, "===\n")
  heidel_results = calculate_heidel_welch(all_modelewbw[[dep_var]], expanding_windows[[dep_var]])
}
```

## Raftery_lewis

```{r}
calculate_convergence_diagnostics <- function(model, window_data) {
  for(i in 1:length(model)) {
    cat("\nWindow", i, "\n")
    cat("-------------------\n")
    
    coef_matrix <- as.matrix(model[[i]]$coefficients)
    
    for(j in 1:ncol(coef_matrix)) {
      param_name <- colnames(coef_matrix)[j]
      chain <- as.vector(coef_matrix[,j])
      
      cat("\nParameter:", param_name, "\n")
      
      # Basic diagnostics
      n_unique <- length(unique(chain))
      autocorr <- cor(chain[-1], chain[-length(chain)])
      
      cat("Chain diagnostics:\n")
      cat("Unique values:", n_unique, "\n")
      cat("Mean:", mean(chain), "\n")
      cat("SD:", sd(chain), "\n")
      cat("Autocorrelation:", autocorr, "\n")
      
      if(n_unique < 10) {
        cat("WARNING: Too few unique values for reliable convergence diagnostics\n")
        next
      }
      
      chain_mcmc <- as.mcmc(chain)
      
      # Both Raftery-Lewis and Geweke
      tryCatch({
        # Raftery-Lewis
        binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
        rl <- raftery.diag(binary_chain)
        
        cat("\nRaftery-Lewis Diagnostic:\n")
        cat("Iterations needed:", rl$resmatrix[1,"N"], "\n")
        cat("Burn-in needed:", rl$resmatrix[1,"M"], "\n")
        cat("Dependence factor:", rl$resmatrix[1,"I"], "\n")
        
        # Geweke
        gw <- geweke.diag(chain_mcmc)
        pvalue <- 2 * (1 - pnorm(abs(gw$z)))  # Two-tailed p-value from z-score
        
        cat("\nGeweke Test:\n")
        cat("Z-score:", gw$z, "\n")
        cat("P-value:", pvalue, "\n")
        
        # Combined assessment
        if(rl$resmatrix[1,"I"] > 5 || pvalue < 0.05) {
          cat("\nConvergence Issues Detected:\n")
          if(rl$resmatrix[1,"I"] > 5) {
            cat("- High dependence factor in Raftery-Lewis\n")
          }
          if(pvalue < 0.05) {
            cat("- Significant Geweke test (p < 0.05)\n")
          }
          if(autocorr > 0.7) {
            cat("- High autocorrelation detected\n")
          }
          if(length(chain) < rl$resmatrix[1,"N"]) {
            cat("- More iterations needed\n")
            cat(sprintf("  Current: %d, Recommended: %d\n", 
                       length(chain), rl$resmatrix[1,"N"]))
          }
        } else {
          cat("\nNo convergence issues detected\n")
        }
        
      }, error = function(e) {
        cat("\nDiagnostic calculation failed:\n")
        cat("- Error:", conditionMessage(e), "\n")
        if(autocorr > 0.7) {
          cat("- High autocorrelation might be causing issues\n")
        }
      })
    }
  }
}

# Run diagnostics
for(dep_var in names(all_modelewbw)) {
  cat("\n\n=== Convergence Diagnostics for", dep_var, "===\n")
  calculate_convergence_diagnostics(all_modelewbw[[dep_var]], expanding_windows[[dep_var]])
}
```

## predictive interval

The regular test sets in your expanding windows only include historical data that was set aside for testing model performance. The extended test sets add the final row of your data (the most recent quarter), which contains the predictors needed to generate a forecast for the next period.

```{r}
# Create extended test sets including the quarter_four data
create_extended_test_sets <- function(dep_var_datasets, expanding_windows, quarter_four) {
  extended_test_sets <- list()
  
  for(dep_var_name in names(dep_var_datasets)) {
    current_data <- dep_var_datasets[[dep_var_name]]
    dataset_windows <- list()
    
    # For each window in the expanding windows
    for(window_idx in 1:length(expanding_windows[[dep_var_name]])) {
      # Get the original test data from the expanding window
      test_data <- expanding_windows[[dep_var_name]][[window_idx]]$test
      
      # Add quarter_four data as the last row
      # Make sure to match column names
      if (!all(names(quarter_four[[dep_var_name]]) == names(test_data))) {
        names(quarter_four[[dep_var_name]]) <- names(test_data)
      }
      
      # Combine test data with quarter_four
      extended_test <- rbind(test_data, quarter_four[[dep_var_name]])
      
      # Add to windows
      dataset_windows[[window_idx]] <- extended_test
    }
    
    extended_test_sets[[dep_var_name]] <- dataset_windows
  }
  
  return(extended_test_sets)
}

# 95% Predictive Intervals using the fixed extended test sets
get_predictive_intervals <- function(all_evaluations, all_modelewall, extended_test_sets) {
  for(dep_var in names(all_evaluations)) {
    # Get SMAPE values and window numbers for mean-based
    smape_mean <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_mean) || is.na(x$errors_mean$SMAPE)) return(Inf)
      return(x$errors_mean$SMAPE)
    })
    
    # Sort and get indices of windows with valid SMAPE values
    valid_windows_mean <- which(is.finite(smape_mean))
    if(length(valid_windows_mean) > 0) {
      top_n_mean <- min(5, length(valid_windows_mean))
      ordered_windows_mean <- valid_windows_mean[order(smape_mean[valid_windows_mean])]
      top_5_mean <- ordered_windows_mean[1:top_n_mean]
    } else {
      top_5_mean <- integer(0)
    }
    
    # Get SMAPE values and window numbers for median-based
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_median) || is.na(x$errors_median$SMAPE)) return(Inf)
      return(x$errors_median$SMAPE)
    })
    
    # Sort and get indices of windows with valid SMAPE values
    valid_windows_median <- which(is.finite(smape_median))
    if(length(valid_windows_median) > 0) {
      top_n_median <- min(5, length(valid_windows_median))
      ordered_windows_median <- valid_windows_median[order(smape_median[valid_windows_median])]
      top_5_median <- ordered_windows_median[1:top_n_median]
    } else {
      top_5_median <- integer(0)
    }
    
    # Calculate intervals for mean-based top models
    cat("\n=====================================")
    cat(sprintf("\n%s - Mean-Based Top %d Models (by sMAPE)\n", dep_var, length(top_5_mean)))
    cat("=====================================\n")
    
    intervals_mean <- data.frame()
    for(window in top_5_mean) {
      # Skip if window or model doesn't exist
      if(is.null(extended_test_sets[[dep_var]][[window]]) || 
         is.null(all_modelewall[[dep_var]][[window]])) {
        cat("Skipping window", window, "- data or model not available\n")
        next
      }
      
      # Get the extended test data and extract the last row
      extended_data <- extended_test_sets[[dep_var]][[window]]
      last_row <- extended_data[nrow(extended_data), ]
      last_row_predictors <- last_row[-1, drop = FALSE]  # Remove first column (response)
      
      # Check for missing values and handle them
      if(any(is.na(last_row_predictors))) {
        cat("Warning: Missing values in predictors for window", window, "\n")
        # Try to impute missing values with column means from the test data
        # This is a simple approach - you might want to use more sophisticated imputation
        for(col in 1:ncol(last_row_predictors)) {
          if(is.na(last_row_predictors[1, col])) {
            # Use mean of non-NA values in this column from test data
            col_mean <- mean(extended_data[-nrow(extended_data), col+1], na.rm = TRUE)
            last_row_predictors[1, col] <- col_mean
          }
        }
      }
      
      # Check for invalid values after imputation
      if(any(is.na(last_row_predictors))) {
        cat("Error: Still have missing values after imputation in window", window, "\n")
        next
      }
      
      # Try to make prediction
      tryCatch({
        pred <- suppressWarnings(predict(all_modelewall[[dep_var]][[window]], 
                                       newdata = as.data.frame(last_row_predictors),
                                       burn = 100))
        
        # Check if prediction was successful
        if(is.null(pred) || is.null(pred$distribution) || ncol(pred$distribution) == 0) {
          cat("Error: Prediction failed for window", window, "\n")
          next
        }
        
        # Get the distribution
        dist <- pred$distribution[, 1]
        
        # Calculate intervals
        intervals_mean <- rbind(intervals_mean, data.frame(
          Window = window,
          SMAPE = smape_mean[window],
          Lower_95 = quantile(dist, 0.025),
          Mean = mean(dist),
          Median = median(dist),
          Upper_95 = quantile(dist, 0.975)
        ))
      }, error = function(e) {
        cat("Error in prediction for window", window, ":", conditionMessage(e), "\n")
      })
    }
    
    # Print results if any
    if(nrow(intervals_mean) > 0) {
      print(round(intervals_mean, 4))
    } else {
      cat("No valid predictions for mean-based models.\n")
    }
    
    # Calculate intervals for median-based top models
    cat("\n=====================================")
    cat(sprintf("\n%s - Median-Based Top %d Models (by sMAPE)\n", dep_var, length(top_5_median)))
    cat("=====================================\n")
    
    intervals_median <- data.frame()
    for(window in top_5_median) {
      # Skip if window or model doesn't exist
      if(is.null(extended_test_sets[[dep_var]][[window]]) || 
         is.null(all_modelewall[[dep_var]][[window]])) {
        cat("Skipping window", window, "- data or model not available\n")
        next
      }
      
      # Get the extended test data and extract the last row
      extended_data <- extended_test_sets[[dep_var]][[window]]
      last_row <- extended_data[nrow(extended_data), ]
      last_row_predictors <- last_row[-1, drop = FALSE]  # Remove first column (response)
      
      # Check for missing values and handle them
      if(any(is.na(last_row_predictors))) {
        cat("Warning: Missing values in predictors for window", window, "\n")
        # Try to impute missing values with column means from the test data
        for(col in 1:ncol(last_row_predictors)) {
          if(is.na(last_row_predictors[1, col])) {
            # Use mean of non-NA values in this column from test data
            col_mean <- mean(extended_data[-nrow(extended_data), col+1], na.rm = TRUE)
            last_row_predictors[1, col] <- col_mean
          }
        }
      }
      
      # Check for invalid values after imputation
      if(any(is.na(last_row_predictors))) {
        cat("Error: Still have missing values after imputation in window", window, "\n")
        next
      }
      
      # Try to make prediction
      tryCatch({
        pred <- suppressWarnings(predict(all_modelewall[[dep_var]][[window]], 
                                       newdata = as.data.frame(last_row_predictors),
                                       burn = 100))
        
        # Check if prediction was successful
        if(is.null(pred) || is.null(pred$distribution) || ncol(pred$distribution) == 0) {
          cat("Error: Prediction failed for window", window, "\n")
          next
        }
        
        # Get the distribution
        dist <- pred$distribution[, 1]
        
        # Calculate intervals
        intervals_median <- rbind(intervals_median, data.frame(
          Window = window,
          SMAPE = smape_median[window],
          Lower_95 = quantile(dist, 0.025),
          Mean = mean(dist),
          Median = median(dist),
          Upper_95 = quantile(dist, 0.975)
        ))
      }, error = function(e) {
        cat("Error in prediction for window", window, ":", conditionMessage(e), "\n")
      })
    }
    
    # Print results if any
    if(nrow(intervals_median) > 0) {
      print(round(intervals_median, 4))
    } else {
      cat("No valid predictions for median-based models.\n")
    }
  }
}

# Example usage:
# Create extended test sets with quarter_four data
# Use the correct variables in your function call
extended_test_sets <- create_extended_test_sets(dep_var_datasets, expanding_windows, quarter_four)
get_predictive_intervals(all_evaluations_bwew, all_modelewbw, extended_test_sets)
```

## MCMC trace plot

```{r}
# Function to find top 2 best models based on evaluations
find_best_models <- function(all_evaluations) {
  best_models <- list()  # To store the best models for each dep var
  
  for(var_name in names(all_evaluations)) {
    cat("\nFor", var_name, ":\n")
    
    # Collect all sMAPE values
    all_smape_results <- data.frame(
      Window = integer(),
      Method = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Collect mean-based sMAPE values
    for(i in 1:5) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "mean",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: mean_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Collect median-based sMAPE values
    for(i in 1:5) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "median",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: median_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Sort by sMAPE (ascending) to find the best models overall
    all_smape_results <- all_smape_results[order(all_smape_results$SMAPE), ]
    
    if (nrow(all_smape_results) >= 2) {
      # Get top 2 models
      top_2_models <- all_smape_results[1:2, ]
      
      cat("Top 2 models for", var_name, "based on sMAPE:\n")
      for (i in 1:2) {
        cat("Rank", i, ": Window", top_2_models$Window[i], "using", top_2_models$Method[i], 
            "method with sMAPE =", round(top_2_models$SMAPE[i], 4), "\n")
      }
      
      # Store the top 2 models info
      best_models[[var_name]] <- list(
        best_window_1 = top_2_models$Window[1],
        best_method_1 = top_2_models$Method[1],
        best_smape_1 = top_2_models$SMAPE[1],
        best_window_2 = top_2_models$Window[2],
        best_method_2 = top_2_models$Method[2],
        best_smape_2 = top_2_models$SMAPE[2]
      )
    } else if (nrow(all_smape_results) == 1) {
      # Handle case where only one valid model is found
      cat("Only one valid model found for", var_name, ":\n")
      cat("Window:", all_smape_results$Window[1], "using", all_smape_results$Method[1], 
          "method with sMAPE =", round(all_smape_results$SMAPE[1], 4), "\n")
      
      best_models[[var_name]] <- list(
        best_window_1 = all_smape_results$Window[1],
        best_method_1 = all_smape_results$Method[1],
        best_smape_1 = all_smape_results$SMAPE[1]
      )
    } else {
      cat("No valid models found for", var_name, "\n")
    }
  }
  
  return(best_models)
}

# Run find_best_models function to get best models based on sMAPE
best_models <- find_best_models(all_evaluations_bwew)
```

```{r}
analyze_bsts_mcmc_trace_plots <- function(all_evaluations, all_models, best_models) {
  # Create a PDF device to save all plots
  pdf("MCMC_trace_plot_EW_BW_2023Q1.pdf", width=12, height=10)
  
  for(dep_var in names(best_models)) {
    # Get the info for the top models
    best_model_info <- best_models[[dep_var]]
    
    # Process Rank 1 model
    window_1 <- best_model_info$best_window_1
    method_1 <- best_model_info$best_method_1
    smape_1 <- best_model_info$best_smape_1
    
    cat("\nCreating MCMC trace plots for", dep_var, ":\n")
    cat("Rank 1: Window:", window_1, "using", method_1, "method, sMAPE =", round(smape_1, 4), "\n")
    
    # Fetch the model
    model_1 <- all_models[[dep_var]][[window_1]]
    
    # Create trace plots for Rank 1 model
    create_trace_plots(model_1, dep_var, "Rank 1", window_1, method_1, smape_1)
    
    # Process Rank 2 model if available
    if(!is.null(best_model_info$best_window_2)) {
      window_2 <- best_model_info$best_window_2
      method_2 <- best_model_info$best_method_2
      smape_2 <- best_model_info$best_smape_2
      
      cat("Rank 2: Window:", window_2, "using", method_2, "method, sMAPE =", round(smape_2, 4), "\n")
      
      # Fetch the second-ranked model
      model_2 <- all_models[[dep_var]][[window_2]]
      
      # Create trace plots for Rank 2 model
      create_trace_plots(model_2, dep_var, "Rank 2", window_2, method_2, smape_2)
    } else {
      cat("No second-best model available for", dep_var, "\n")
    }
  }
  
  dev.off()
}

# Helper function to create trace plots for a model
create_trace_plots <- function(model, dep_var, rank_label, window, method, smape) {
  tryCatch({
    # Set up plot for state variances
    par(mfrow=c(3,1), mar=c(4,4,3,1))
    
    # Plot observation variance
    if (!is.null(model$sigma.obs)) {
      plot(1:length(model$sigma.obs), model$sigma.obs, type="l", col="blue",
           main="Observation Variance (sigma.obs)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot seasonal variance if it exists
    if (!is.null(model$sigma.seasonal.4)) {
      plot(1:length(model$sigma.seasonal.4), model$sigma.seasonal.4, type="l", col="red",
           main="Seasonal Variance (sigma.seasonal.4)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot trend level variance if it exists
    if (!is.null(model$sigma.trend.level)) {
      plot(1:length(model$sigma.trend.level), model$sigma.trend.level, type="l", col="green",
           main="Trend Level Variance (sigma.trend.level)",
           xlab="Iteration", ylab="Value")
    }
    
    mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Variances"), 
          outer=TRUE, line=-1.5, cex=1.2)
    
    # If the model has regression coefficients, plot those on a new page
    if (!is.null(model$coefficients) && !is.null(model$coefficients.samples) && 
        ncol(model$coefficients.samples) > 0) {
      
      # Calculate number of coefficient plots needed
      num_coefs <- ncol(model$coefficients.samples)
      rows_needed <- min(4, num_coefs)  # Maximum 4 coefficients per page
      
      # Start a new page
      par(mfrow=c(rows_needed, 1), mar=c(4,4,3,1))
      
      # Plot each coefficient
      for (i in 1:rows_needed) {
        coef_name <- colnames(model$coefficients.samples)[i]
        if (is.null(coef_name)) coef_name <- paste("Coefficient", i)
        
        coef_samples <- model$coefficients.samples[,i]
        plot(1:length(coef_samples), coef_samples, type="l", col="purple",
             main=paste("Regression Coefficient:", coef_name),
             xlab="Iteration", ylab="Value")
      }
      
      mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Coefficients"), 
            outer=TRUE, line=-1.5, cex=1.2)
    }
    
  }, error = function(e) {
    # If error occurs, print the error and try the built-in plotting function
    cat("Error in custom trace plotting for", rank_label, ":", e$message, "\n")
    cat("Falling back to built-in plotting...\n")
    
    # Reset the plot area
    par(mfrow=c(1,1))
    
    # Try the built-in plotting method
    tryCatch({
      # Try plotting state components instead
      plot(model, "components")
      title(main=paste0(dep_var, " (", rank_label, "): Component Contributions"))
    }, error = function(e2) {
      cat("Error in fallback plotting:", e2$message, "\n")
      
      # Create an empty plot with error message
      plot(1, 1, type="n", axes=FALSE, xlab="", ylab="")
      text(1, 1, paste("Error plotting MCMC traces for", rank_label), col="red", cex=1.5)
    })
  })
}

analyze_bsts_mcmc_trace_plots(all_evaluations_bwew, all_modelewbw, best_models)
```

## Distribution Plot

```{r}
create_prediction_plots <- function(all_evaluations, all_models, extended_test_sets, hold_out_dataset) {
  library(ggplot2)
  pdf("Prediction_Distribution_EW_BW_2023Q1.pdf", width = 12, height = 8)
  
  for(dep_var in names(all_evaluations)) {
    # Create a data frame of all SMAPE values (mean and median) per window
    all_results <- data.frame(
      Window = numeric(),
      Type = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    for(i in 1:length(all_evaluations[[dep_var]])) {
      # Add mean results
      mean_smape <- all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE
      if(!is.na(mean_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "mean", SMAPE = mean_smape, stringsAsFactors = FALSE))
      }
      # Add median results
      median_smape <- all_evaluations[[dep_var]][[i]]$errors_median$SMAPE
      if(!is.na(median_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "median", SMAPE = median_smape, stringsAsFactors = FALSE))
      }
    }
    
    # Sort by SMAPE (ascending) and select the top 4 rows
    top_4 <- all_results[order(all_results$SMAPE), ][1:4, ]
    
    plot_data <- data.frame()
    label_list <- c()  # to track labels and catch duplicates
    
    for(i in 1:nrow(top_4)) {
      window <- top_4$Window[i]
      model_type <- top_4$Type[i]
      smape_value <- top_4$SMAPE[i]
      
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(extended_test_sets[[dep_var]][[window]][nrow(extended_test_sets[[dep_var]][[window]]), -1, drop = FALSE]),
                           burn = 100)
      
      # Create a label that includes rank, window, type, and SMAPE.
      base_label <- paste("Rank", i, ": Window", window, "-", model_type, "(SMAPE:", round(smape_value, 4), ")")
      # If the same label already exists, append a suffix.
      duplicate_count <- sum(label_list == base_label)
      if(duplicate_count > 0) {
        label <- paste0(base_label, " - Copy", duplicate_count + 1)
      } else {
        label <- base_label
      }
      label_list <- c(label_list, label)
      
      plot_data <- rbind(plot_data, 
                         data.frame(Value = pred$distribution[,1],
                                    Model = factor(label),
                                    stringsAsFactors = FALSE))
    }
    
    # Get the actual observed value from the hold_out_dataset (assume it's the last value in column 1)
    actual_value <- tail(hold_out_dataset[[dep_var]][, 1], 1)
    
    p <- ggplot(plot_data, aes(x = Value, fill = Model)) +
      geom_density(alpha = 0.4) +
      theme_minimal() +
      labs(title = paste(dep_var, "- Top 4 Models Based on sMAPE"),
           x = "Predicted Value",
           y = "Density") +
      geom_vline(xintercept = actual_value, color = "red", linetype = "dashed", size = 1) +
      annotate("text", x = actual_value, y = Inf, label = paste("Actual:", actual_value),
               vjust = -0.5, color = "red", size = 3)
    
    print(p)
  }
  
  dev.off()
}


# Apply the function
create_prediction_plots(all_evaluations_bwew, all_modelewbw, extended_test_sets, hold_out_dataset)
```

## Continuous Ranked Probability Score (CRPS)

```{r}
# Function to calculate CRPS
calculate_crps <- function(actual, pred_dist) {
  n <- length(pred_dist)
  sorted_pred <- sort(pred_dist)
  H <- function(x) ifelse(x >= 0, 1, 0)
  
  integral <- 0
  for(i in 1:(n-1)) {
    x <- sorted_pred[i]
    dx <- sorted_pred[i+1] - x
    F_x <- i/n
    integral <- integral + (F_x - H(x - actual))^2 * dx
  }
  return(integral)
}

# Fixed function to calculate CRPS for all windows and dependent variables
get_crps_scores <- function(all_models, expanding_windows) {
  crps_results <- list()
  
  for(dep_var in names(all_models)) {
    window_crps <- data.frame()
    
    for(i in 1:5) {  # Changed from 7 to 6 to match your window structure
      # Get the correct model (the argument order was incorrect before)
      model <- all_models[[dep_var]][[i]]
      
      # Check if the model is a bsts object
      if(!inherits(model, "bsts")) {
        cat("Warning: Model for", dep_var, "window", i, "is not a bsts object. Skipping.\n")
        next
      }
      
      # Get test data for this window
      test_data <- expanding_windows[[dep_var]][[i]]$test
      actual <- test_data[, 1]
      
      # Make predictions
      pred <- predict(model, newdata = as.data.frame(test_data[, -1]), burn = 100)
      
      # Calculate CRPS for each observation
      crps_values <- numeric(length(actual))
      for(j in seq_along(actual)) {
        crps_values[j] <- calculate_crps(actual[j], pred$distribution[,j])
      }
      
      # Store results
      window_crps <- rbind(window_crps, data.frame(
        Window = i,
        CRPS_Mean = mean(crps_values),
        CRPS_SD = sd(crps_values)
      ))
    }
    
    crps_results[[dep_var]] <- window_crps
  }
  
  # Print results
  for(dep_var in names(crps_results)) {
    cat("\n=====================================")
    cat(sprintf("\n%s - CRPS Scores\n", dep_var))
    cat("=====================================\n")
    print(round(crps_results[[dep_var]], 4))
  }
  
  return(crps_results)
}

# Apply the fixed function - with correct argument order
crps_scores <- get_crps_scores(all_modelewbw, expanding_windows)
```

## log predictive density

```{r}
calculate_lpd <- function(all_models, expanding_windows) {
 lpd_results <- list()
 
 for(dep_var in names(all_models)) {
   window_lpd <- data.frame()
   
   for(i in 1:5) {
     model <- all_models[[dep_var]][[i]]
     test_data <- expanding_windows[[dep_var]][[i]]$test
     actual <- test_data[, 1]
     
     # Get predictions
     pred <- predict.bsts(model, newdata = as.data.frame(test_data[, -1]), burn = 100)
     
     # Calculate LPD for each observation
     lpd_values <- numeric(length(actual))
     for(j in seq_along(actual)) {
       # Get density estimate of prediction distribution
       density_est <- density(pred$distribution[,j])
       # Find density at actual value
       actual_density <- approx(density_est$x, density_est$y, xout = actual[j])$y
       # Log density
       lpd_values[j] <- log(actual_density)
     }
     
     window_lpd <- rbind(window_lpd, data.frame(
       Window = i,
       LPD_Mean = mean(lpd_values, na.rm = TRUE),
       LPD_SD = sd(lpd_values, na.rm = TRUE)
     ))
   }
   lpd_results[[dep_var]] <- window_lpd
 }
 
 # Print results
 for(dep_var in names(lpd_results)) {
   cat("\n=====================================")
   cat(sprintf("\n%s - Log Predictive Density\n", dep_var))
   cat("=====================================\n")
   print(round(lpd_results[[dep_var]], 4))
 }
 
 return(lpd_results)
}

# Apply function
lpd_scores <- calculate_lpd(all_modelewbw, expanding_windows)
```


## Sensitivity

```{r}
# First, clean up any existing parallel connections
try(stopImplicitCluster(), silent = TRUE)
try(stopCluster(cl), silent = TRUE)
gc()  # Force garbage collection

# Load required packages
library(bsts)
library(parallel)

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to identify the best models
get_best_models_safe <- function() {
  best_predictions <- list()
  
  for (dep_var in names(all_modelewbw)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:6) {
      if (!dep_var %in% names(all_modelewbw) || length(all_modelewbw[[dep_var]]) < i) {
        next
      }
      
      tryCatch({
        model <- all_modelewbw[[dep_var]][[i]]
        
        # Get the testing data for this window and variable
        test_data <- expanding_windows[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        mape_mean <- mean(abs((actual_values - pred_mean) / actual_values)) * 100
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                          (abs(actual_values) + abs(pred_mean))) * 100
        
        mape_median <- mean(abs((actual_values - pred_median) / actual_values)) * 100
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                            (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to results
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = mape_mean, SMAPE = smape_mean),
                       data.frame(Window = i, Method = "median", 
                                MAPE = mape_median, SMAPE = smape_median))
      }, error = function(e) {
        cat("Error processing", dep_var, "window", i, ":", e$message, "\n")
      })
    }
    
    # Select the best five models
    if (nrow(results) > 0) {
      best_five <- results[order(results$MAPE), ][1:min(5, nrow(results)), ]
      best_predictions[[dep_var]] <- best_five
    }
  }
  
  return(best_predictions)
}

# Function to analyze one configuration
analyze_config <- function(params) {
  dep_var <- params$dep_var
  window_size <- params$window_size
  method <- params$method
  sigma <- params$sigma
  slab_var <- params$slab_var
  
  cat("Processing", dep_var, "window", window_size, "method", method, 
      "sigma", sigma, "slab_var", slab_var, "\n")
  
  # Get training and testing data
  train_data <- expanding_windows[[dep_var]][[window_size]]$train
  test_data <- expanding_windows[[dep_var]][[window_size]]$test
  
  # Extract values
  y <- train_data[, 1]
  X <- as.data.frame(train_data[, -1])
  actual_values <- test_data[, 1]
  test_predictors <- as.data.frame(test_data[, -1])
  
  # Create formula
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Full data for model
  model_data <- as.data.frame(cbind(y = y, X))
  
  # Create state specification
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y, 
                           level.sigma.prior = SdPrior(sigma = sigma),
                           slope.sigma.prior = SdPrior(sigma = sigma))
  
  # Fit model
  result <- tryCatch({
    # Using the approach that worked in our test
    model <- bsts(formula, 
                 state.specification = ss,
                 niter = 10000,
                 data = model_data)
    
    # Make predictions
    all_predictions <- numeric(length(actual_values))
    
    for (t in 1:length(actual_values)) {
      # Prepare data for this period
      pred_data <- data.frame(y = NA)
      pred_data <- cbind(pred_data, test_predictors[t, , drop = FALSE])
      
      # Predict
      single_pred <- predict(model, newdata = pred_data, burn = 100)
      
      # Extract prediction
      if (method == "mean") {
        all_predictions[t] <- mean(single_pred$distribution[, 1])
      } else {
        all_predictions[t] <- median(single_pred$distribution[, 1])
      }
    }
    
    # Check if predictions vary
    has_varying_predictions <- length(unique(round(all_predictions, 2))) > 1
    
    # Calculate metrics
    mape <- mean(abs((actual_values - all_predictions) / actual_values)) * 100
    smape <- mean(2 * abs(actual_values - all_predictions) / 
                 (abs(actual_values) + abs(all_predictions))) * 100
    
    waic <- NA
    looic <- NA
    if (!is.null(model$log.likelihood)) {
      waic <- -2 * mean(model$log.likelihood)
      looic <- -2 * mean(model$log.likelihood)
    }
    
    return(list(
      success = TRUE,
      sigma = sigma,
      slab_var = slab_var,
      mape = mape,
      smape = smape,
      waic = waic,
      looic = looic,
      has_regression = has_varying_predictions
    ))
    
  }, error = function(e) {
    return(list(
      success = FALSE,
      error = e$message
    ))
  })
  
  return(result)
}

# Controlled parallel approach
run_sensitivity_analysis_parallel <- function() {
  # Define parameter combinations
  sigmas <- c(0.2, 0.4, 0.6, 0.8)
  slab_vars <- c(50, 100, 200)
  
  # Get best models
  best_models <- get_best_models_safe()
  
  # Initialize results
  sensitivity_results <- list()
  
  # For each dependent variable
  for (dep_var in names(best_models)) {
    cat("\n=== Starting analysis for", dep_var, "===\n")
    sensitivity_results[[dep_var]] <- list()
    models_info <- best_models[[dep_var]]
    
    # For each top model
    for (i in 1:min(5, nrow(models_info))) {
      model_info <- models_info[i, ]
      window_size <- model_info$Window
      method <- as.character(model_info$Method)
      model_name <- paste0("model_", i)
      
      cat("\n--- Processing", dep_var, "window", window_size, "method", method, "---\n")
      
      # Initialize results for this model
      model_results <- list()
      
      # Create parameter combinations
      param_list <- list()
      for (sigma in sigmas) {
        for (slab_var in slab_vars) {
          param_list[[length(param_list) + 1]] <- list(
            dep_var = dep_var,
            window_size = window_size,
            method = method,
            sigma = sigma,
            slab_var = slab_var
          )
        }
      }
      
      # Create a small cluster - use just 2 or 3 cores for stability
      num_cores <- min(3, detectCores() - 1)
      cat("Using", num_cores, "cores for parallel processing\n")
      cl <- makeCluster(num_cores)
      
      # Export required data and functions
      clusterExport(cl, c("expanding_windows"), envir = .GlobalEnv)
      clusterEvalQ(cl, {
        library(bsts)
      })
      
      # Run analysis in parallel
      config_results <- parLapply(cl, param_list, analyze_config)
      
      # Clean up
      stopCluster(cl)
      
      # Process results
      for (j in 1:length(param_list)) {
        params <- param_list[[j]]
        result <- config_results[[j]]
        
        config_name <- paste0("sigma_", params$sigma, "_slab_", params$slab_var)
        
        if (result$success) {
          # Store successful result
          model_results[[config_name]] <- list(
            sigma = params$sigma,
            slab_var = params$slab_var,
            mape = result$mape,
            smape = result$smape,
            waic = result$waic,
            looic = result$looic,
            has_regression = result$has_regression
          )
          
          cat("Configuration", config_name, "- MAPE:", round(result$mape, 4),
             "SMAPE:", round(result$smape, 4), "Has regression:", result$has_regression, "\n")
        } else {
          cat("Configuration", config_name, "failed:", result$error, "\n")
        }
      }
      
      # Store results for this model
      sensitivity_results[[dep_var]][[model_name]] <- list(
        window = window_size,
        method = method,
        results = model_results
      )
      
      # Find best configuration
      best_config <- NULL
      best_mape <- Inf
      best_config_name <- ""
      
      for (config_name in names(model_results)) {
        config <- model_results[[config_name]]
        if (!is.na(config$mape) && config$mape < best_mape) {
          best_mape <- config$mape
          best_config <- config
          best_config_name <- config_name
        }
      }
      
      if (is.null(best_config)) {
        cat("No valid configurations found.\n")
      } else {
        # Print best configuration
        cat("\nBest configuration:", best_config_name, "\n")
        cat("MAPE:", round(best_config$mape, 4), "\n")
        cat("SMAPE:", round(best_config$smape, 4), "\n")
        cat("Sigma:", best_config$sigma, "\n")
        cat("Slab variance:", best_config$slab_var, "\n")
        cat("Has regression:", best_config$has_regression, "\n\n")
        
        # Create summary table
        config_summary <- data.frame(
          Sigma = numeric(),
          SlabVar = numeric(),
          MAPE = numeric(),
          SMAPE = numeric(),
          WAIC = numeric(),
          LOOIC = numeric(),
          HasRegression = logical()
        )
        
        for (config_name in names(model_results)) {
          config <- model_results[[config_name]]
          config_summary <- rbind(config_summary, 
                                data.frame(
                                  Sigma = config$sigma,
                                  SlabVar = config$slab_var,
                                  MAPE = config$mape,
                                  SMAPE = config$smape,
                                  WAIC = config$waic,
                                  LOOIC = config$looic,
                                  HasRegression = config$has_regression
                                ))
        }
        
        # Set row names and sort
        rownames(config_summary) <- names(model_results)
        config_summary <- config_summary[order(config_summary$MAPE), ]
        print(round(config_summary, 4))
      }
      
      # Clean up after each model
      rm(model_results, config_results)
      gc()
    }
  }
  
  return(sensitivity_results)
}

# Run the analysis
start_time <- Sys.time()
cat("Starting sensitivity analysis at:", format(start_time), "\n")

sensitivity_results <- run_sensitivity_analysis_parallel()

end_time <- Sys.time()
execution_time <- end_time - start_time
cat("\nSensitivity analysis completed in:", format(execution_time), "\n")
```
```{r}
# Function to identify the top 5 models by SMAPE
find_top_models <- function(all_modelewbw, sensitivity_results) {
  top_models_summary <- list()
  
  # Process each dependent variable
  for(dep_var in names(sensitivity_results)) {
    cat("\n================================================================\n")
    cat("TOP 5 MODELS FOR", toupper(dep_var), "BASED ON SMAPE\n")
    cat("================================================================\n")
    
    # 1. Collect all model results including window performance and sensitivity analysis
    all_models <- data.frame(
      Source = character(),
      Window = integer(),
      Method = character(),
      Sigma = numeric(),
      SlabVar = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Add window performance results (from best_models)
    for (i in 1:5) { # Assuming 6 windows
      if (dep_var %in% names(all_modelewbw) && length(all_modelewbw[[dep_var]]) >= i) {
        # Get the testing data for this window and variable
        test_data <- expanding_windows[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(all_modelewbw[[dep_var]][[i]], newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate SMAPE for mean
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                         (abs(actual_values) + abs(pred_mean))) * 100
        
        # Calculate SMAPE for median
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                           (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to all_models dataframe
        all_models <- rbind(all_models, 
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "mean",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_mean,
                            stringsAsFactors = FALSE
                          ),
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "median",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_median,
                            stringsAsFactors = FALSE
                          ))
      }
    }
    
    # Add sensitivity analysis results
    if (dep_var %in% names(sensitivity_results)) {
      for (model_name in names(sensitivity_results[[dep_var]])) {
        model_data <- sensitivity_results[[dep_var]][[model_name]]
        window <- model_data$window
        method <- model_data$method
        
        for (config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_models <- rbind(all_models,
                            data.frame(
                              Source = "Sensitivity",
                              Window = window,
                              Method = method,
                              Sigma = config$sigma,
                              SlabVar = config$slab_var,
                              SMAPE = config$smape,
                              stringsAsFactors = FALSE
                            ))
        }
      }
    }
    
    # 2. Sort by SMAPE and select top 5 models
    top_models <- all_models[order(all_models$SMAPE), ][1:min(5, nrow(all_models)), ]
    
    # 3. Create a formatted output
    formatted_models <- data.frame(
      Rank = 1:nrow(top_models),
      Description = character(nrow(top_models)),
      SMAPE = top_models$SMAPE,
      stringsAsFactors = FALSE
    )
    
    for (i in 1:nrow(top_models)) {
      model <- top_models[i, ]
      
      if (model$Source == "Window") {
        desc <- sprintf("%s Window %d", model$Method, model$Window)
      } else {
        desc <- sprintf("%s Window %d (sigma=%.1f, slab=%.0f)", 
                      model$Method, model$Window, model$Sigma, model$SlabVar)
      }
      
      formatted_models$Description[i] <- desc
    }
    
    # Display the top models
    print(formatted_models)
    
    # Store the results
    top_models_summary[[dep_var]] <- list(
      top_models = top_models,
      formatted_output = formatted_models
    )
  }
  
  return(top_models_summary)
}

# Example usage:
 top_model_results <- find_top_models(all_modelewbw, sensitivity_results)
```

```{r}
# Function to predict holdout period using top 5 models and create weighted ensemble
predict_and_create_ensemble <- function(all_modelewbw, top_model_results, hold_out_dataset) {
  # Clean up any existing connections
  try(stopImplicitCluster(), silent = TRUE)
  try(stopCluster(cl), silent = TRUE)
  gc()  # Force garbage collection
  
  # Load required packages
  library(parallel)
  
  # To store all results
  all_predictions <- list()
  
  # For each dependent variable
  for(dep_var in names(top_model_results)) {
    cat("\n================================================================\n")
    cat("HOLDOUT PREDICTIONS FOR", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Get top models for this variable
    top_models <- top_model_results[[dep_var]]$top_models
    
    # Get holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      cat("Holdout data not available for", dep_var, "\n")
      next
    }
    
    holdout_data <- hold_out_dataset[[dep_var]]
    
    # Prepare parameters for parallel execution
    param_list <- list()
    for(i in 1:nrow(top_models)) {
      param_list[[i]] <- list(
        dep_var = dep_var,
        model_info = top_models[i, ],
        model_index = i
      )
    }
    
    # Create a small cluster - use just 2 or 3 cores for stability
    num_cores <- min(3, detectCores() - 1)
    cat("Using", num_cores, "cores for parallel processing\n")
    cl <- makeCluster(num_cores)
    
    # Export required data and functions
    clusterExport(cl, c("all_modelewbw", "hold_out_dataset", "expanding_windows"), envir = .GlobalEnv)
    clusterEvalQ(cl, {
      library(bsts)
    })
    
    # Worker function for parallel execution
    predict_model_worker <- function(params) {
      dep_var <- params$dep_var
      model_info <- params$model_info
      model_index <- params$model_index
      
      # Create result container
      result <- list(
        model_index = model_index,
        dep_var = dep_var,
        success = FALSE
      )
      
      # Get the holdout data
      holdout_data <- hold_out_dataset[[dep_var]]
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Create period labels
      period_labels <- c(as.character(105:111), "Q1")#period
      
      # Create model description
      if(model_info$Source == "Window") {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, ")")
      } else {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, 
                          ", sigma=", model_info$Sigma, 
                          ", slab=", model_info$SlabVar, ")")
      }
      
      tryCatch({
        # Get or create model based on source
        if(model_info$Source == "Window") {
          # Use existing model
          window <- model_info$Window
          method <- as.character(model_info$Method)
          
          if(!(dep_var %in% names(all_modelewbw)) || length(all_modelewbw[[dep_var]]) < window) {
            return(c(result, list(error = "Model not available")))
          }
          
          model <- all_modelewbw[[dep_var]][[window]]
          
        } else {
          # Create model with sensitivity parameters
          window <- model_info$Window
          method <- as.character(model_info$Method)
          sigma <- model_info$Sigma
          slab_var <- model_info$SlabVar
          
          # Get original model
          if(!(dep_var %in% names(all_modelewbw)) || length(all_modelewbw[[dep_var]]) < window) {
            return(c(result, list(error = "Original model not available")))
          }
          
          original_model <- all_modelewbw[[dep_var]][[window]]
          
          # Get training data
          train_data <- expanding_windows[[dep_var]][[window]]$train
          y <- train_data[, 1]
          X <- as.data.frame(train_data[, -1])
          
          # Create formula
          predictors <- colnames(X)
          formula_str <- paste("y ~", paste(predictors, collapse = " + "))
          formula <- as.formula(formula_str)
          
          # Create model data
          model_data <- as.data.frame(cbind(y = y, X))
          
          # Set up state specification
          ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
          ss <- AddLocalLinearTrend(ss, y, 
                                  level.sigma.prior = SdPrior(sigma = sigma),
                                  slope.sigma.prior = SdPrior(sigma = sigma))
          
          # Create model
          model <- bsts(formula, 
                       state.specification = ss,
                       niter = 10000,
                       data = model_data)
        }
        
        # Make predictions - using individual predictions for each period
        predictions <- numeric(nrow(holdout_data))
        
        for(t in 1:nrow(holdout_data)) {
          # Create data for this period
          if(model_info$Source == "Window") {
            # For window models
            this_period_data <- pred_data[t, , drop = FALSE]
            
            # Make prediction
            single_pred <- predict(model, newdata = this_period_data, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          } else {
            # For sensitivity models
            pred_row <- data.frame(y = NA)
            pred_row <- cbind(pred_row, pred_data[t, , drop = FALSE])
            
            # Make prediction
            single_pred <- predict(model, newdata = pred_row, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          }
        }
        
        # Calculate errors
        abs_errors <- abs(predictions - actual_values)
        avg_abs_error <- mean(abs_errors)
        
        # Return successful result
        return(list(
          model_index = model_index,
          dep_var = dep_var,
          success = TRUE,
          model_desc = model_desc,
          predictions = predictions,
          actual_values = actual_values,
          abs_errors = abs_errors,
          avg_abs_error = avg_abs_error,
          period_labels = period_labels,
          smape = model_info$SMAPE
        ))
        
      }, error = function(e) {
        return(c(result, list(error = e$message)))
      })
    }
    
    # Run predictions in parallel
    model_results <- parLapply(cl, param_list, predict_model_worker)
    
    # Stop cluster
    stopCluster(cl)
    
    # Process and display results
    var_predictions <- list()
    
    for(result in model_results) {
      if(result$success) {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, ":", result$model_desc, "\n")
        cat("----------------------------------------------------------------\n")
        
        # Print results in a tabular format
        cat("\nPeriod\tActual\t\tPredicted\tAbsError\n")
        for(p in 1:length(result$period_labels)) {
          cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                    result$period_labels[p], 
                    result$actual_values[p], 
                    result$predictions[p], 
                    result$abs_errors[p]))
        }
        
        cat("\nAverage Absolute Error:", round(result$avg_abs_error, 2), "\n")
        
        # Store successful predictions
        var_predictions[[result$model_index]] <- result
      } else {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, "failed:", result$error, "\n")
        cat("----------------------------------------------------------------\n")
      }
    }
    
    # Store predictions for this variable
    all_predictions[[dep_var]] <- var_predictions
    
    # Now create weighted ensemble
    successful_models <- model_results[sapply(model_results, function(r) r$success)]
    
    if(length(successful_models) > 0) {
      # Get the actual values and period labels from the first successful model
      actual_values <- successful_models[[1]]$actual_values
      period_labels <- successful_models[[1]]$period_labels
      
      # Create prediction matrix
      pred_matrix <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      smape_values <- numeric(length(successful_models))
      model_descriptions <- character(length(successful_models))
      
      # Fill the matrix with predictions
      for(i in 1:length(successful_models)) {
        pred_matrix[, i] <- successful_models[[i]]$predictions
        smape_values[i] <- successful_models[[i]]$smape
        model_descriptions[i] <- successful_models[[i]]$model_desc
      }
      
      # Calculate weights based on inverse SMAPE
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      
      # Display the weights
      cat("\n================================================================\n")
      cat("WEIGHTED ENSEMBLE FOR", toupper(dep_var), "\n")
      cat("================================================================\n")
      
      cat("\nModel Weights:\n")
      for(i in 1:length(weights)) {
        cat(sprintf("Model %d: %s - Weight: %.4f (SMAPE: %.4f)\n", 
                  i, model_descriptions[i], weights[i], smape_values[i]))
      }
      
      # Calculate weighted predictions
      weighted_predictions <- numeric(length(actual_values))
      for(i in 1:length(actual_values)) {
        weighted_predictions[i] <- sum(pred_matrix[i, ] * weights, na.rm = TRUE)
      }
      
      # Calculate errors
      abs_errors <- abs(weighted_predictions - actual_values)
      
      # Calculate ensemble SMAPE
      ensemble_smape <- mean(2 * abs(actual_values - weighted_predictions) / 
                           (abs(actual_values) + abs(weighted_predictions))) * 100
      
      # Display results
      cat("\n----------------------------------------------------------------\n")
      cat("WEIGHTED ENSEMBLE RESULTS\n")
      cat("----------------------------------------------------------------\n")
      cat("Ensemble SMAPE:", round(ensemble_smape, 4), "%\n\n")
      
      cat("Period\tActual\t\tPredicted\tAbsError\n")
      for(p in 1:length(period_labels)) {
        cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                  period_labels[p], 
                  actual_values[p], 
                  weighted_predictions[p], 
                  abs_errors[p]))
      }
      
      cat("\nAverage Absolute Error:", round(mean(abs_errors), 2), "\n")
    } else {
      cat("\nNo successful models for", dep_var, "- cannot create ensemble\n")
    }
  }
  
  # Return all predictions
  return(all_predictions)
}

# Run the function to predict and create weighted ensemble
all_results <- predict_and_create_ensemble(all_modelewbw, top_model_results, hold_out_dataset)
```

```{r}
# This function replicates your 'print_ensemble_tables' logic but returns a named vector
# of Weighted Ensemble SMAPEs (one entry per dep_var).
get_ensemble_smapes <- function(all_results, top_model_results, hold_out_dataset) {
  ensemble_smapes <- numeric(0)  # named vector
  
  for(dep_var in names(top_model_results)) {
    # Skip if no holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      next
    }
    holdout_data  <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    
    # Filter out NULL models
    successful_models <- all_results[[dep_var]]
    successful_models <- successful_models[!sapply(successful_models, is.null)]
    
    if(length(successful_models) > 0) {
      smape_values <- numeric(length(successful_models))
      pred_matrix  <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      
      for(i in seq_along(successful_models)) {
        pred_matrix[, i]  <- successful_models[[i]]$predictions
        smape_values[i]   <- successful_models[[i]]$smape
      }
      
      # Weighted predictions
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      weighted_predictions <- rowSums(t(t(pred_matrix) * weights), na.rm = TRUE)
      
      # Weighted Ensemble SMAPE
      ensemble_smape <- mean(
        2 * abs(actual_values - weighted_predictions) /
        (abs(actual_values) + abs(weighted_predictions))
      ) * 100
      
      # Store in named vector
      ensemble_smapes[dep_var] <- ensemble_smape
    }
  }
  
  return(ensemble_smapes)
}
ensemble_smapes = get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
```

## ensemble result 
```{r}
# Function to combine existing results and create ensemble SMAPE
combine_existing_results <- function(all_modelewbw, sensitivity_results, hold_out_dataset) {
  
  final_results <- data.frame(
    Variable = character(),
    Prediction_Type = character(),
    Ensemble_SMAPE = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Process each dependent variable
  for(dep_var in names(hold_out_dataset)) {
    cat("\n================================================================\n")
    cat("PROCESSING", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Collect all SMAPE values and predictions from both methods
    all_configs <- data.frame(
      Method = character(),
      Window = integer(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Method 1: Original window models (from all_modelewbw)
    for(window in 1:5) {
      if(dep_var %in% names(all_modelewbw) && length(all_modelewbw[[dep_var]]) >= window) {
        
        # Get test data
        test_data <- expanding_windows[[dep_var]][[window]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Get existing model and make predictions
        model <- all_modelewbw[[dep_var]][[window]]
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        
        # Calculate SMAPE for this configuration
        smape <- mean(2 * abs(actual_values - pred_mean) / 
                        (abs(actual_values) + abs(pred_mean))) * 100
        
        all_configs <- rbind(all_configs, 
                             data.frame(Method = "Original", Window = window, SMAPE = smape))
      }
    }
    
    # Method 2: Sensitivity analysis (from sensitivity_results)
    if(dep_var %in% names(sensitivity_results)) {
      for(model_name in names(sensitivity_results[[dep_var]])) {
        model_data <- sensitivity_results[[dep_var]][[model_name]]
        
        for(config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_configs <- rbind(all_configs, 
                               data.frame(Method = "Sensitivity", 
                                          Window = model_data$window, 
                                          SMAPE = config$smape))
        }
      }
    }
    
    # Select top 10 configurations
    top_configs <- all_configs[order(all_configs$SMAPE), ][1:min(10, nrow(all_configs)), ]
    
    cat("Top 10 configurations selected:\n")
    print(top_configs)
    
    # Get holdout data
    holdout_data <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    pred_data <- as.data.frame(holdout_data[, -1])
    period_labels <- c(as.character(73:79), "Q4")
    
    # Generate holdout predictions for top 10 configurations
    pred_matrix_mean <- matrix(NA, nrow = length(actual_values), ncol = nrow(top_configs))
    pred_matrix_median <- matrix(NA, nrow = length(actual_values), ncol = nrow(top_configs))
    weights <- 1 / top_configs$SMAPE  # Inverse SMAPE weighting
    
    # For each top configuration, get holdout predictions
    for(i in 1:nrow(top_configs)) {
      config <- top_configs[i, ]
      
      if(config$Method == "Original") {
        # Use existing original model
        model <- all_modelewbw[[dep_var]][[config$Window]]
        pred <- predict(model, newdata = pred_data, burn = 100)
        pred_matrix_mean[, i] <- colMeans(pred$distribution)
        pred_matrix_median[, i] <- apply(pred$distribution, 2, median)
        
      } else {
        # For sensitivity models, we already have the predictions in all_results
        # Find the corresponding predictions from your existing all_results
        found_pred <- FALSE
        
        if(exists("all_results") && dep_var %in% names(all_results)) {
          for(result in all_results[[dep_var]]) {
            if(!is.null(result) && "predictions" %in% names(result)) {
              # This is a simplified approach - you might need to match more precisely
              # based on your specific data structure
              pred_matrix_mean[, i] <- result$predictions
              pred_matrix_median[, i] <- result$predictions  # Assuming same for now
              found_pred <- TRUE
              break
            }
          }
        }
        
        if(!found_pred) {
          cat("Could not find predictions for sensitivity config", i, "\n")
          pred_matrix_mean[, i] <- NA
          pred_matrix_median[, i] <- NA
        }
      }
    }
    
    # Remove failed predictions
    valid_cols <- !is.na(colSums(pred_matrix_mean))
    pred_matrix_mean <- pred_matrix_mean[, valid_cols, drop = FALSE]
    pred_matrix_median <- pred_matrix_median[, valid_cols, drop = FALSE]
    weights <- weights[valid_cols]
    
    if(ncol(pred_matrix_mean) > 0) {
      # Normalize weights
      weights <- weights / sum(weights)
      
      # Calculate weighted ensemble predictions
      ensemble_preds_mean <- rowSums(t(t(pred_matrix_mean) * weights), na.rm = TRUE)
      ensemble_preds_median <- rowSums(t(t(pred_matrix_median) * weights), na.rm = TRUE)
      
      # Calculate ensemble SMAPE
      ensemble_smape_mean <- mean(2 * abs(actual_values - ensemble_preds_mean) / 
                                    (abs(actual_values) + abs(ensemble_preds_mean))) * 100
      ensemble_smape_median <- mean(2 * abs(actual_values - ensemble_preds_median) / 
                                      (abs(actual_values) + abs(ensemble_preds_median))) * 100
      
      # Store results
      final_results <- rbind(final_results, 
                             data.frame(Variable = dep_var, Prediction_Type = "Mean", Ensemble_SMAPE = ensemble_smape_mean),
                             data.frame(Variable = dep_var, Prediction_Type = "Median", Ensemble_SMAPE = ensemble_smape_median))
      
      cat("\nResults for", dep_var, ":\n")
      cat("Mean Ensemble SMAPE:", round(ensemble_smape_mean, 4), "%\n")
      cat("Median Ensemble SMAPE:", round(ensemble_smape_median, 4), "%\n")
    }
  }
  
  return(final_results)
}

# Run the analysis using existing results only
ensemble_results <- combine_existing_results(all_modelewbw, sensitivity_results, hold_out_dataset)

# Create the final table you requested
cat("\n================================================================\n")
cat("FINAL RESULTS TABLE\n")
cat("================================================================\n")

# Aggregate across all variables
mean_smape <- mean(ensemble_results[ensemble_results$Prediction_Type == "Mean", "Ensemble_SMAPE"])
median_smape <- mean(ensemble_results[ensemble_results$Prediction_Type == "Median", "Ensemble_SMAPE"])

final_table <- data.frame(
  Prediction_Type = c("Mean", "Median"),
  SMAPE = c(mean_smape, median_smape)
)

print(final_table)

cat("\nDetailed by variable:\n")
print(ensemble_results)
```

## Influence

```{r}
if (!requireNamespace("foreach", quietly = TRUE)) {
  install.packages("foreach")
}
if (!requireNamespace("doParallel", quietly = TRUE)) {
  install.packages("doParallel")
}

# Setup parallel processing
library(parallel)
library(foreach)
library(doParallel)
library(bsts)

# Register parallel backend
cores <- detectCores() - 1  # Leave one core free for system processes
registerDoParallel(cores)
cat("Using", cores, "cores for parallel processing\n")

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to get window data
get_window_data <- function(dep_var, window_size) {
  window_data <- expanding_windows[[dep_var]][[window_size]]$train
  return(window_data)
}

# Function to get the best models based on testing dataset performance (changed from holdout)
get_best_models_test <- function(all_models) {
  best_predictions <- list()
  
  for (dep_var in names(all_models)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:length(all_models[[dep_var]])) {
      model <- all_models[[dep_var]][[i]]
      
      # Get the testing data for this window and variable
      test_data <- expanding_windows[[dep_var]][[i]]$test
      actual_values <- test_data[, 1]
      test_predictors <- as.data.frame(test_data[, -1])
      
      # Make predictions for testing period
      tryCatch({
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        errors_mean <- calculate_errors(actual_values, pred_mean)
        errors_median <- calculate_errors(actual_values, pred_median)
        
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = errors_mean$MAPE, SMAPE = errors_mean$SMAPE),
                       data.frame(Window = i, Method = "median", 
                                MAPE = errors_median$MAPE, SMAPE = errors_median$SMAPE))
      }, error = function(e) {
        cat("Error predicting for", dep_var, "window", i, ":", conditionMessage(e), "\n")
      })
    }
    
    # If we have results, select the best two models based on MAPE
    if (nrow(results) > 0) {
      best_two <- results[order(results$MAPE), ][1:min(2, nrow(results)), ]
      best_predictions[[dep_var]] <- best_two
    } else {
      cat("No valid predictions for", dep_var, "- skipping\n")
    }
  }
  
  # Print best models
  cat("\n=== Best Models for Testing Period ===\n")
  for(dep_var in names(best_predictions)) {
    cat("\n", dep_var, ":\n")
    print(best_predictions[[dep_var]])
  }
  
  return(best_predictions)
}

# FIXED Function to calculate influence measures with LOOIC and LPD
calculate_influence_test <- function(model, train_data, method, dep_var, window_size) {
  # Get original training data
  y <- train_data[, 1]
  X <- train_data[, -1]
  n_obs <- length(y)
  
  # Create model matrix for X
  X_matrix <- model.matrix(~ ., data = as.data.frame(X))[, -1]
  
  # Create time labels starting from 1997 Q1
  start_year <- 1997
  time_labels <- paste0(rep(start_year:(start_year + floor(n_obs/4)), each=4)[1:n_obs], 
                       " Q", rep(1:4, length.out=n_obs))
  
  # Results storage
  results <- data.frame(
    time = time_labels,
    pointwise_lpd = numeric(n_obs),
    delta_looic = numeric(n_obs),
    delta_test_forecast = numeric(n_obs)
  )
  
  # Get test data - FIXED to use passed parameters
  test_data <- expanding_windows[[dep_var]][[window_size]]$test
  test_y <- test_data[, 1]
  test_X <- as.data.frame(test_data[, -1])
  
  # Make original predictions for test period
  original_pred <- predict(model, newdata = test_X, burn = 100)
  original_point_preds <- if(method == "median") {
    apply(original_pred$distribution, 2, median)
  } else {
    colMeans(original_pred$distribution)
  }
  
  # Calculate pointwise log predictive density 
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)  # Using number of samples from log-likelihood
  
  # Ensure we have enough samples (safety check)
  if (n_samples == 0) {
    stop("No log-likelihood values available.")
  }

  log_lik_matrix <- matrix(log_lik, nrow = n_samples, ncol = n_obs, byrow = FALSE)
  
  results$pointwise_lpd <- colMeans(log_lik_matrix, na.rm = TRUE)  # Average LPD
  
  # LOOIC Calculation and test prediction changes - Parallelized version
  looic_values <- rep(NA, n_obs)
  delta_forecasts <- rep(NA, n_obs)
  
  # Parallel loop over observations
  parallel_results <- foreach(i = 1:n_obs, 
                           .packages = c("bsts"),
                           .export = c("test_X", "original_point_preds", "method")) %dopar% {
    # Create leave-one-out dataset
    loo_data <- train_data[-i, ]
    y_loo <- loo_data[, 1]
    X_loo <- loo_data[, -1]
    
    # Fit model on leave-one-out data with reduced iterations for speed
    ss <- AddSeasonal(list(), y_loo, nseasons = 4, season.duration = 1)
    ss <- AddLocalLinearTrend(ss, y_loo)
    
    model_loo <- tryCatch({
      bsts(y_loo,
           X = X_loo,
           state.specification = ss,
           niter = 10000,  # Reduced from 10000 for speed
           ping = 0)
    }, 
    error = function(e) {
      return(NULL)  # Return NULL if fitting fails
    })
    
    if (is.null(model_loo)) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    # Calculate LOOIC for leave-out model
    log_lik_loo <- model_loo$log.likelihood
    if (length(log_lik_loo) == 0) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    loo_liks <- numeric(length(log_lik_loo))
    for (j in 1:length(log_lik_loo)) {
      loo_liks[j] <- mean(log_lik_loo[-j], na.rm = TRUE)
    }
    
    looic <- -2 * mean(loo_liks, na.rm = TRUE)
    
    # Calculate change in test forecasts
    # Predict for test period using the leave-one-out model
    pred_loo <- predict(model_loo, newdata = test_X, burn = 100)
    
    point_pred_loo <- if(method == "median") {
      apply(pred_loo$distribution, 2, median)
    } else {
      colMeans(pred_loo$distribution)
    }
    
    # Calculate average change across all test observations
    delta_forecast <- mean(point_pred_loo - original_point_preds)
    
    return(list(looic = looic, delta_forecast = delta_forecast))
  }
  
  # Collect the parallel results
  for (i in 1:n_obs) {
    if (!is.null(parallel_results[[i]])) {
      looic_values[i] <- parallel_results[[i]]$looic
      delta_forecasts[i] <- parallel_results[[i]]$delta_forecast
    }
  }
  
  # Calculate original LOOIC
  looic_original <- -2 * mean(colMeans(log_lik_matrix, na.rm = TRUE))
  
  # Assign results
  results$delta_looic <- looic_values - looic_original  # Calculate delta LOOIC
  results$delta_test_forecast <- delta_forecasts
  
  return(results)
}

# FIXED Function to analyze the top models with test data
analyze_top_models_parallel <- function(best_models, all_models) {
  all_influence_results <- list()
  
  for(dep_var in names(best_models)) {
    cat("\n\nAnalyzing dependent variable:", dep_var)
    dep_influence <- list()
    models <- best_models[[dep_var]]
    
    for(i in 1:nrow(models)) {
      cat("\n\nProcessing model", i, "for", dep_var)
      model_info <- models[i, ]
      
      # Get training data for this window
      window_data <- get_window_data(dep_var, model_info$Window)
      
      # Get the original model 
      original_model <- all_models[[dep_var]][[model_info$Window]]
      
      # Calculate influence measures - FIXED to pass dep_var and window_size
      influence_results <- calculate_influence_test(
        original_model, 
        window_data,
        model_info$Method,
        dep_var,
        model_info$Window
      )
      
      # Add model identifier
      identifier <- paste("Window", model_info$Window, "-", model_info$Method)
      dep_influence[[identifier]] <- influence_results
    }
    
    all_influence_results[[dep_var]] <- dep_influence
  }
  
  return(all_influence_results)
}

# Function to generate comprehensive results tables
generate_influence_summary <- function(influence_results) {
  for(dep_var in names(influence_results)) {
    cat("\n=== Results for", dep_var, "===\n")
    
    for(model_name in names(influence_results[[dep_var]])) {
      cat("\n--- Model:", model_name, "---\n")
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # Round numeric columns to 4 decimal places
      results_df[, 2:4] <- round(results_df[, 2:4], 4)
      
      # Sort by absolute delta_test_forecast to find most influential observations
      sorted_df <- results_df[order(abs(results_df$delta_test_forecast), decreasing = TRUE), ]
      
      cat("Top 25 most influential observations:\n")
      print(head(sorted_df, 25))
      
      cat("\nSummary statistics:\n")
      
      # Calculate summary statistics separately
      lpd_summary <- summary(results_df$pointwise_lpd)
      looic_summary <- summary(results_df$delta_looic)
      forecast_summary <- summary(results_df$delta_test_forecast)
      
      # Print each metric separately to avoid data frame mixing issues
      cat("\nPointwise LPD:\n")
      print(round(lpd_summary, 4))
      cat("Std Dev:", round(sd(results_df$pointwise_lpd, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta LOOIC:\n")
      print(round(looic_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_looic, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta Test Forecast:\n")
      print(round(forecast_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_test_forecast, na.rm = TRUE), 4), "\n")
    }
  }
}

# Run the analysis with parallel processing
run_influence_analysis_parallel <- function() {
  # Start timing
  start_time <- Sys.time()
  cat("Starting parallel influence analysis at:", format(start_time), "\n")
  
  # First get the best models based on test data performance
  best_models <- get_best_models_test(all_modelewbw)
  
  # Run the influence analysis in parallel
  influence_results <- analyze_top_models_parallel(best_models, all_modelewbw)
  
  # Generate summary tables
  generate_influence_summary(influence_results)
  
  # End timing
  end_time <- Sys.time()
  execution_time <- end_time - start_time
  cat("\nInfluence analysis completed in:", format(execution_time), "\n")
  
  return(influence_results)
}

# Run the analysis
influence_results_parallel <- run_influence_analysis_parallel()

# Clean up the parallel backend when done
stopImplicitCluster()
```

# Excel

```{r}
create_enhanced_prediction_tables <- function(all_evaluations, all_models, extended_test_sets, 
                                              crps_scores, lpd_scores, file_path, 
                                              aggregate_smape_results,
                                              ensemble_smapes) {
  dep_vars <- names(all_evaluations)
  wb <- createWorkbook()
  
  for(dep_var in dep_vars) {
    addWorksheet(wb, dep_var)
    
    mean_smape   <- aggregate_smape_results[[dep_var]]$Mean_SMAPE
    median_smape <- aggregate_smape_results[[dep_var]]$Median_SMAPE  # if your code uses Median_SMAPE
    # Use weighted ensemble SMAPE if available
    sensi_smape <- if(!is.null(ensemble_smapes[dep_var])) round(ensemble_smapes[dep_var], 2) else NA
    
    variable_data <- data.frame(
      Col1 = c(
        dep_var,
        "Holds out period",
        "Prediction based on testing SMAPE",
        "Use Mean",
        "Use Median",
        "Prediction based on Sensitivity"
      ),
      Col2 = c(
        "",
        "2023Q1 - 2024Q4",
        "SMAPE",
        round(mean_smape, 2),
        round(median_smape, 2),
        sensi_smape
      ),
      stringsAsFactors = FALSE
    )
    
    writeData(wb, dep_var, variable_data, startRow = 1, startCol = 1, colNames = FALSE)
    setColWidths(wb, dep_var, cols = 1:2, widths = c(25, 20))
    
    start_row <- 8  # row 7 is blank
    
    smape_mean   <- sapply(all_evaluations[[dep_var]], function(x) x$errors_mean$SMAPE)
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) x$errors_median$SMAPE)
    
    crps_mean <- numeric(7)
    crps_sd   <- numeric(7)
    for(i in 1:7) {
      window_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        crps_mean[i] <- window_data$CRPS_Mean
        crps_sd[i]   <- window_data$CRPS_SD
      } else {
        crps_mean[i] <- NA
        crps_sd[i]   <- NA
      }
    }
    
    lpd_mean <- numeric(7)
    lpd_sd   <- numeric(7)
    for(i in 1:7) {
      window_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == i, ]
      if(nrow(window_data) > 0) {
        lpd_mean[i] <- window_data$LPD_Mean
        lpd_sd[i]   <- window_data$LPD_SD
      } else {
        lpd_mean[i] <- NA
        lpd_sd[i]   <- NA
      }
    }
    
    top_5_mean   <- order(smape_mean)[1:5]
    top_5_median <- order(smape_median)[1:5]
    
    writeData(wb, dep_var, "Predictive interval for Top 5 models based on sMAPE", 
              startRow = start_row, startCol = 1)
    
    writeData(wb, dep_var, "Use Mean", startRow = start_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = start_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = start_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = start_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = start_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = start_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = start_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = start_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = start_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = start_row + 2, startCol = 9)
    
    current_row <- start_row + 3
    for(window in top_5_mean) {
      pred_data <- extended_test_sets[[dep_var]][[window]]
      last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(last_row_predictors),
                           burn = 100)
      dist <- pred$distribution[,1]
      
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
      writeData(wb, dep_var, mean(dist),            startRow = current_row, startCol = 3)
      writeData(wb, dep_var, median(dist),          startRow = current_row, startCol = 4)
      writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
      
      writeData(wb, dep_var, smape_mean[window],  startRow = current_row, startCol = 6)
      writeData(wb, dep_var, crps_mean[window],   startRow = current_row, startCol = 7)
      writeData(wb, dep_var, lpd_mean[window],    startRow = current_row, startCol = 8)
      writeData(wb, dep_var, lpd_sd[window],      startRow = current_row, startCol = 9)
      
      current_row <- current_row + 1
    }
    
    writeData(wb, dep_var, "Use Median", startRow = current_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = current_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = current_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = current_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = current_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = current_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = current_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = current_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = current_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = current_row + 2, startCol = 9)
    
    current_row <- current_row + 3
    for(window in top_5_median) {
      pred_data <- extended_test_sets[[dep_var]][[window]]
      last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(last_row_predictors),
                           burn = 100)
      dist <- pred$distribution[,1]
      
      writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
      writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
      writeData(wb, dep_var, mean(dist),            startRow = current_row, startCol = 3)
      writeData(wb, dep_var, median(dist),          startRow = current_row, startCol = 4)
      writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
      
      writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
      writeData(wb, dep_var, crps_mean[window],    startRow = current_row, startCol = 7)
      writeData(wb, dep_var, lpd_mean[window],     startRow = current_row, startCol = 8)
      writeData(wb, dep_var, lpd_sd[window],       startRow = current_row, startCol = 9)
      
      current_row <- current_row + 1
    }
    
    setColWidths(wb, dep_var, cols = 1, width = 50)
    setColWidths(wb, dep_var, cols = 2:9, width = 15)
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}


###############################################################################
## 3) add_sensitivity_analysis
##    Moves the sensitivity analysis block down by 3 rows and removes any extra row (e.g. SMAPE row)
###############################################################################
add_sensitivity_analysis <- function(sensitivity_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for(dep_var in names(sensitivity_results)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    
    # Calculate the exact row to place the sensitivity analysis
    # Find the last non-empty row of the existing content
    last_content_row <- max(which(!is.na(sheet_data[,1]) & sheet_data[,1] != ""), na.rm = TRUE)
    
    # Add a small gap (3 rows) after the last content
    last_row <- last_content_row + 6
    
    # Create the results data frame
    results_data <- data.frame(
      Window   = integer(),
      Method   = character(),
      Sigma    = numeric(),
      Slab_Var = numeric(),
      WAIC     = numeric(),
      LOOIC    = numeric(),
      MAPE     = numeric(),
      SMAPE    = numeric(),
      stringsAsFactors = FALSE
    )
    
    for (model_name in names(sensitivity_results[[dep_var]])) {
      model_info <- sensitivity_results[[dep_var]][[model_name]]
      window <- model_info$window
      method <- model_info$method
      
      for (result_name in names(model_info$results)) {
        result <- model_info$results[[result_name]]
        results_data <- rbind(results_data, data.frame(
          Window   = window,
          Method   = method,
          Sigma    = if(!is.null(result$sigma)) result$sigma else NA,
          Slab_Var = if(!is.null(result$slab_var)) result$slab_var else NA,
          WAIC     = if(!is.null(result$waic)) result$waic else NA,
          LOOIC    = if(!is.null(result$looic)) result$looic else NA,
          MAPE     = if(!is.null(result$mape)) result$mape else NA,
          SMAPE    = if(!is.null(result$smape)) result$smape else NA,
          stringsAsFactors = FALSE
        ))
      }
    }
    
    # Write the title directly (no mysterious dep var title)
    writeData(wb, dep_var, "Sensitivity Analysis Results", startRow = last_row, startCol = 1)
    
    # Immediately write the table
    writeData(wb, dep_var, results_data, startRow = last_row + 1, startCol = 1, colNames = TRUE)
    
    # Set column widths
    setColWidths(wb, dep_var, cols = 1:8, widths = c(10,10,10,10,12,12,10,10))
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## Fixed add_influence_analysis_to_workbook function
## Fixes the spacing issue and handles delta_test_forecast/delta_holdout_forecast
###############################################################################
add_influence_analysis_to_workbook <- function(influence_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for (dep_var in names(influence_results)) {
    if (!dep_var %in% names(wb)) {
      addWorksheet(wb, dep_var)
    }
    
    sheet_data <- tryCatch({
      readWorkbook(wb, sheet = dep_var)
    }, error = function(e) {
      data.frame()
    })
    
    # Calculate exact start row by finding the last sensitivity analysis row
    # Find the last non-empty row in the sheet
    non_empty_rows <- which(!is.na(sheet_data[,1]) & sheet_data[,1] != "")
    
    if (length(non_empty_rows) > 0) {
      last_content_row <- max(non_empty_rows, na.rm = TRUE)
      # Look for "Sensitivity Analysis Results" in the sheet
      sensitivity_rows <- which(sheet_data[,1] == "Sensitivity Analysis Results")
      
      if (length(sensitivity_rows) > 0) {
        sensitivity_row <- max(sensitivity_rows)
        # Find the last row of the sensitivity table
        # Usually it's a block of data after the title
        for (i in (sensitivity_row + 1):nrow(sheet_data)) {
          if (is.na(sheet_data[i,1]) || sheet_data[i,1] == "") {
            last_sensitivity_row <- i - 1
            break
          }
          # If we reach the end of the dataframe
          if (i == nrow(sheet_data)) {
            last_sensitivity_row <- i
          }
        }
        # Start influence analysis 5 rows after the sensitivity table
        current_row <- last_sensitivity_row + 9
      } else {
        # If sensitivity analysis not found, start 5 rows after last content
        current_row <- last_content_row + 9
      }
    } else {
      # If sheet is empty
      current_row <- 1
    }
    
    processed_models <- c()
    
    for (model_name in names(influence_results[[dep_var]])) {
      if (model_name %in% processed_models) next
      processed_models <- c(processed_models, model_name)
      
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # CRITICAL FIX: Map delta_test_forecast to delta_holdout_forecast if needed
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        results_df$delta_holdout_forecast <- results_df$delta_test_forecast
      }
      
      # Process delta_looic if present
      if ("delta_looic" %in% names(results_df)) {
        results_df$delta_looic <- as.numeric(as.character(results_df$delta_looic))
      } else {
        results_df$delta_looic <- rep(NA, nrow(results_df))
      }
      
      # Round numeric columns to 4 decimal places
      numeric_cols <- c("pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      for (col in numeric_cols) {
        if (col %in% names(results_df) && is.numeric(results_df[[col]])) {
          results_df[[col]] <- round(results_df[[col]], 4)
        }
      }
      
      # Sort by absolute delta_holdout_forecast
      if (sum(!is.na(results_df$delta_holdout_forecast)) > 0) {
        sorted_df <- results_df[order(abs(results_df$delta_holdout_forecast), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df <- results_df
      }
      top_25_forecast <- head(sorted_df, 25)
      
      # Write the title
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta Holdout Forecast - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      # Get the correct column names
      if (all(c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      } else if (all(c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast")
        colnames(top_25_forecast)[colnames(top_25_forecast) == "delta_test_forecast"] <- "delta_holdout_forecast"
      } else {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      }
      
      # Make sure we only write columns that actually exist
      write_cols <- intersect(write_cols, colnames(top_25_forecast))
      
      # Write the data
      writeData(wb, dep_var, top_25_forecast[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_forecast)), 
                cols = col_idx)
      }
      
      # Proceed to next section - 5 rows after this table
      current_row <- current_row + nrow(top_25_forecast) + 5
      
      # Sort by absolute delta_looic
      if (sum(!is.na(results_df$delta_looic)) > 0) {
        sorted_df_looic <- results_df[order(abs(results_df$delta_looic), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df_looic <- results_df
      }
      top_25_looic <- head(sorted_df_looic, 25)
      
      # Write looic section
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta LOOIC - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      writeData(wb, dep_var, top_25_looic[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_looic)), 
                cols = col_idx)
      }
      
      # Proceed to next model section - 5 rows after this table
      current_row <- current_row + nrow(top_25_looic) + 5
    }
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  cat("Influence analysis results added to workbook:", file_path, "\n")
}
###############################################################################
## Parallelized version of create_excel_results
###############################################################################
create_excel_results_parallel <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  expanding_windows, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  all_results,         
  top_model_results,   
  hold_out_dataset     
) {
  file_path <- "EW_BW_H2.xlsx"
  
  # This part is harder to parallelize as it builds on previous results
  ensemble_smapes <- get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
  
  # Create prediction tables
  wb <- create_enhanced_prediction_tables(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets = extended_test_sets, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_smapes = ensemble_smapes
  )
  
  # Now we'll parallelize these steps which can be run independently
  
  # Set up parallel processing
  no_cores <- detectCores() - 1  # Leave one core free
  cat("Using", no_cores, "cores for parallel Excel processing\n")
  
  # Register the parallel backend
  cl <- makeCluster(no_cores)
  registerDoParallel(cl)
  
  # Export necessary packages to all workers
  clusterEvalQ(cl, {
    library(openxlsx)
    library(bsts)
    library(coda)
  })
  
  # Process the different analyses in parallel
  # Each function will load, modify, and save the workbook independently
  
  # Run tasks in parallel
  results <- foreach(task_num = 1:3, .packages = c("openxlsx", "bsts")) %dopar% {
    result <- switch(task_num,
      # Task 1: Add sensitivity analysis
      {
        tryCatch({
          add_sensitivity_analysis(sensitivity_results, file_path)
          "Sensitivity analysis completed successfully"
        }, error = function(e) {
          paste("Error in sensitivity analysis:", e$message)
        })
      },
      
      # Task 2: Add influence analysis
      {
        tryCatch({
          # Use the fixed version of the function
          add_influence_analysis_to_workbook(influence_results, file_path)
          "Influence analysis completed successfully"
        }, error = function(e) {
          paste("Error in influence analysis:", e$message)
        })
      },
      
      # Task 3: Add diagnostics
      {
        tryCatch({
          add_diagnostics_to_workbook(
            all_models = all_models, 
            expanding_windows = expanding_windows, 
            all_diagnostics = all_diagnostics, 
            all_looic = all_looic,
            bayesian_R2_results = bayesian_R2_results, 
            crps_scores = crps_scores, 
            lpd_scores = lpd_scores, 
            ljung_box_results = ljung_box_results,
            file_path = file_path
          )
          "Diagnostics completed successfully"
        }, error = function(e) {
          paste("Error in diagnostics:", e$message)
        })
      }
    )
    return(result)
  }
  
  # Stop the cluster
  stopCluster(cl)
  registerDoSEQ()  # Reset to sequential processing
  
  # Clean up
  rm(cl)
  gc()  # Force garbage collection
  
  # Summarize results
  summary <- paste("Excel processing results:", 
                   paste(results, collapse = "; "))
  
  return(paste("Results successfully written to", file_path, "-", summary))
}
###############################################################################
## 5) add_diagnostics_to_workbook (unchanged)
###############################################################################
add_diagnostics_to_workbook <- function(all_models, expanding_windows, all_diagnostics, all_looic, 
                                        bayesian_R2_results, crps_scores, lpd_scores, 
                                        ljung_box_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  suppressWarnings({
    if (!requireNamespace("coda", quietly = TRUE)) {
      install.packages("coda")
    }
    library(coda)
  })
  
  for(dep_var in names(all_models)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    current_row <- nrow(sheet_data) + 22
    
    for(window in 1:length(all_models[[dep_var]])) {
      if (is.null(all_models[[dep_var]][[window]])) next
      
      train_size <- if (!is.null(expanding_windows[[dep_var]]) && 
                        !is.null(expanding_windows[[dep_var]][[window]]) && 
                        !is.null(expanding_windows[[dep_var]][[window]]$train)) {
        nrow(expanding_windows[[dep_var]][[window]]$train)
      } else {
        NA
      }
      
      test_size <- if (!is.null(expanding_windows[[dep_var]]) && 
                       !is.null(expanding_windows[[dep_var]][[window]]) && 
                       !is.null(expanding_windows[[dep_var]][[window]]$test)) {
        nrow(expanding_windows[[dep_var]][[window]]$test)
      } else {
        NA
      }
      
      n_predictors <- if (!is.null(expanding_windows[[dep_var]]) && 
                          !is.null(expanding_windows[[dep_var]][[window]]) && 
                          !is.null(expanding_windows[[dep_var]][[window]]$train)) {
        ncol(expanding_windows[[dep_var]][[window]]$train) - 1
      } else {
        NA
      }
      
      dic_value <- if (!is.null(all_diagnostics) && 
                       !is.null(all_diagnostics[[dep_var]]) && 
                       !is.null(all_diagnostics[[dep_var]]$DIC) &&
                       length(all_diagnostics[[dep_var]]$DIC) >= window) {
        all_diagnostics[[dep_var]]$DIC[window]
      } else {
        NA
      }
      
      waic_value <- if (!is.null(all_diagnostics) && 
                        !is.null(all_diagnostics[[dep_var]]) && 
                        !is.null(all_diagnostics[[dep_var]]$WAIC) &&
                        length(all_diagnostics[[dep_var]]$WAIC) >= window) {
        all_diagnostics[[dep_var]]$WAIC[window]
      } else {
        NA
      }
      
      looic_value <- if (!is.null(all_looic) && 
                         !is.null(all_looic[[dep_var]]) && 
                         length(all_looic[[dep_var]]) >= window) {
        all_looic[[dep_var]][window]
      } else {
        NA
      }
      
      r2_value <- if (!is.null(bayesian_R2_results) && 
                      !is.null(bayesian_R2_results[[dep_var]]) && 
                      !is.null(bayesian_R2_results[[dep_var]]$Mean_R2) &&
                      length(bayesian_R2_results[[dep_var]]$Mean_R2) >= window) {
        bayesian_R2_results[[dep_var]]$Mean_R2[window]
      } else {
        NA
      }
      
      crps_value <- if (!is.null(crps_scores) && 
                        !is.null(crps_scores[[dep_var]]) && 
                        any(crps_scores[[dep_var]]$Window == window)) {
        crps_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == window, ]
        if (nrow(crps_data) > 0 && !is.na(crps_data$CRPS_Mean) && !is.na(crps_data$CRPS_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", crps_data$CRPS_Mean, crps_data$CRPS_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lpd_value <- if (!is.null(lpd_scores) && 
                       !is.null(lpd_scores[[dep_var]]) && 
                       any(lpd_scores[[dep_var]]$Window == window)) {
        lpd_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == window, ]
        if (nrow(lpd_data) > 0 && !is.na(lpd_data$LPD_Mean) && !is.na(lpd_data$LPD_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", lpd_data$LPD_Mean, lpd_data$LPD_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lb_stat <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_Statistic) &&
                     length(ljung_box_results[[dep_var]]$LB_Statistic) >= window) {
        ljung_box_results[[dep_var]]$LB_Statistic[window]
      } else {
        NA
      }
      
      lb_pval <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_pvalue) &&
                     length(ljung_box_results[[dep_var]]$LB_pvalue) >= window) {
        ljung_box_results[[dep_var]]$LB_pvalue[window]
      } else {
        NA
      }
      
      metrics <- data.frame(
        Metric = c(
          "Training Sample Size",
          "Test Sample Size",
          "Number of Predictors",
          "DIC/number of training period",
          "WAIC/number of training period",
          "LOOIC",
          "Bayesian R²",
          "Number of iterations used in BSTS",
          "CRPS",
          "LPD",
          "LB_Statistic",
          "LB_pvalue"
        ),
        Value = c(
          train_size,
          test_size,
          n_predictors,
          dic_value,
          waic_value,
          looic_value,
          r2_value,
          10000,
          crps_value,
          lpd_value,
          lb_stat,
          lb_pval
        )
      )
      
      writeData(wb, dep_var, paste("Window", window), startRow = current_row)
      current_row <- current_row + 2
      writeData(wb, dep_var, metrics, startRow = current_row, startCol = 1, colNames = FALSE)
      current_row <- current_row + nrow(metrics) + 2
      
      tryCatch({
        coef_stats <- data.frame(
          Parameter     = character(),
          Mean          = numeric(),
          Median        = numeric(),
          CI_2.5        = numeric(),
          CI_97.5       = numeric(),
          ESS           = numeric(),
          Post_Prob     = numeric(),
          Geweke_Pval   = numeric(),
          HW_Pval       = numeric(),
          RL_Autocorr   = numeric(),
          RL_Iterations = numeric(),
          RL_Burnin     = numeric(),
          RL_Dependence = numeric(),
          stringsAsFactors = FALSE
        )
        
        model <- all_models[[dep_var]][[window]]
        if (!is.null(model) && !is.null(model$coefficients)) {
          coef_matrix <- as.matrix(model$coefficients)
          
          for(j in 1:ncol(coef_matrix)) {
            param_name <- colnames(coef_matrix)[j]
            chain <- as.vector(coef_matrix[,j])
            if (length(chain) < 2) next
            
            ci <- tryCatch({
              quantile(chain, probs = c(0.025, 0.975))
            }, error = function(e) c(NA, NA))
            
            mean_val   <- tryCatch(mean(chain), error = function(e) NA)
            median_val <- tryCatch(median(chain), error = function(e) NA)
            ess_val    <- tryCatch(as.numeric(effectiveSize(mcmc(chain))), error = function(e) NA)
            pip        <- tryCatch(mean(chain != 0), error = function(e) NA)
            
            geweke_pval <- tryCatch({
              geweke <- geweke.diag(mcmc(chain))
              2 * pnorm(-abs(geweke$z))
            }, error = function(e) NA)
            
            hw_pval <- tryCatch({
              hw <- heidel.diag(mcmc(chain))
              if (is.null(hw)) NA else hw[1, "pvalue"]
            }, error = function(e) NA)
            
            rl_stats <- tryCatch({
              if (length(unique(chain)) >= 10) {
                binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
                rl <- raftery.diag(binary_chain)
                c(cor(chain[-1], chain[-length(chain)]),
                  rl$resmatrix[1,"N"],
                  rl$resmatrix[1,"M"],
                  rl$resmatrix[1,"I"])
              } else {
                rep(NA, 4)
              }
            }, error = function(e) rep(NA, 4))
            
            coef_stats <- rbind(coef_stats, data.frame(
              Parameter     = param_name,
              Mean          = round(mean_val, 4),
              Median        = round(median_val, 4),
              CI_2.5        = round(ci[1], 4),
              CI_97.5       = round(ci[2], 4),
              ESS           = round(ess_val, 0),
              Post_Prob     = round(pip, 4),
              Geweke_Pval   = round(geweke_pval, 4),
              HW_Pval       = round(hw_pval, 4),
              RL_Autocorr   = round(rl_stats[1], 4),
              RL_Iterations = round(rl_stats[2], 0),
              RL_Burnin     = round(rl_stats[3], 0),
              RL_Dependence = round(rl_stats[4], 2),
              stringsAsFactors=FALSE
            ))
          }
        }
        
        if (nrow(coef_stats) > 0) {
          writeData(wb, dep_var, coef_stats, startRow = current_row)
          current_row <- current_row + nrow(coef_stats) + 3
        } else {
          writeData(wb, dep_var, "No coefficient data available", startRow = current_row)
          current_row <- current_row + 4
        }
        
      }, error = function(e) {
        writeData(wb, dep_var, paste("Error processing coefficients:", e$message), startRow = current_row)
        current_row <- current_row + 4
      })
    }
    
    setColWidths(wb, dep_var, cols = 1:13, widths = "auto")
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}


###############################################################################
## 6) format_window_range (unchanged)
###############################################################################
format_window_range <- function(window) {
  train_ranges <- c(
    "1-70", 
    "1-75",
    "1-80",
    "1-85",
    "1-90"
  )
  
  test_ranges <- c(
    "71-105",
    "76-105",
    "81-105",
    "86-105",
    "91-105"
  )
  
  paste(train_ranges[window], "as the training and the", test_ranges[window], "as the testing")
}


###############################################################################
## 7) create_excel_results
##    Computes ensemble SMAPEs then calls the above functions.
###############################################################################
create_excel_results <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  expanding_windows, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  all_results,         
  top_model_results,   
  hold_out_dataset     
) {
  file_path <- "EW_BW_2023Q1.xlsx"
  
  # Map delta_test_forecast to delta_holdout_forecast in influence results
  for (dep_var in names(influence_results)) {
    for (model_name in names(influence_results[[dep_var]])) {
      results_df <- influence_results[[dep_var]][[model_name]]
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        influence_results[[dep_var]][[model_name]]$delta_holdout_forecast <- 
          results_df$delta_test_forecast
      }
    }
  }
  
  # Get ensemble SMAPEs
  ensemble_smapes <- get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
  
  # Create prediction tables
  cat("Creating prediction tables...\n")
  wb <- create_enhanced_prediction_tables(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets = extended_test_sets, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_smapes = ensemble_smapes
  )
  
  # Add sensitivity analysis with proper spacing
  cat("Adding sensitivity analysis...\n")
  add_sensitivity_analysis(sensitivity_results, file_path)
  
  # Add influence analysis with proper spacing
  cat("Adding influence analysis...\n")
  add_influence_analysis_to_workbook(influence_results, file_path)
  
  # Add diagnostics
  cat("Adding diagnostics...\n")
  add_diagnostics_to_workbook(
    all_models = all_models, 
    expanding_windows = expanding_windows, 
    all_diagnostics = all_diagnostics, 
    all_looic = all_looic,
    bayesian_R2_results = bayesian_R2_results, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    ljung_box_results = ljung_box_results,
    file_path = file_path
  )
  
  return(paste("Results successfully written to", file_path))
}
###############################################################################
## 8) Example usage
###############################################################################
 result <- create_excel_results(
   all_evaluations = all_evaluations_bwew, 
   all_models = all_modelewbw, 
   extended_test_sets = extended_test_sets, 
   crps_scores = crps_scores, 
   lpd_scores = lpd_scores, 
   sensitivity_results = sensitivity_results, 
   influence_results = influence_results_parallel, 
   expanding_windows = expanding_windows, 
   all_diagnostics = all_diagnostics, 
   all_looic = all_looic_ew, 
   bayesian_R2_results = bayesian_R2_results,
   ljung_box_results = ljung_box_results,
   aggregate_smape_results = aggregate_smape_results,
   all_results = all_results,
   top_model_results = top_model_results,
   hold_out_dataset = hold_out_dataset
 )
 print(result)

```


# EWStep
# original dataset lagged for 24NA （ create DATASET from 1997Q1 to 2024Q3）
```{r}
# Create datasets for each dependent variable
dep_var_datasets <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Store in list
  dep_var_datasets[[dep_var_name]] <- dataset
}


# Take lag of the missing value of the quarter we want to predict 
dep_var_datasets_modified <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  # Repeat the first column (dep_var) and add it before the first column
  dataset <- cbind(dataset[, 1, drop = FALSE], dataset)  # Add first column as the first column again
  # Loop over columns starting from the second column
  for(col in 2:ncol(dataset)) {
    # Check if the last row value is missing
    if(is.na(dataset[nrow(dataset), col])) {
      # Apply lag: take the value from the previous row for all rows of the column
      dataset[, col] <- lag(dataset[, col], 1, default = NA)
      # Modify column name to indicate lag if it was modified
      new_col_name <- paste0(colnames(dataset)[col], "_lag1")
      colnames(dataset)[col] <- new_col_name
    }
  }
  # Store in list
  dep_var_datasets_modified[[dep_var_name]] <- dataset
}

# Delete before 1997Q1
dep_var_datasets_modified <- lapply(dep_var_datasets_modified, function(x) {
  x[-(1:8), ]#period
})

# Extract last row from each dataset
last_row <- lapply(dep_var_datasets_modified, function(x) {
  x[nrow(x), ]
})

# Remove last row from each dataset
dep_var_datasets <- lapply(dep_var_datasets_modified, function(x) {
  x[-nrow(x), ]
})

# Verify dimensions
cat("Number of rows in main datasets:\n")
sapply(dep_var_datasets, nrow)
cat("\nNumber of columns in main datasets:\n")
sapply(dep_var_datasets, ncol)
print(dep_var_datasets[[1]][nrow(dep_var_datasets[[1]]), ])  # For the first dataset
#period
create_quarter_four_from_last_row <- function(dep_var_datasets, dep_var_sets) {
  quarter_four <- list()
  
  for (dep_var_name in names(dep_var_sets)) {
    # Get the original dep_var column name
    original_dep_var_name <- dep_var_sets[[dep_var_name]]$dep_var
    
    # Extract last row from dataset
    last_row_data <- dep_var_datasets[[dep_var_name]][nrow(dep_var_datasets[[dep_var_name]]), , drop = FALSE]
    
    # Separate and rename dep_var column
    dep_var_column <- last_row_data[, original_dep_var_name, drop = FALSE]
    colnames(dep_var_column) <- "dep_var"
    
    # Get the independent variables
    indep_vars_data <- last_row_data[, setdiff(colnames(last_row_data), original_dep_var_name), drop = FALSE]
    
    # Combine into final row format
    quarter_four[[dep_var_name]] <- cbind(dep_var_column, indep_vars_data)
    
    # Remove the last row from the dataset
    dep_var_datasets[[dep_var_name]] <- dep_var_datasets[[dep_var_name]][-nrow(dep_var_datasets[[dep_var_name]]), ]
  }
  
  return(list(quarter_four = quarter_four, dep_var_datasets = dep_var_datasets))
}

# Usage:
result <- create_quarter_four_from_last_row(dep_var_datasets, dep_var_sets)
quarter_four <- result$quarter_four
dep_var_datasets <- result$dep_var_datasets

# Extracting rows 80-87 from each dataset in dep_var_datasets
hold_out_dataset <- lapply(dep_var_datasets, function(df) {
  df[105:111, ]#period
})

# Create a list to store the updated datasets with quarter_four at the end
holdout_with_last_row <- list()

# Iterate through each dep_var_name and append quarter_four at the end
for(dep_var_name in names(hold_out_dataset)) {
  
  # Ensure column names match between quarter_four and the data frame in hold_out_dataset
  if (!all(names(quarter_four[[dep_var_name]]) == names(hold_out_dataset[[dep_var_name]]))) {
    # Manually adjust the column names of quarter_four to match hold_out_dataset
    names(quarter_four[[dep_var_name]]) <- names(hold_out_dataset[[dep_var_name]])
  }
  
  # Combine the selected holdout (80-87 rows) and the corresponding last row from quarter_four
  holdout_with_last_row[[dep_var_name]] <- rbind(
    hold_out_dataset[[dep_var_name]],
    quarter_four[[dep_var_name]]
  )
}

# Update hold_out_dataset with the modified datasets
hold_out_dataset <- holdout_with_last_row

hold_out_dataset 

hold_out_period <- 105:111#period

# Function to exclude the hold-out period from the dataset
exclude_hold_out_1 <- function(df) {
  # Exclude the rows that are in the hold-out period (81-88)
  df_no_hold <- df[!rownames(df) %in% hold_out_period, ]
  return(df_no_hold)
}

#--------------------------------------------------------------
# This is the dataset without the hold out period 
Indice_testing_dataset <- lapply(dep_var_datasets, function(df) {
  # Exclude rows 81-88 from the dataset and return the modified dataset
  exclude_hold_out_1(df)
})

dep_var_datasets = Indice_testing_dataset
# Verify the result
Indice_testing_dataset
# Create rolling windows with consistent sizes (4-5 windows)
# Create expanding windows with training starting from rows 1-70
expanding_windows <- list()

for(dep_var_name in names(dep_var_datasets)) {
  current_data <- dep_var_datasets[[dep_var_name]]
  dataset_windows <- list()
  
  # Get total number of rows in the dataset
  total_rows <- nrow(current_data)
  
  # Initial training window: rows 1-70
  train_end <- 70
  window_index <- 1
  
  # Maximum training window size is 95 rows
  max_train_rows <- 95
  
  # Create windows while:
  # 1. The training window doesn't exceed 95 rows
  # 2. There's enough data for testing (at least 10 rows for testing)
  while (train_end <= max_train_rows && train_end < total_rows - 9) {
    train_data <- current_data[1:train_end, ]
    test_data <- current_data[(train_end + 1):total_rows, ]
    
    dataset_windows[[window_index]] <- list(train = train_data, test = test_data)
    
    # Expand training window by 5 rows for next iteration
    train_end <- train_end + 5
    window_index <- window_index + 1
    
    # Break if we would exceed 95 rows on the next iteration
    if (train_end > max_train_rows) {
      break
    }
  }
  
  expanding_windows[[dep_var_name]] <- dataset_windows
}

```

## Step selection
```{r}
# Stepwise var selection
# Load required library for BSTS (if not already loaded)
library(bsts)

# Custom LOOIC calculation function
calculate_looic <- function(model, train_data) {
  n_train <- nrow(train_data)
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  looic <- (-2 * sum(loo_liks)) / n_train
  return(looic)
}

# Revised stepwise_selection function starting with forward selection instead of backward.
# The function alternates between forward and backward phases. In both phases, 
# candidate improvement is measured relative to the best LOOIC so far.
stepwise_selection <- function(train_data, min_vars = 15, significance_threshold = 0.00004, max_iterations = 40) {
  set.seed(1234567)
  
  # Separate response (assumed in first column) and predictors.
  y <- train_data[, 1]
  X <- train_data[, -1]
  all_predictors <- colnames(X)
  
  # Check and ensure y is numeric
  if (!is.numeric(y)) {
    cat("Warning: Response variable is not numeric. Converting to numeric...\n")
    y <- as.numeric(as.character(y))
    cat("Conversion complete.\n")
  }
  
  # Set up state space specification for the BSTS model.
  state_spec <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  state_spec <- AddLocalLinearTrend(state_spec, y)
  
  # Helper: Function to fit the BSTS model using a given set of predictors.
  fit_model <- function(predictors) {
    set.seed(12345678)
    if (length(predictors) == 0) {
      model_data <- data.frame(y = y)
      formula <- y ~ 1
    } else {
      model_data <- data.frame(y = y, X[, predictors, drop = FALSE])
      formula <- as.formula(paste("y ~", paste(predictors, collapse = " + ")))
    }
    bsts(formula, state.specification = state_spec, niter = 10000, data = model_data)
  }
  
  # Start with an empty set of predictors
  cat("Starting with an empty set of predictors\n")
  selected_vars <- character(0)
  current_model <- fit_model(selected_vars)
  current_looic <- calculate_looic(current_model, train_data)
  
  cat("Initial empty model LOOIC:", current_looic, "\n")
  
  # Save best model so far.
  best_model <- current_model
  best_looic <- current_looic
  best_selected_vars <- selected_vars
  
  iteration <- 1
  improved_in_forward <- TRUE
  improved_in_backward <- TRUE
  
  # Stepwise procedure: alternate forward then backward until no improvement in either phase.
  while (iteration <= max_iterations && (improved_in_forward || improved_in_backward)) {
    cat("\nIteration", iteration, "\n")
    
    # ----- FORWARD SELECTION -----
    improved_in_forward <- FALSE
    # All predictors not currently in the model are available for addition.
    available_predictors <- setdiff(all_predictors, selected_vars)
    if (length(available_predictors) > 0) {
      cat("Forward selection phase:\n")
      addition_candidates <- list()
      
      # Try adding each available predictor one at a time.
      for (var in available_predictors) {
        candidate_vars <- c(selected_vars, var)
        try({
          candidate_model <- fit_model(candidate_vars)
          candidate_looic <- calculate_looic(candidate_model, train_data)
          # Compare candidate improvement relative to the best LOOIC so far.
          improvement_pct <- (best_looic - candidate_looic) / best_looic
          addition_candidates[[var]] <- list(var = var, model = candidate_model, 
                                             looic = candidate_looic, improvement_pct = improvement_pct)
          cat("  Adding", var, ": LOOIC =", candidate_looic, 
              ", improvement =", round(improvement_pct * 100, 4), "%\n")
        }, silent = TRUE)
      }
      
      if (length(addition_candidates) > 0) {
        valid_additions <- Filter(function(x) x$improvement_pct > significance_threshold, addition_candidates)
        if (length(valid_additions) > 0) {
          best_addition <- valid_additions[[which.min(sapply(valid_additions, function(x) x$looic))]]
          cat("Adding predictor:", best_addition$var, 
              "improves LOOIC from", best_looic, "to", best_addition$looic, 
              "with improvement =", round(best_addition$improvement_pct * 100, 4), "%\n")
          selected_vars <- c(selected_vars, best_addition$var)
          current_model <- best_addition$model
          current_looic <- best_addition$looic
          best_model <- current_model
          best_looic <- current_looic
          best_selected_vars <- selected_vars
          improved_in_forward <- TRUE
        } else {
          cat("No predictor addition yielded improvement greater than", significance_threshold * 100, "%\n")
        }
      }
    } else {
      cat("Forward selection skipped: no available predictors to add.\n")
    }
    
    # ----- BACKWARD SELECTION -----
    improved_in_backward <- FALSE
    if (length(selected_vars) > min_vars) {
      cat("Backward selection phase:\n")
      removal_candidates <- list()
      
      # Try removing each predictor one at a time.
      for (i in seq_along(selected_vars)) {
        candidate_vars <- selected_vars[-i]
        if (length(candidate_vars) < min_vars) next
        var_removed <- selected_vars[i]
        try({
          candidate_model <- fit_model(candidate_vars)
          candidate_looic <- calculate_looic(candidate_model, train_data)
          # Compare candidate improvement relative to best LOOIC so far.
          improvement_pct <- (best_looic - candidate_looic) / best_looic
          removal_candidates[[var_removed]] <- list(var = var_removed, model = candidate_model, 
                                                   looic = candidate_looic, improvement_pct = improvement_pct, 
                                                   index = i)
          cat("  Removing", var_removed, ": LOOIC =", candidate_looic, 
              ", improvement =", round(improvement_pct * 100, 4), "%\n")
        }, silent = TRUE)
      }
      
      if (length(removal_candidates) > 0) {
        valid_removals <- Filter(function(x) x$improvement_pct > significance_threshold, removal_candidates)
        if (length(valid_removals) > 0) {
          best_removal <- valid_removals[[which.min(sapply(valid_removals, function(x) x$looic))]]
          cat("Removing predictor:", best_removal$var, 
              "improves LOOIC from", best_looic, "to", best_removal$looic, 
              "with improvement =", round(best_removal$improvement_pct * 100, 4), "%\n")
          selected_vars <- selected_vars[-best_removal$index]
          current_model <- best_removal$model
          current_looic <- best_removal$looic
          best_model <- current_model
          best_looic <- current_looic
          best_selected_vars <- selected_vars
          improved_in_backward <- TRUE
        } else {
          cat("No predictor removal yielded improvement greater than", significance_threshold * 100, "%\n")
        }
      }
    } else {
      cat("Backward selection skipped: number of predictors equals minimum allowed.\n")
    }
    
    if (!improved_in_forward && !improved_in_backward) {
      cat("No further improvements in forward or backward phases. Terminating stepwise selection.\n")
      break
    }
    
    iteration <- iteration + 1
  }
  
  # Ensure the minimum number of variables is met.
  if (length(selected_vars) < min_vars) {
    cat("Selected variables fewer than", min_vars, "- adding predictors to meet minimum requirement.\n")
    
    # Calculate correlations between response and predictors not already selected
    corrs <- c()
    for (var in setdiff(all_predictors, selected_vars)) {
      corr_val <- try(abs(cor(y, X[[var]], use = "pairwise.complete.obs")), silent = TRUE)
      if (!inherits(corr_val, "try-error") && !is.na(corr_val)) {
        corrs[var] <- corr_val
      } else {
        corrs[var] <- 0  # Set to 0 if correlation can't be calculated
      }
    }
    
    # Sort predictors by absolute correlation (highest first)
    corrs_sorted <- sort(corrs, decreasing = TRUE)
    
    # Add most correlated predictors until min_vars is reached
    num_to_add <- min_vars - length(selected_vars)
    additional_vars <- names(head(corrs_sorted, num_to_add))
    
    cat("Adding most correlated predictors:", paste(additional_vars, collapse = ", "), "\n")
    selected_vars <- c(selected_vars, additional_vars)
    
    # Refit the model with the new set of predictors
    current_model <- fit_model(selected_vars)
    current_looic <- calculate_looic(current_model, train_data)
    
    cat("New LOOIC after adding correlated predictors:", current_looic, "\n")
  }
  
  excluded_vars <- setdiff(all_predictors, selected_vars)
  
  cat("\nFinal model LOOIC:", current_looic, "\n")
  cat("Selected predictors:", paste(selected_vars, collapse = ", "), "\n")
  if (length(excluded_vars) > 0) {
    cat("Excluded predictors:", paste(excluded_vars, collapse = ", "), "\n")
  }
  
  return(list(
    model = current_model,
    selected_vars = selected_vars,
    looic = current_looic,
    excluded_vars = excluded_vars
  ))
}

# -------------------------------
# Function to Run Stepwise Selection for Each Dependent Variable
# -------------------------------
run_best_selection <- function(best_models, expanding_windows, min_vars = 15) {
  all_models_ewstep <- list()
  
  for (dep_var in names(best_models)) {
    cat("\n\n==== Variable Selection for", dep_var, "====\n")
    
    # Skip if this variable doesn't exist in expanding_windows
    if (!dep_var %in% names(expanding_windows)) {
      cat("ERROR: No expanding window data found for", dep_var, ". Skipping this variable.\n")
      next
    }
    
    # Use only the top model based on sMAPE
    model_info <- best_models[[dep_var]]
    if ("best_window_1" %in% names(model_info)) {
      best_window <- model_info$best_window_1
      best_method <- model_info$best_method_1
      best_smape <- model_info$best_smape_1
    } else if ("best_window" %in% names(model_info)) {
      best_window <- model_info$best_window
      best_method <- model_info$best_method
      best_smape <- model_info$best_smape
    } else {
      best_window <- 1
      best_method <- "unknown"
      best_smape <- NA
    }
    
    cat("\n--- Using best model: Window", best_window, "with", 
        ifelse(is.null(best_method), "unknown", best_method), 
        "method (sMAPE =", 
        ifelse(is.null(best_smape) || !is.numeric(best_smape), "NA", round(best_smape, 4)), 
        ") for", dep_var, "---\n")
    
    # Access by numeric index, making sure it's within range
    if (best_window < 1 || best_window > length(expanding_windows[[dep_var]])) {
      cat("Window index", best_window, "out of range for", dep_var, "\n")
      cat("Available indices: 1 to", length(expanding_windows[[dep_var]]), "\n")
      # Use first window as fallback
      best_window <- 1
      cat("Using window index 1 instead\n")
    }
    
    # Get the training data using numeric index
    train_data <- expanding_windows[[dep_var]][[best_window]]$train
    
    # Check if train_data exists and has the right structure
    if (is.null(train_data) || !is.data.frame(train_data) || ncol(train_data) < 2) {
      cat("Invalid training data for", dep_var, "at window index", best_window, "\n")
      next
    }
    
    # Run stepwise selection with error handling
    tryCatch({
      selection_result <- stepwise_selection(train_data, min_vars = min_vars)
      all_models_ewstep[[dep_var]] <- selection_result
      
      cat("Selected predictors for", dep_var, ":", paste(selection_result$selected_vars, collapse = ", "), "\n")
      cat("LOOIC:", selection_result$looic, "\n")
      
      if (length(selection_result$excluded_vars) > 0) {
        cat("Excluded predictors:", paste(selection_result$excluded_vars, collapse = ", "), "\n")
      }
    }, error = function(e) {
      cat("Error in stepwise selection for", dep_var, ":", conditionMessage(e), "\n")
    })
  }
  
  return(all_models_ewstep)
}

# Function to create a detailed report of selected variables.
report_selected_variables <- function(models) {
  results_df <- data.frame(
    DependentVariable = character(),
    SelectedVariables = character(),
    NumSelectedVars = integer(),
    LOOIC = numeric(),
    stringsAsFactors = FALSE
  )
  
  cat("\n============================================================\n")
  cat("STEPWISE SELECTION RESULTS: SELECTED VARIABLES BY MODEL\n")
  cat("============================================================\n\n")
  
  for (dep_var_name in names(models)) {
    model_result <- models[[dep_var_name]]
    if (length(model_result$selected_vars) > 0) {
      selected_vars_str <- paste(model_result$selected_vars, collapse = ", ")
    } else {
      selected_vars_str <- "None (intercept-only model)"
    }
    
    results_df <- rbind(results_df, data.frame(
      DependentVariable = dep_var_name,
      SelectedVariables = selected_vars_str,
      NumSelectedVars = length(model_result$selected_vars),
      LOOIC = model_result$looic,
      stringsAsFactors = FALSE
    ))
    
    cat(paste0("Dependent Variable: ", dep_var_name, "\n"))
    cat(paste0("Number of Selected Variables: ", length(model_result$selected_vars), "\n"))
    cat(paste0("LOOIC: ", round(model_result$looic, 4), "\n"))
    cat("Selected Variables:\n")
    if (length(model_result$selected_vars) > 0) {
      for (i in 1:length(model_result$selected_vars)) {
        cat(paste0("  ", i, ". ", model_result$selected_vars[i], "\n"))
      }
    } else {
      cat("  None (intercept-only model)\n")
    }
    cat("\n------------------------------------------------------------\n\n")
  }
  
  return(results_df)
}

# Function to create a comparison table of variable selection across models.
create_variable_comparison <- function(models) {
  all_vars <- c()
  for (dep_var_name in names(models)) {
    all_vars <- union(all_vars, models[[dep_var_name]]$selected_vars)
  }
  
  comparison_matrix <- matrix(0, nrow = length(all_vars), ncol = length(models))
  rownames(comparison_matrix) <- all_vars
  colnames(comparison_matrix) <- names(models)
  
  for (dep_var_name in names(models)) {
    selected_vars <- models[[dep_var_name]]$selected_vars
    for (var in selected_vars) {
      comparison_matrix[var, dep_var_name] <- 1
    }
  }
  
  comparison_df <- as.data.frame(comparison_matrix)
  comparison_df$TotalModels <- rowSums(comparison_matrix)
  comparison_df <- comparison_df[order(-comparison_df$TotalModels), ]
  
  cat("\n============================================================\n")
  cat("VARIABLE SELECTION COMPARISON ACROSS MODELS\n")
  cat("============================================================\n\n")
  print(comparison_df)
  
  return(comparison_df)
}

# Function to generate variable importance based on selection frequency.
generate_variable_importance <- function(comparison_df) {
  model_cols <- setdiff(colnames(comparison_df), "TotalModels")
  num_models <- length(model_cols)
  
  importance_df <- data.frame(
    Variable = rownames(comparison_df),
    ModelCount = comparison_df$TotalModels,
    Percentage = round(comparison_df$TotalModels / num_models * 100, 1),
    stringsAsFactors = FALSE
  )
  
  importance_df <- importance_df[order(-importance_df$Percentage), ]
  
  cat("\n============================================================\n")
  cat("VARIABLE IMPORTANCE BASED ON SELECTION FREQUENCY\n")
  cat("============================================================\n\n")
  print(importance_df)
  
  return(importance_df)
}

# Example usage:
 best_model_ewST <- find_best_models(all_evaluations_allew)
 final_models_ewST <- run_best_selection(best_model_ewST, expanding_windows, min_vars = 15)
```




```{r}
# Load library for Excel export
library(openxlsx)

# Create a new workbook
wb <- createWorkbook()

# Loop over each dependent variable model and export excluded predictors
for (dep_var in names(final_models_ewST)) {
  model_info <- final_models_ewST[[dep_var]]
  
  # Extract excluded predictors
  excluded_vars <- model_info$excluded_vars
  
  # Create a data frame
  excluded_df <- data.frame(
    ExcludedPredictors = excluded_vars,
    stringsAsFactors = FALSE
  )
  
  # Add a worksheet named after the dependent variable
  addWorksheet(wb, sheetName = dep_var)
  writeData(wb, sheet = dep_var, x = excluded_df)
}

# Save the workbook
saveWorkbook(wb, file = "Removed_pre_StepEW_2023.xlsx", overwrite = TRUE)

cat("Excluded predictors saved to 'excluded_predictors_stepwise.xlsx'\n")

```


## New dataset use removed colm number
```{r}
# Create datasets for each dependent variable
dep_var_datasets <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Store in list
  dep_var_datasets[[dep_var_name]] <- dataset
}


# Take lag of the missing value of 2024 Q1
dep_var_datasets_modified <- list()
for(dep_var_name in names(dep_var_sets)) {
  # Get variables for this dependent variable
  dep_var <- dep_var_sets[[dep_var_name]]$dep_var
  ivs <- dep_var_sets[[dep_var_name]]$ivs
  
  # Select columns for this dataset
  dataset <- mydata[, c(dep_var, ivs)]
  
  # Repeat the first column (dep_var) and add it before the first column
  dataset <- cbind(dataset[, 1, drop = FALSE], dataset)  # Add first column as the first column again
  
  # Loop over columns starting from the second column
  for(col in 2:ncol(dataset)) {
    # Check if the last row value is missing
    if(is.na(dataset[nrow(dataset), col])) {
      # Apply lag: take the value from the previous row for all rows of the column
      dataset[, col] <- lag(dataset[, col], 1, default = NA)
      
      # Modify column name to indicate lag if it was modified
      new_col_name <- paste0(colnames(dataset)[col], "_lag1")
      colnames(dataset)[col] <- new_col_name
    }
  }
  
  # Store in list
  dep_var_datasets_modified[[dep_var_name]] <- dataset
}

# Delete 1996Q1
dep_var_datasets_modified <- lapply(dep_var_datasets_modified, function(x) {
  x[-(1:8), ]#period
})

# Extract last row from each dataset
last_row <- lapply(dep_var_datasets_modified, function(x) {
  x[nrow(x), ]
})

# Remove last row from each dataset
dep_var_datasets <- lapply(dep_var_datasets_modified, function(x) {
  x[-nrow(x), ]
})

```


```{r}
# Function to construct datasets based on selected variables from stepwise selection
construct_selected_datasets <- function(final_models, full_datasets) {
  selected_datasets <- list()
  
  for (dep_var in names(final_models)) {
    # Get the original dataset
    dataset <- full_datasets[[dep_var]]
    
    # Get selected variables from stepwise selection results
    selected_vars <- final_models[[dep_var]]$selected_vars
    
    # Find column indices of selected variables
    selected_indices <- c()
    for (var in selected_vars) {
      # Try exact match first
      exact_match <- which(colnames(dataset) == var)
      if (length(exact_match) > 0) {
        selected_indices <- c(selected_indices, exact_match)
      } else {
        # If exact match fails, try with potential lag suffix if it exists
        var_lag <- paste0(var, "_lag1")
        lag_match <- which(colnames(dataset) == var_lag)
        if (length(lag_match) > 0) {
          selected_indices <- c(selected_indices, lag_match)
          cat("Using lagged variable", var_lag, "for", var, "in", dep_var, "\n")
        } else {
          # Try partial match as last resort
          partial_matches <- grep(var, colnames(dataset), fixed=TRUE)
          if (length(partial_matches) > 0) {
            selected_indices <- c(selected_indices, partial_matches[1])  # Use first partial match
            cat("Warning: Using partial match for", var, "in", dep_var, ":", colnames(dataset)[partial_matches[1]], "\n")
          } else {
            cat("Warning: Variable", var, "not found in", dep_var, "dataset\n")
          }
        }
      }
    }
    
    # Ensure no duplicates in selected indices
    selected_indices <- unique(selected_indices)
    
    # Create new dataset with dependent variable (first column) and selected predictors
    if (length(selected_indices) > 0) {
      # First column is the dependent variable
      new_dataset <- dataset[, c(1, selected_indices)]
      
      cat("\nReconstructed", dep_var, "dataset:\n")
      cat("Original dimensions:", nrow(dataset), "rows,", ncol(dataset), "columns\n")
      cat("New dimensions:", nrow(new_dataset), "rows,", ncol(new_dataset), "columns\n")
      cat("Selected columns:", paste(colnames(new_dataset)[-1], collapse=", "), "\n")
      
      selected_datasets[[dep_var]] <- new_dataset
    } else {
      cat("Warning: No columns were selected for", dep_var, "\n")
      # Include only the dependent variable column
      selected_datasets[[dep_var]] <- dataset[, 1, drop=FALSE]
    }
  }
  
  return(selected_datasets)
}

 dep_var_datasets <- construct_selected_datasets(final_models_ewST , dep_var_datasets)
 quarter_four <- construct_selected_datasets(final_models_ewST , quarter_four)
```
```{r}
# Delete the last row from each dataframe in dep_var_datasets
dep_var_datasets <- lapply(dep_var_datasets, function(df) {
  # Get the number of rows in the dataframe
  n_rows <- nrow(df)
  
  # Return all rows except the last one
  return(df[1:(n_rows-1), ])
})

# Verify the result
dep_var_datasets
```

# Expanding Window
```{r}

# Create expanding windows with training starting from rows 1-70
expanding_windows <- list()

for(dep_var_name in names(dep_var_datasets)) {
  current_data <- dep_var_datasets[[dep_var_name]]
  dataset_windows <- list()
  
  # Get total number of rows in the dataset
  total_rows <- nrow(current_data)
  
  # Initial training window: rows 1-70
  train_end <- 70
  window_index <- 1
  
  # Maximum training window size is 95 rows
  max_train_rows <- 95
  
  # Create windows while:
  # 1. The training window doesn't exceed 95 rows
  # 2. There's enough data for testing (at least 10 rows for testing)
  while (train_end <= max_train_rows && train_end < total_rows - 9) {
    train_data <- current_data[1:train_end, ]
    test_data <- current_data[(train_end + 1):total_rows, ]
    
    dataset_windows[[window_index]] <- list(train = train_data, test = test_data)
    
    # Expand training window by 5 rows for next iteration
    train_end <- train_end + 5
    window_index <- window_index + 1
    
    # Break if we would exceed 95 rows on the next iteration
    if (train_end > max_train_rows) {
      break
    }
  }
  
  expanding_windows[[dep_var_name]] <- dataset_windows
}
expanding_windows$net_interest_income
```

## fit the model

```{r}
set.seed(1234)
fit_bsts_model <- function(train_data) {
  set.seed(1234)
  # (first column): dep
  y <- train_data[, 1]
  # (all columns except first): IVs
  X <- train_data[, -1]
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y)
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Model
  model <- bsts(formula,
                state.specification = ss,
                niter = 10000,
                data = as.data.frame(cbind(y = y, X)))
  
  return(model)
}

fit_all_windows <- function(dep_var_splits) {
  models <- list()
  for(i in 1:5) {
    models[[i]] <- fit_bsts_model(dep_var_splits[[i]]$train)
  }
  return(models)
}
all_modelewstep <- list()
# Net Interest Income
all_modelewstep$net_interest_income <- fit_all_windows(expanding_windows$net_interest_income)
# Non-Interest Income
all_modelewstep$non_interest_income <- fit_all_windows(expanding_windows$non_interest_income)
# Provision Credit Loss
all_modelewstep$provision_credit_loss <- fit_all_windows(expanding_windows$provision_credit_loss)
# Non-Interest Expense
all_modelewstep$non_interest_expense <- fit_all_windows(expanding_windows$non_interest_expense)
```

##error

```{r}
# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  # Handle potential length mismatches
  min_length <- min(length(actual), length(predicted))
  if (length(actual) != length(predicted)) {
    warning("Length mismatch between actual and predicted values. Using the first ", min_length, " elements.")
    actual <- actual[1:min_length]
    predicted <- predicted[1:min_length]
  }
  
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Updated evaluate_predictions function for expanding windows that handles missing values
evaluate_predictions <- function(model, full_data, window_num, expanding_windows, dep_var_name) {
  # Get the specific window
  window <- expanding_windows[[dep_var_name]][[window_num]]
  
  # Get test indices from the window structure
  test_indices <- as.numeric(rownames(window$test))
  
  # Extract actual values for testing
  actual_combined <- full_data[test_indices, 1]
  
  # Prepare test data for prediction
  pred_data <- as.data.frame(full_data[test_indices, -1, drop = FALSE])
  
  # Check for missing values in prediction data
  missing_rows <- apply(pred_data, 1, function(x) any(is.na(x)))
  if(any(missing_rows)) {
    warning("Some rows in test data contain missing values and will be excluded from evaluation.")
    # Filter out rows with missing values
    pred_data <- pred_data[!missing_rows, , drop = FALSE]
    actual_combined <- actual_combined[!missing_rows]
    # Update test indices
    test_indices <- test_indices[!missing_rows]
  }
  
  # Make predictions with error handling
  tryCatch({
    pred <- predict(model, newdata = pred_data, burn = 100)
    
    # Handle potential issues with prediction distributions
    if(is.null(pred$distribution) || ncol(pred$distribution) == 0) {
      stop("Prediction distribution is empty or null")
    }
    
    mean_predictions <- colMeans(pred$distribution)
    median_predictions <- apply(pred$distribution, 2, median)
    
    # Check for length match
    if(length(mean_predictions) != length(actual_combined)) {
      warning("Length mismatch between predictions and actual values. Adjusting...")
      min_length <- min(length(mean_predictions), length(actual_combined))
      mean_predictions <- mean_predictions[1:min_length]
      median_predictions <- median_predictions[1:min_length]
      actual_combined <- actual_combined[1:min_length]
    }
    
    # Calculate errors
    errors_mean <- calculate_errors(actual_combined, mean_predictions)
    errors_median <- calculate_errors(actual_combined, median_predictions)
    
    return(list(
      actual = actual_combined,
      predicted_mean = mean_predictions,
      predicted_median = median_predictions,
      errors_mean = errors_mean,
      errors_median = errors_median,
      test_indices = test_indices
    ))
  }, error = function(e) {
    warning("Error in prediction: ", e$message)
    # Return empty results for this window
    empty_errors <- data.frame(MSE = NA, MAE = NA, MAPE = NA, SMAPE = NA, MASE = NA, OWA = NA)
    return(list(
      actual = actual_combined,
      predicted_mean = rep(NA, length(actual_combined)),
      predicted_median = rep(NA, length(actual_combined)),
      errors_mean = empty_errors,
      errors_median = empty_errors,
      test_indices = test_indices
    ))
  })
}

# Revised function to evaluate all windows
evaluate_all_windows <- function(models, full_data, expanding_windows, dep_var_name) {
  results <- list()
  
  # Get the number of windows (should match number of models)
  num_windows <- min(length(models), length(expanding_windows[[dep_var_name]]))
  
  for(i in 1:num_windows) {
    # Skip if the model is NULL or invalid
    if(is.null(models[[i]])) {
      warning("Model ", i, " is NULL. Skipping evaluation.")
      results[[i]] <- NULL
      next
    }
    
    # Evaluate predictions with the expanding window structure
    results[[i]] <- evaluate_predictions(
      models[[i]], 
      full_data,
      i,
      expanding_windows,
      dep_var_name
    )
  }
  
  return(results)
}

# Function to print summary tables
print_summary_tables <- function(all_evaluations) {
  for(var_name in names(all_evaluations)) {
    cat("\nResults for", var_name, "\n")
    
    # Mean-based metrics
    cat("\nMean-based metrics:\n")
    window_results_mean <- data.frame()
    for(i in 1:length(all_evaluations[[var_name]])) {
      if(!is.null(all_evaluations[[var_name]][[i]])) {
        window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
        window_errors$Window <- i
        window_results_mean <- rbind(window_results_mean, window_errors)
      }
    }
    if(nrow(window_results_mean) > 0) {
      print(round(window_results_mean[, c("Window", "MSE", "MAE", "MAPE", "SMAPE", "MASE", "OWA")], 4))
    } else {
      cat("No valid results found for mean metrics.\n")
    }
    
    # Median-based metrics
    cat("\nMedian-based metrics:\n")
    window_results_median <- data.frame()
    for(i in 1:length(all_evaluations[[var_name]])) {
      if(!is.null(all_evaluations[[var_name]][[i]])) {
        window_errors <- all_evaluations[[var_name]][[i]]$errors_median
        window_errors$Window <- i
        window_results_median <- rbind(window_results_median, window_errors)
      }
    }
    if(nrow(window_results_median) > 0) {
      print(round(window_results_median[, c("Window", "MSE", "MAE", "MAPE", "SMAPE", "MASE", "OWA")], 4))
    } else {
      cat("No valid results found for median metrics.\n")
    }
  }
}

# Example of how to run the evaluation with the expanding window approach and backward elimination
# Run evaluations for all dependent variables
all_evaluations_ewstep <- list()

# Net Interest Income
all_evaluations_ewstep$net_interest_income <- evaluate_all_windows(
  all_modelewstep$net_interest_income, 
  dep_var_datasets$net_interest_income,
  expanding_windows,
  "net_interest_income"
)

# Non-Interest Income
all_evaluations_ewstep$non_interest_income <- evaluate_all_windows(
  all_modelewstep$non_interest_income, 
  dep_var_datasets$non_interest_income,
  expanding_windows,
  "non_interest_income"
)

# Provision Credit Loss
all_evaluations_ewstep$provision_credit_loss <- evaluate_all_windows(
  all_modelewstep$provision_credit_loss, 
  dep_var_datasets$provision_credit_loss,
  expanding_windows,
  "provision_credit_loss"
)

# Non-Interest Expense
all_evaluations_ewstep$non_interest_expense <- evaluate_all_windows(
  all_modelewstep$non_interest_expense, 
  dep_var_datasets$non_interest_expense,
  expanding_windows,
  "non_interest_expense"
)

# Print summary tables
print_summary_tables(all_evaluations_ewstep)
```

```{r}
# Calculate model weights
model_weights_ewstep <- calculate_model_weights(all_evaluations_ewstep)
```

```{r}
# Fixed function to generate prediction tables and SMAPE for all windows and all variables using holdout data
predict_all_windows_variables <- function(all_modelewstep, hold_out_dataset) {
  # Initialize results storage
  smape_results <- list()
  
  # Loop through each window
  for(window_num in 1:5) {
    cat("\n================================================================\n")
    cat("WINDOW", window_num, "PREDICTIONS (HOLDOUT PERIOD)\n")
    cat("================================================================\n")
    
    # Loop through each variable
    for(var_name in names(all_modelewstep)) {
      # Skip if model or holdout data doesn't exist
      if (!(var_name %in% names(all_modelewstep)) || 
          length(all_modelewstep[[var_name]]) < window_num || 
          !(var_name %in% names(hold_out_dataset))) {
        cat("Skipping", var_name, "for window", window_num, "(data not available)\n")
        next
      }
      
      # Get the model for this window and variable
      model <- all_modelewstep[[var_name]][[window_num]]
      
      # Get the holdout dataset for this variable
      holdout_data <- hold_out_dataset[[var_name]]
      
      # Create period labels (80-87 plus Q1)
      period_labels <- c(as.character(105:111), "Q1")#period
      
      # Get actual values and predictors
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Make predictions
      pred <- predict(model, newdata = pred_data, burn = 100)
      mean_predictions <- colMeans(pred$distribution)
      median_predictions <- apply(pred$distribution, 2, median)
      
      # Calculate errors
      mean_errors <- mean_predictions - actual_values
      median_errors <- median_predictions - actual_values
      
      # Calculate SMAPE for mean predictions
      mean_smape <- mean(2 * abs(actual_values - mean_predictions) / 
                       (abs(actual_values) + abs(mean_predictions))) * 100
      
      # Calculate SMAPE for median predictions
      median_smape <- mean(2 * abs(actual_values - median_predictions) / 
                         (abs(actual_values) + abs(median_predictions))) * 100
      
      # Store SMAPE results
      if (is.null(smape_results[[var_name]])) {
        smape_results[[var_name]] <- data.frame(
          Window = numeric(),
          Mean_SMAPE = numeric(),
          Median_SMAPE = numeric()
        )
      }
      
      smape_results[[var_name]] <- rbind(smape_results[[var_name]], 
                                      data.frame(
                                        Window = window_num,
                                        Mean_SMAPE = mean_smape,
                                        Median_SMAPE = median_smape
                                      ))
      
      # Display SMAPE for this window and variable
      cat("\n----------------------------------------------------------------\n")
      cat("SMAPE FOR WINDOW", window_num, "PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      cat("Mean SMAPE:", round(mean_smape, 4), "%\n")
      cat("Median SMAPE:", round(median_smape, 4), "%\n")
      
      # Display mean table - FIX: Create numeric data first, then add Period as a separate column
      cat("\n----------------------------------------------------------------\n")
      cat("TABLE: WINDOW", window_num, "MEAN PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      
      # Create numeric results first
      numeric_results <- data.frame(
        Actual = actual_values,
        Predicted = mean_predictions,
        Error = mean_errors
      )
      
      # Then display with Period column
      cat("Period\tActual\tPredicted\tError\n")
      for (i in 1:length(period_labels)) {
        cat(period_labels[i], "\t", 
            round(numeric_results$Actual[i], 4), "\t", 
            round(numeric_results$Predicted[i], 4), "\t", 
            round(numeric_results$Error[i], 4), "\n")
      }
      
      # Display median table - Use same approach
      cat("\n----------------------------------------------------------------\n")
      cat("TABLE: WINDOW", window_num, "MEDIAN PREDICTIONS FOR", toupper(var_name), "\n")
      cat("----------------------------------------------------------------\n")
      
      # Create numeric results first
      numeric_results <- data.frame(
        Actual = actual_values,
        Predicted = median_predictions,
        Error = median_errors
      )
      
      # Then display with Period column
      cat("Period\tActual\tPredicted\tError\n")
      for (i in 1:length(period_labels)) {
        cat(period_labels[i], "\t", 
            round(numeric_results$Actual[i], 4), "\t", 
            round(numeric_results$Predicted[i], 4), "\t", 
            round(numeric_results$Error[i], 4), "\n")
      }
    }
  }
  
  # Display summary of SMAPE results for all windows and variables
  cat("\n================================================================\n")
  cat("SUMMARY OF SMAPE RESULTS FOR ALL WINDOWS AND VARIABLES (HOLDOUT PERIOD)\n")
  cat("================================================================\n")
  
  for(var_name in names(smape_results)) {
    cat("\n----------------------------------------------------------------\n")
    cat("SMAPE SUMMARY FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    print(round(smape_results[[var_name]], 4))
  }
  
  return(smape_results)
}

# Fixed function to aggregate predictions for all windows
aggregate_and_calculate_smape <- function(smape_results, all_modelewstep, hold_out_dataset) {
  # Initialize results storage for aggregated SMAPE
  aggregate_smape_results <- list()
  
  # Loop through each dependent variable
  for (var_name in names(all_modelewstep)) {
    # Skip if holdout data doesn't exist
    if (!(var_name %in% names(hold_out_dataset))) {
      cat("Skipping", var_name, "(holdout data not available)\n")
      next
    }
    
    cat("\n================================================================\n")
    cat("AGGREGATED PREDICTIONS AND SMAPE FOR", toupper(var_name), "(HOLDOUT PERIOD)\n")
    cat("================================================================\n")
    
    # Create period labels (80-87 plus Q1)
    period_labels <- c(as.character(105:111), "Q1")#period
    
    # Get holdout data for this variable
    holdout_data <- hold_out_dataset[[var_name]]
    actual_values <- holdout_data[, 1]
    pred_data <- as.data.frame(holdout_data[, -1])
    
    # Initialize matrices to store predictions from all windows (each row is a window)
    all_mean_preds <- matrix(0, nrow = 6, ncol = length(actual_values))
    all_median_preds <- matrix(0, nrow = 6, ncol = length(actual_values))
    windows_available <- numeric()
    
    # Loop through each window to collect predictions
    for(window_num in 1:5) {
      # Skip if model doesn't exist
      if (!(var_name %in% names(all_modelewstep)) || 
          length(all_modelewstep[[var_name]]) < window_num) {
        next
      }
      
      # Get the model for this window and variable
      model <- all_modelewstep[[var_name]][[window_num]]
      
      # Make predictions
      pred <- predict(model, newdata = pred_data, burn = 100)
      mean_predictions <- colMeans(pred$distribution)
      median_predictions <- apply(pred$distribution, 2, median)
      
      # Store in matrices
      all_mean_preds[window_num, ] <- mean_predictions
      all_median_preds[window_num, ] <- median_predictions
      windows_available <- c(windows_available, window_num)
    }
    
    # Filter matrices to only include available windows
    all_mean_preds <- all_mean_preds[windows_available, , drop = FALSE]
    all_median_preds <- all_median_preds[windows_available, , drop = FALSE]
    
    # Calculate aggregated predictions (average across windows)
    agg_mean_preds <- colMeans(all_mean_preds)
    agg_median_preds <- colMeans(all_median_preds)
    
    # Calculate errors
    mean_errors <- agg_mean_preds - actual_values
    median_errors <- agg_median_preds - actual_values
    
    # Calculate SMAPE for the aggregated predictions
    mean_smape <- mean(2 * abs(actual_values - agg_mean_preds) / 
                       (abs(actual_values) + abs(agg_mean_preds))) * 100
    
    median_smape <- mean(2 * abs(actual_values - agg_median_preds) / 
                         (abs(actual_values) + abs(agg_median_preds))) * 100
    
    # Store aggregated SMAPE results
    aggregate_smape_results[[var_name]] <- data.frame(
      Mean_SMAPE = mean_smape,
      Median_SMAPE = median_smape
    )
    
    # Display aggregated SMAPE results for this variable
    cat("\n----------------------------------------------------------------\n")
    cat("AGGREGATED SMAPE FOR", toupper(var_name), "(HOLDOUT PERIOD)\n")
    cat("----------------------------------------------------------------\n")
    cat("Mean SMAPE:", round(mean_smape, 4), "%\n")
    cat("Median SMAPE:", round(median_smape, 4), "%\n")
    
    # Display aggregated mean predictions table - FIX: Avoid mixing types in data frame
    cat("\n----------------------------------------------------------------\n")
    cat("TABLE: AGGREGATED MEAN PREDICTIONS FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    
    # Create numeric results first
    numeric_results <- data.frame(
      Actual = actual_values,
      Predicted = agg_mean_preds,
      Error = mean_errors
    )
    
    # Then display with Period column
    cat("Period\tActual\tPredicted\tError\n")
    for (i in 1:length(period_labels)) {
      cat(period_labels[i], "\t", 
          round(numeric_results$Actual[i], 4), "\t", 
          round(numeric_results$Predicted[i], 4), "\t", 
          round(numeric_results$Error[i], 4), "\n")
    }
    
    # Display aggregated median predictions table - Use same approach
    cat("\n----------------------------------------------------------------\n")
    cat("TABLE: AGGREGATED MEDIAN PREDICTIONS FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    
    # Create numeric results first
    numeric_results <- data.frame(
      Actual = actual_values,
      Predicted = agg_median_preds,
      Error = median_errors
    )
    
    # Then display with Period column
    cat("Period\tActual\tPredicted\tError\n")
    for (i in 1:length(period_labels)) {
      cat(period_labels[i], "\t", 
          round(numeric_results$Actual[i], 4), "\t", 
          round(numeric_results$Predicted[i], 4), "\t", 
          round(numeric_results$Error[i], 4), "\n")
    }
  }
  
  # Display summary of aggregated SMAPE results
  cat("\n================================================================\n")
  cat("SUMMARY OF AGGREGATED SMAPE RESULTS FOR ALL VARIABLES (HOLDOUT PERIOD)\n")
  cat("================================================================\n")
  
  for (var_name in names(aggregate_smape_results)) {
    cat("\n----------------------------------------------------------------\n")
    cat("AGGREGATED SMAPE SUMMARY FOR", toupper(var_name), "\n")
    cat("----------------------------------------------------------------\n")
    print(round(aggregate_smape_results[[var_name]], 4))
  }
  
  return(aggregate_smape_results)
}


# Run the functions with your models and holdout data
smape_results_stepew <- predict_all_windows_variables(all_modelewstep, hold_out_dataset)
aggregate_smape_results_stepew <- aggregate_and_calculate_smape(smape_results_stepew, all_modelewstep, hold_out_dataset)
```

# Summary stat

```{r}
all_diagnostics_ew_step <- list()
for(dep_var in names(all_modelewstep)) {
  all_diagnostics_ew_step[[dep_var]] <- get_model_diagnostics(all_modelewstep[[dep_var]], expanding_windows[[dep_var]])
}

print_all_diagnostics(all_diagnostics_ew_step)
```

## Function of getting LOOIC

```{r}
calculate_looic <- function(model, window_data) {
  # Get training sample size 
  n_train <- nrow(window_data$train)
  
  # Calculate LOOIC
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)
  loo_liks <- numeric(n_samples)
  
  for (i in 1:n_samples) {
    loo_liks[i] <- mean(log_lik[-i])
  }
  
  # normalized LOOIC
  looic <- (-2 * sum(loo_liks)) / n_train
  
  return(looic)
}

# Function to apply for all windows
get_all_looic <- function(models, windows) {
  n_windows <- length(models)
  looic_results <- numeric(n_windows)
  
  for (i in 1:n_windows) {
    looic_results[i] <- calculate_looic(models[[i]], windows[[i]])
  }
  
  return(looic_results)
}
all_looic_ew_step <- list()
for (dep_var in names(all_modelewstep)) {
  all_looic_ew_step[[dep_var]] <- get_all_looic(
    all_modelewstep[[dep_var]], 
    expanding_windows[[dep_var]]
  )
}
# Display results
for (dep_var in names(all_looic_ew_step)) {
  cat(sprintf("\nNormalized LOOIC Results for %s\n", dep_var))
  for (i in 1:length(all_looic_ew_step[[dep_var]])) {
    cat(sprintf("Window %d: %.4f\n", i, all_looic_ew_step[[dep_var]][i]))
  }
}
```

## 95% confi interval

```{r}
confidence_intervals_ew_step <- calculate_confidence_intervals(all_modelewstep, expanding_windows)
```

## mcmc size

```{r}
mcmc_stats_ew <- get_mcmc_stats(all_modelewstep, expanding_windows)
```

## Bayesian R² (Gelman et al.)

```{r}

## Loop over each dependent variable and each window to compute Bayesian R²
bayesian_R2_results_ew_step <- list()

for (dep_var_name in names(all_modelewstep)) {
  models_list <- all_modelewstep[[dep_var_name]]
  
  # Data frame to store summary metrics for each window
  bayes_R2_summary <- data.frame(
    Window = integer(),
    Mean_R2 = numeric(),
    Median_R2 = numeric(),
    Lower_R2 = numeric(),
    Upper_R2 = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Loop through each of the 7 expanding windows
  for (i in 1:length(models_list)) {
    # Get the training data for the current window
    train_data <- expanding_windows[[dep_var_name]][[i]]$train
    
    # Extract the fitted model for this window
    model <- models_list[[i]]
    
    # Compute the Bayesian R² draws for the training set
    R2_draws <- calculate_bayes_R2(model, train_data, burn = 100)
    
    # Summarize the draws (you can adjust the summary metrics as desired)
    bayes_R2_summary <- rbind(
      bayes_R2_summary,
      data.frame(
        Window = i,
        Mean_R2 = mean(R2_draws),
        Median_R2 = median(R2_draws),
        Lower_R2 = quantile(R2_draws, 0.025),
        Upper_R2 = quantile(R2_draws, 0.975)
      )
    )
  }
  
  # Store the summary for the current dependent variable
  bayesian_R2_results_ew_step[[dep_var_name]] <- bayes_R2_summary
}

## Print the Bayesian R² summary for each dependent variable and window
for (dep_var_name in names(bayesian_R2_results_ew_step)) {
  cat("\nBayesian R² for", dep_var_name, ":\n")
  print(round(bayesian_R2_results_ew_step[[dep_var_name]], 4))
}

```

## Ljung Box

```{r}
# Calculate the Ljung-Box diagnostics (with default lag = 10)
ljung_box_results_ew_step <- get_ljung_box_diagnostics(all_modelewstep, expanding_windows, lags = 10)

# Print the Ljung-Box test results for each dependent variable
for (dep_var in names(ljung_box_results_ew_step)) {
  cat("\nLjung-Box Test Diagnostics for", dep_var, ":\n")
  print(ljung_box_results_ew_step[[dep_var]])
}

```

## Posterior Prob

```{r}
pips_results_ew <- analyze_pips(all_modelewstep, expanding_windows)
```

## Geweke’s Diagnostic

```{r}
# Apply 
geweke_results_ew <- calculate_geweke(all_modelewstep, expanding_windows)
```

## Heidelberger-Welch

```{r}
for(dep_var in names(all_modelewstep)) {
  cat("\n\n=== Results for", dep_var, "===\n")
  heidel_results = calculate_heidel_welch(all_modelewstep[[dep_var]], expanding_windows[[dep_var]])
}
```

## Raftery_lewis

```{r}
# Run diagnostics
for(dep_var in names(all_modelewstep)) {
  cat("\n\n=== Convergence Diagnostics for", dep_var, "===\n")
  calculate_convergence_diagnostics(all_modelewstep[[dep_var]], expanding_windows[[dep_var]])
}
```

## predictive interval

```{r}
# Create extended test sets including the quarter_four data
create_extended_test_sets <- function(dep_var_datasets, expanding_windows, quarter_four) {
  extended_test_sets <- list()
  
  for(dep_var_name in names(dep_var_datasets)) {
    current_data <- dep_var_datasets[[dep_var_name]]
    dataset_windows <- list()
    
    # For each window in the expanding windows
    for(window_idx in 1:length(expanding_windows[[dep_var_name]])) {
      # Get the original test data from the expanding window
      test_data <- expanding_windows[[dep_var_name]][[window_idx]]$test
      
      # Add quarter_four data as the last row
      # Make sure to match column names
      if (!all(names(quarter_four[[dep_var_name]]) == names(test_data))) {
        names(quarter_four[[dep_var_name]]) <- names(test_data)
      }
      
      # Combine test data with quarter_four
      extended_test <- rbind(test_data, quarter_four[[dep_var_name]])
      
      # Add to windows
      dataset_windows[[window_idx]] <- extended_test
    }
    
    extended_test_sets[[dep_var_name]] <- dataset_windows
  }
  
  return(extended_test_sets)
}

# 95% Predictive Intervals using the fixed extended test sets
get_predictive_intervals <- function(all_evaluations, all_modelewall, extended_test_sets) {
  for(dep_var in names(all_evaluations)) {
    # Get SMAPE values and window numbers for mean-based
    smape_mean <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_mean) || is.na(x$errors_mean$SMAPE)) return(Inf)
      return(x$errors_mean$SMAPE)
    })
    
    # Sort and get indices of windows with valid SMAPE values
    valid_windows_mean <- which(is.finite(smape_mean))
    if(length(valid_windows_mean) > 0) {
      top_n_mean <- min(5, length(valid_windows_mean))
      ordered_windows_mean <- valid_windows_mean[order(smape_mean[valid_windows_mean])]
      top_5_mean <- ordered_windows_mean[1:top_n_mean]
    } else {
      top_5_mean <- integer(0)
    }
    
    # Get SMAPE values and window numbers for median-based
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) {
      if(is.null(x) || is.null(x$errors_median) || is.na(x$errors_median$SMAPE)) return(Inf)
      return(x$errors_median$SMAPE)
    })
    
    # Sort and get indices of windows with valid SMAPE values
    valid_windows_median <- which(is.finite(smape_median))
    if(length(valid_windows_median) > 0) {
      top_n_median <- min(5, length(valid_windows_median))
      ordered_windows_median <- valid_windows_median[order(smape_median[valid_windows_median])]
      top_5_median <- ordered_windows_median[1:top_n_median]
    } else {
      top_5_median <- integer(0)
    }
    
    # Calculate intervals for mean-based top models
    cat("\n=====================================")
    cat(sprintf("\n%s - Mean-Based Top %d Models (by sMAPE)\n", dep_var, length(top_5_mean)))
    cat("=====================================\n")
    
    intervals_mean <- data.frame()
    for(window in top_5_mean) {
      # Skip if window or model doesn't exist
      if(is.null(extended_test_sets[[dep_var]][[window]]) || 
         is.null(all_modelewall[[dep_var]][[window]])) {
        cat("Skipping window", window, "- data or model not available\n")
        next
      }
      
      # Get the extended test data and extract the last row
      extended_data <- extended_test_sets[[dep_var]][[window]]
      last_row <- extended_data[nrow(extended_data), ]
      last_row_predictors <- last_row[-1, drop = FALSE]  # Remove first column (response)
      
      # Check for missing values and handle them
      if(any(is.na(last_row_predictors))) {
        cat("Warning: Missing values in predictors for window", window, "\n")
        # Try to impute missing values with column means from the test data
        # This is a simple approach - you might want to use more sophisticated imputation
        for(col in 1:ncol(last_row_predictors)) {
          if(is.na(last_row_predictors[1, col])) {
            # Use mean of non-NA values in this column from test data
            col_mean <- mean(extended_data[-nrow(extended_data), col+1], na.rm = TRUE)
            last_row_predictors[1, col] <- col_mean
          }
        }
      }
      
      # Check for invalid values after imputation
      if(any(is.na(last_row_predictors))) {
        cat("Error: Still have missing values after imputation in window", window, "\n")
        next
      }
      
      # Try to make prediction
      tryCatch({
        pred <- suppressWarnings(predict(all_modelewall[[dep_var]][[window]], 
                                       newdata = as.data.frame(last_row_predictors),
                                       burn = 100))
        
        # Check if prediction was successful
        if(is.null(pred) || is.null(pred$distribution) || ncol(pred$distribution) == 0) {
          cat("Error: Prediction failed for window", window, "\n")
          next
        }
        
        # Get the distribution
        dist <- pred$distribution[, 1]
        
        # Calculate intervals
        intervals_mean <- rbind(intervals_mean, data.frame(
          Window = window,
          SMAPE = smape_mean[window],
          Lower_95 = quantile(dist, 0.025),
          Mean = mean(dist),
          Median = median(dist),
          Upper_95 = quantile(dist, 0.975)
        ))
      }, error = function(e) {
        cat("Error in prediction for window", window, ":", conditionMessage(e), "\n")
      })
    }
    
    # Print results if any
    if(nrow(intervals_mean) > 0) {
      print(round(intervals_mean, 4))
    } else {
      cat("No valid predictions for mean-based models.\n")
    }
    
    # Calculate intervals for median-based top models
    cat("\n=====================================")
    cat(sprintf("\n%s - Median-Based Top %d Models (by sMAPE)\n", dep_var, length(top_5_median)))
    cat("=====================================\n")
    
    intervals_median <- data.frame()
    for(window in top_5_median) {
      # Skip if window or model doesn't exist
      if(is.null(extended_test_sets[[dep_var]][[window]]) || 
         is.null(all_modelewall[[dep_var]][[window]])) {
        cat("Skipping window", window, "- data or model not available\n")
        next
      }
      
      # Get the extended test data and extract the last row
      extended_data <- extended_test_sets[[dep_var]][[window]]
      last_row <- extended_data[nrow(extended_data), ]
      last_row_predictors <- last_row[-1, drop = FALSE]  # Remove first column (response)
      
      # Check for missing values and handle them
      if(any(is.na(last_row_predictors))) {
        cat("Warning: Missing values in predictors for window", window, "\n")
        # Try to impute missing values with column means from the test data
        for(col in 1:ncol(last_row_predictors)) {
          if(is.na(last_row_predictors[1, col])) {
            # Use mean of non-NA values in this column from test data
            col_mean <- mean(extended_data[-nrow(extended_data), col+1], na.rm = TRUE)
            last_row_predictors[1, col] <- col_mean
          }
        }
      }
      
      # Check for invalid values after imputation
      if(any(is.na(last_row_predictors))) {
        cat("Error: Still have missing values after imputation in window", window, "\n")
        next
      }
      
      # Try to make prediction
      tryCatch({
        pred <- suppressWarnings(predict(all_modelewall[[dep_var]][[window]], 
                                       newdata = as.data.frame(last_row_predictors),
                                       burn = 100))
        
        # Check if prediction was successful
        if(is.null(pred) || is.null(pred$distribution) || ncol(pred$distribution) == 0) {
          cat("Error: Prediction failed for window", window, "\n")
          next
        }
        
        # Get the distribution
        dist <- pred$distribution[, 1]
        
        # Calculate intervals
        intervals_median <- rbind(intervals_median, data.frame(
          Window = window,
          SMAPE = smape_median[window],
          Lower_95 = quantile(dist, 0.025),
          Mean = mean(dist),
          Median = median(dist),
          Upper_95 = quantile(dist, 0.975)
        ))
      }, error = function(e) {
        cat("Error in prediction for window", window, ":", conditionMessage(e), "\n")
      })
    }
    
    # Print results if any
    if(nrow(intervals_median) > 0) {
      print(round(intervals_median, 4))
    } else {
      cat("No valid predictions for median-based models.\n")
    }
  }
}

# Example usage:
# Create extended test sets with quarter_four data
# Use the correct variables in your function call
extended_test_sets <- create_extended_test_sets(dep_var_datasets, expanding_windows, quarter_four)
get_predictive_intervals(all_evaluations_ewstep, all_modelewstep, extended_test_sets)
```

## MCMC trace plot

```{r}
# Function to find top 2 best models based on evaluations
find_best_models <- function(all_evaluations) {
  best_models <- list()  # To store the best models for each dep var
  
  for(var_name in names(all_evaluations)) {
    cat("\nFor", var_name, ":\n")
    
    # Collect all sMAPE values
    all_smape_results <- data.frame(
      Window = integer(),
      Method = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Collect mean-based sMAPE values
    for(i in 1:5) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_mean
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "mean",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: mean_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Collect median-based sMAPE values
    for(i in 1:5) {
      window_errors <- all_evaluations[[var_name]][[i]]$errors_median
      if (!is.na(window_errors$SMAPE)) {
        all_smape_results <- rbind(all_smape_results, data.frame(
          Window = i,
          Method = "median",
          SMAPE = window_errors$SMAPE,
          stringsAsFactors = FALSE
        ))
      } else {
        cat("Warning: median_sMAPE is missing for Window", i, "for", var_name, "\n")
      }
    }
    
    # Sort by sMAPE (ascending) to find the best models overall
    all_smape_results <- all_smape_results[order(all_smape_results$SMAPE), ]
    
    if (nrow(all_smape_results) >= 2) {
      # Get top 2 models
      top_2_models <- all_smape_results[1:2, ]
      
      cat("Top 2 models for", var_name, "based on sMAPE:\n")
      for (i in 1:2) {
        cat("Rank", i, ": Window", top_2_models$Window[i], "using", top_2_models$Method[i], 
            "method with sMAPE =", round(top_2_models$SMAPE[i], 4), "\n")
      }
      
      # Store the top 2 models info
      best_models[[var_name]] <- list(
        best_window_1 = top_2_models$Window[1],
        best_method_1 = top_2_models$Method[1],
        best_smape_1 = top_2_models$SMAPE[1],
        best_window_2 = top_2_models$Window[2],
        best_method_2 = top_2_models$Method[2],
        best_smape_2 = top_2_models$SMAPE[2]
      )
    } else if (nrow(all_smape_results) == 1) {
      # Handle case where only one valid model is found
      cat("Only one valid model found for", var_name, ":\n")
      cat("Window:", all_smape_results$Window[1], "using", all_smape_results$Method[1], 
          "method with sMAPE =", round(all_smape_results$SMAPE[1], 4), "\n")
      
      best_models[[var_name]] <- list(
        best_window_1 = all_smape_results$Window[1],
        best_method_1 = all_smape_results$Method[1],
        best_smape_1 = all_smape_results$SMAPE[1]
      )
    } else {
      cat("No valid models found for", var_name, "\n")
    }
  }
  
  return(best_models)
}

# Run find_best_models function to get best models based on sMAPE
best_models <- find_best_models(all_evaluations_ewstep)

```

```{r}
analyze_bsts_mcmc_trace_plots <- function(all_evaluations, all_models, best_models) {
  # Create a PDF device to save all plots
  pdf("MCMC_trace_plot_EW_STEP_2023Q1.pdf", width=12, height=10)
  
  for(dep_var in names(best_models)) {
    # Get the info for the top models
    best_model_info <- best_models[[dep_var]]
    
    # Process Rank 1 model
    window_1 <- best_model_info$best_window_1
    method_1 <- best_model_info$best_method_1
    smape_1 <- best_model_info$best_smape_1
    
    cat("\nCreating MCMC trace plots for", dep_var, ":\n")
    cat("Rank 1: Window:", window_1, "using", method_1, "method, sMAPE =", round(smape_1, 4), "\n")
    
    # Fetch the model
    model_1 <- all_models[[dep_var]][[window_1]]
    
    # Create trace plots for Rank 1 model
    create_trace_plots(model_1, dep_var, "Rank 1", window_1, method_1, smape_1)
    
    # Process Rank 2 model if available
    if(!is.null(best_model_info$best_window_2)) {
      window_2 <- best_model_info$best_window_2
      method_2 <- best_model_info$best_method_2
      smape_2 <- best_model_info$best_smape_2
      
      cat("Rank 2: Window:", window_2, "using", method_2, "method, sMAPE =", round(smape_2, 4), "\n")
      
      # Fetch the second-ranked model
      model_2 <- all_models[[dep_var]][[window_2]]
      
      # Create trace plots for Rank 2 model
      create_trace_plots(model_2, dep_var, "Rank 2", window_2, method_2, smape_2)
    } else {
      cat("No second-best model available for", dep_var, "\n")
    }
  }
  
  dev.off()
}

# Helper function to create trace plots for a model
create_trace_plots <- function(model, dep_var, rank_label, window, method, smape) {
  tryCatch({
    # Set up plot for state variances
    par(mfrow=c(3,1), mar=c(4,4,3,1))
    
    # Plot observation variance
    if (!is.null(model$sigma.obs)) {
      plot(1:length(model$sigma.obs), model$sigma.obs, type="l", col="blue",
           main="Observation Variance (sigma.obs)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot seasonal variance if it exists
    if (!is.null(model$sigma.seasonal.4)) {
      plot(1:length(model$sigma.seasonal.4), model$sigma.seasonal.4, type="l", col="red",
           main="Seasonal Variance (sigma.seasonal.4)",
           xlab="Iteration", ylab="Value")
    }
    
    # Plot trend level variance if it exists
    if (!is.null(model$sigma.trend.level)) {
      plot(1:length(model$sigma.trend.level), model$sigma.trend.level, type="l", col="green",
           main="Trend Level Variance (sigma.trend.level)",
           xlab="Iteration", ylab="Value")
    }
    
    mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Variances"), 
          outer=TRUE, line=-1.5, cex=1.2)
    
    # If the model has regression coefficients, plot those on a new page
    if (!is.null(model$coefficients) && !is.null(model$coefficients.samples) && 
        ncol(model$coefficients.samples) > 0) {
      
      # Calculate number of coefficient plots needed
      num_coefs <- ncol(model$coefficients.samples)
      rows_needed <- min(4, num_coefs)  # Maximum 4 coefficients per page
      
      # Start a new page
      par(mfrow=c(rows_needed, 1), mar=c(4,4,3,1))
      
      # Plot each coefficient
      for (i in 1:rows_needed) {
        coef_name <- colnames(model$coefficients.samples)[i]
        if (is.null(coef_name)) coef_name <- paste("Coefficient", i)
        
        coef_samples <- model$coefficients.samples[,i]
        plot(1:length(coef_samples), coef_samples, type="l", col="purple",
             main=paste("Regression Coefficient:", coef_name),
             xlab="Iteration", ylab="Value")
      }
      
      mtext(paste0(dep_var, " (", rank_label, "): MCMC Trace Plots - Coefficients"), 
            outer=TRUE, line=-1.5, cex=1.2)
    }
    
  }, error = function(e) {
    # If error occurs, print the error and try the built-in plotting function
    cat("Error in custom trace plotting for", rank_label, ":", e$message, "\n")
    cat("Falling back to built-in plotting...\n")
    
    # Reset the plot area
    par(mfrow=c(1,1))
    
    # Try the built-in plotting method
    tryCatch({
      # Try plotting state components instead
      plot(model, "components")
      title(main=paste0(dep_var, " (", rank_label, "): Component Contributions"))
    }, error = function(e2) {
      cat("Error in fallback plotting:", e2$message, "\n")
      
      # Create an empty plot with error message
      plot(1, 1, type="n", axes=FALSE, xlab="", ylab="")
      text(1, 1, paste("Error plotting MCMC traces for", rank_label), col="red", cex=1.5)
    })
  })
}
analyze_bsts_mcmc_trace_plots(all_evaluations_ewstep, all_modelewstep, best_models)
```

## Distribution Plot

```{r}
create_prediction_plots <- function(all_evaluations, all_models, extended_test_sets_step, hold_out_dataset) {
  library(ggplot2)
  pdf("Prediction_Distribution_EW_Step_2023Q1.pdf", width = 12, height = 8)
  
  for(dep_var in names(all_evaluations)) {
    # Create a data frame of all SMAPE values (mean and median) per window
    all_results <- data.frame(
      Window = numeric(),
      Type = character(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    for(i in 1:length(all_evaluations[[dep_var]])) {
      # Add mean results
      mean_smape <- all_evaluations[[dep_var]][[i]]$errors_mean$SMAPE
      if(!is.na(mean_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "mean", SMAPE = mean_smape, stringsAsFactors = FALSE))
      }
      # Add median results
      median_smape <- all_evaluations[[dep_var]][[i]]$errors_median$SMAPE
      if(!is.na(median_smape)) {
        all_results <- rbind(all_results, data.frame(Window = i, Type = "median", SMAPE = median_smape, stringsAsFactors = FALSE))
      }
    }
    
    # Sort by SMAPE (ascending) and select the top 4 rows
    top_4 <- all_results[order(all_results$SMAPE), ][1:4, ]
    
    plot_data <- data.frame()
    label_list <- c()  # to track labels and catch duplicates
    
    for(i in 1:nrow(top_4)) {
      window <- top_4$Window[i]
      model_type <- top_4$Type[i]
      smape_value <- top_4$SMAPE[i]
      
      pred <- predict.bsts(all_models[[dep_var]][[window]], 
                           newdata = as.data.frame(extended_test_sets_step[[dep_var]][[window]][nrow(extended_test_sets_step[[dep_var]][[window]]), -1, drop = FALSE]),
                           burn = 100)
      
      # Create a label that includes rank, window, type, and SMAPE.
      base_label <- paste("Rank", i, ": Window", window, "-", model_type, "(SMAPE:", round(smape_value, 4), ")")
      # If the same label already exists, append a suffix.
      duplicate_count <- sum(label_list == base_label)
      if(duplicate_count > 0) {
        label <- paste0(base_label, " - Copy", duplicate_count + 1)
      } else {
        label <- base_label
      }
      label_list <- c(label_list, label)
      
      plot_data <- rbind(plot_data, 
                         data.frame(Value = pred$distribution[,1],
                                    Model = factor(label),
                                    stringsAsFactors = FALSE))
    }
    
    # Get the actual observed value from the hold_out_dataset (assume it's the last value in column 1)
    actual_value <- tail(hold_out_dataset[[dep_var]][, 1], 1)
    
    p <- ggplot(plot_data, aes(x = Value, fill = Model)) +
      geom_density(alpha = 0.4) +
      theme_minimal() +
      labs(title = paste(dep_var, "- Top 4 Models Based on sMAPE"),
           x = "Predicted Value",
           y = "Density") +
      geom_vline(xintercept = actual_value, color = "red", linetype = "dashed", size = 1) +
      annotate("text", x = actual_value, y = Inf, label = paste("Actual:", actual_value),
               vjust = -0.5, color = "red", size = 3)
    
    print(p)
  }
  
  dev.off()
}


# Apply the function
create_prediction_plots(all_evaluations_ewstep, all_modelewstep, extended_test_sets, hold_out_dataset)
```

## Continuous Ranked Probability Score (CRPS)

```{r}
# Function to calculate CRPS
calculate_crps <- function(actual, pred_dist) {
  n <- length(pred_dist)
  sorted_pred <- sort(pred_dist)
  H <- function(x) ifelse(x >= 0, 1, 0)
  
  integral <- 0
  for(i in 1:(n-1)) {
    x <- sorted_pred[i]
    dx <- sorted_pred[i+1] - x
    F_x <- i/n
    integral <- integral + (F_x - H(x - actual))^2 * dx
  }
  return(integral)
}

# Fixed function to calculate CRPS for all windows and dependent variables
get_crps_scores <- function(all_models, expanding_windows) {
  crps_results <- list()
  
  for(dep_var in names(all_models)) {
    cat("\nProcessing CRPS for:", dep_var)
    window_crps <- data.frame()
    
    for(i in 1:5) {  # 6 windows as per your structure
      cat("\n  Window", i)
      
      # Skip if expanding_windows don't have this window
      if(is.null(expanding_windows[[dep_var]]) || length(expanding_windows[[dep_var]]) < i || 
         is.null(expanding_windows[[dep_var]][[i]]) || is.null(expanding_windows[[dep_var]][[i]]$test)) {
        cat(" - No test data available. Skipping.")
        next
      }
      
      # Get the model
      model <- all_models[[dep_var]][[i]]
      
      # Skip if model is missing
      if(is.null(model)) {
        cat(" - No model available. Skipping.")
        next
      }
      
      # Check if the model is a bsts object
      if(!inherits(model, "bsts")) {
        cat(" - Not a bsts model. Skipping.")
        next
      }
      
      # Get test data for this window
      test_data <- expanding_windows[[dep_var]][[i]]$test
      
      # Skip if no test data
      if(is.null(test_data) || nrow(test_data) == 0) {
        cat(" - Empty test data. Skipping.")
        next
      }
      
      actual <- test_data[, 1]
      test_features <- as.data.frame(test_data[, -1, drop = FALSE])
      
      # Make predictions
      tryCatch({
        pred <- predict(model, newdata = test_features, burn = 100)
        
        # Check prediction dimensions
        if(ncol(pred$distribution) != length(actual)) {
          cat(" - Prediction dimensions mismatch:", ncol(pred$distribution), "vs", length(actual))
          # Try to reconcile dimensions
          min_cols <- min(ncol(pred$distribution), length(actual))
          actual <- actual[1:min_cols]
          pred$distribution <- pred$distribution[, 1:min_cols, drop = FALSE]
          cat(" - Using only first", min_cols, "observations")
        }
        
        # Calculate CRPS for each observation
        crps_values <- numeric(length(actual))
        for(j in seq_along(actual)) {
          crps_values[j] <- calculate_crps(actual[j], pred$distribution[, j])
        }
        
        # Store results
        window_crps <- rbind(window_crps, data.frame(
          Window = i,
          CRPS_Mean = mean(crps_values),
          CRPS_SD = sd(crps_values)
        ))
        
        cat(" - CRPS calculation successful")
      }, error = function(e) {
        cat(" - Error:", e$message)
      })
    }
    
    crps_results[[dep_var]] <- window_crps
  }
  
  # Print results
  cat("\n\n=====================================")
  cat("\nCRPS Scores Summary\n")
  cat("=====================================\n")
  for(dep_var in names(crps_results)) {
    cat("\n", dep_var, ":\n")
    print(round(crps_results[[dep_var]], 4))
  }
  
  return(crps_results)
}

# Apply the fixed function
crps_scores_ew <- get_crps_scores(all_modelewstep, expanding_windows)
```

## log predictive density

```{r}
calculate_lpd <- function(all_models, expanding_windows) {
  lpd_results <- list()
  
  for(dep_var in names(all_models)) {
    cat("\nProcessing LPD for:", dep_var)
    window_lpd <- data.frame()
    
    for(i in 1:5) {
      cat("\n  Window", i)
      
      # Skip if expanding_windows don't have this window
      if(is.null(expanding_windows[[dep_var]]) || length(expanding_windows[[dep_var]]) < i || 
         is.null(expanding_windows[[dep_var]][[i]]) || is.null(expanding_windows[[dep_var]][[i]]$test)) {
        cat(" - No test data available. Skipping.")
        next
      }
      
      # Get the model
      model <- all_models[[dep_var]][[i]]
      
      # Skip if model is missing
      if(is.null(model)) {
        cat(" - No model available. Skipping.")
        next
      }
      
      # Get test data for this window
      test_data <- expanding_windows[[dep_var]][[i]]$test
      
      # Skip if no test data
      if(is.null(test_data) || nrow(test_data) == 0) {
        cat(" - Empty test data. Skipping.")
        next
      }
      
      actual <- test_data[, 1]
      test_features <- as.data.frame(test_data[, -1, drop = FALSE])
      
      # Get predictions
      tryCatch({
        pred <- predict.bsts(model, newdata = test_features, burn = 100)
        
        # Check prediction dimensions
        if(ncol(pred$distribution) != length(actual)) {
          cat(" - Prediction dimensions mismatch:", ncol(pred$distribution), "vs", length(actual))
          # Try to reconcile dimensions
          min_cols <- min(ncol(pred$distribution), length(actual))
          actual <- actual[1:min_cols]
          pred$distribution <- pred$distribution[, 1:min_cols, drop = FALSE]
          cat(" - Using only first", min_cols, "observations")
        }
        
        # Calculate LPD for each observation
        lpd_values <- numeric(length(actual))
        for(j in seq_along(actual)) {
          # Get density estimate of prediction distribution
          density_est <- density(pred$distribution[,j])
          # Find density at actual value
          actual_density <- approx(density_est$x, density_est$y, xout = actual[j])$y
          
          # Handle NA or 0 values in density
          if(is.na(actual_density) || actual_density <= 0) {
            cat(" - Warning: Zero or NA density for observation", j)
            lpd_values[j] <- NA
          } else {
            # Log density
            lpd_values[j] <- log(actual_density)
          }
        }
        
        # Store results
        window_lpd <- rbind(window_lpd, data.frame(
          Window = i,
          LPD_Mean = mean(lpd_values, na.rm = TRUE),
          LPD_SD = sd(lpd_values, na.rm = TRUE)
        ))
        
        cat(" - LPD calculation successful")
      }, error = function(e) {
        cat(" - Error:", e$message)
      })
    }
    
    lpd_results[[dep_var]] <- window_lpd
  }
  
  # Print results
  cat("\n\n=====================================")
  cat("\nLog Predictive Density Summary\n")
  cat("=====================================\n")
  for(dep_var in names(lpd_results)) {
    cat("\n", dep_var, ":\n")
    print(round(lpd_results[[dep_var]], 4))
  }
  
  return(lpd_results)
}

# Apply the function
lpd_scores_ew_step <- calculate_lpd(all_modelewstep, expanding_windows)
```

# Sensitivity Analysis

```{r}
# First, clean up any existing parallel connections
try(stopImplicitCluster(), silent = TRUE)
try(stopCluster(cl), silent = TRUE)
gc()  # Force garbage collection

# Load required packages
library(bsts)
library(parallel)

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to identify the best models
get_best_models_safe <- function() {
  best_predictions <- list()
  
  for (dep_var in names(all_modelewstep)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:6) {
      if (!dep_var %in% names(all_modelewstep) || length(all_modelewstep[[dep_var]]) < i) {
        next
      }
      
      tryCatch({
        model <- all_modelewstep[[dep_var]][[i]]
        
        # Get the testing data for this window and variable
        test_data <- expanding_windows[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        mape_mean <- mean(abs((actual_values - pred_mean) / actual_values)) * 100
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                          (abs(actual_values) + abs(pred_mean))) * 100
        
        mape_median <- mean(abs((actual_values - pred_median) / actual_values)) * 100
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                            (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to results
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = mape_mean, SMAPE = smape_mean),
                       data.frame(Window = i, Method = "median", 
                                MAPE = mape_median, SMAPE = smape_median))
      }, error = function(e) {
        cat("Error processing", dep_var, "window", i, ":", e$message, "\n")
      })
    }
    
    # Select the best five models
    if (nrow(results) > 0) {
      best_five <- results[order(results$MAPE), ][1:min(5, nrow(results)), ]
      best_predictions[[dep_var]] <- best_five
    }
  }
  
  return(best_predictions)
}

# Function to analyze one configuration
analyze_config <- function(params) {
  dep_var <- params$dep_var
  window_size <- params$window_size
  method <- params$method
  sigma <- params$sigma
  slab_var <- params$slab_var
  
  cat("Processing", dep_var, "window", window_size, "method", method, 
      "sigma", sigma, "slab_var", slab_var, "\n")
  
  # Get training and testing data
  train_data <- expanding_windows[[dep_var]][[window_size]]$train
  test_data <- expanding_windows[[dep_var]][[window_size]]$test
  
  # Extract values
  y <- train_data[, 1]
  X <- as.data.frame(train_data[, -1])
  actual_values <- test_data[, 1]
  test_predictors <- as.data.frame(test_data[, -1])
  
  # Create formula
  predictors <- colnames(X)
  formula_str <- paste("y ~", paste(predictors, collapse = " + "))
  formula <- as.formula(formula_str)
  
  # Full data for model
  model_data <- as.data.frame(cbind(y = y, X))
  
  # Create state specification
  ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
  ss <- AddLocalLinearTrend(ss, y, 
                           level.sigma.prior = SdPrior(sigma = sigma),
                           slope.sigma.prior = SdPrior(sigma = sigma))
  
  # Fit model
  result <- tryCatch({
    # Using the approach that worked in our test
    model <- bsts(formula, 
                 state.specification = ss,
                 niter = 10000,
                 data = model_data)
    
    # Make predictions
    all_predictions <- numeric(length(actual_values))
    
    for (t in 1:length(actual_values)) {
      # Prepare data for this period
      pred_data <- data.frame(y = NA)
      pred_data <- cbind(pred_data, test_predictors[t, , drop = FALSE])
      
      # Predict
      single_pred <- predict(model, newdata = pred_data, burn = 100)
      
      # Extract prediction
      if (method == "mean") {
        all_predictions[t] <- mean(single_pred$distribution[, 1])
      } else {
        all_predictions[t] <- median(single_pred$distribution[, 1])
      }
    }
    
    # Check if predictions vary
    has_varying_predictions <- length(unique(round(all_predictions, 2))) > 1
    
    # Calculate metrics
    mape <- mean(abs((actual_values - all_predictions) / actual_values)) * 100
    smape <- mean(2 * abs(actual_values - all_predictions) / 
                 (abs(actual_values) + abs(all_predictions))) * 100
    
    waic <- NA
    looic <- NA
    if (!is.null(model$log.likelihood)) {
      waic <- -2 * mean(model$log.likelihood)
      looic <- -2 * mean(model$log.likelihood)
    }
    
    return(list(
      success = TRUE,
      sigma = sigma,
      slab_var = slab_var,
      mape = mape,
      smape = smape,
      waic = waic,
      looic = looic,
      has_regression = has_varying_predictions
    ))
    
  }, error = function(e) {
    return(list(
      success = FALSE,
      error = e$message
    ))
  })
  
  return(result)
}

# Controlled parallel approach
run_sensitivity_analysis_parallel <- function() {
  # Define parameter combinations
  sigmas <- c(0.2, 0.4, 0.6, 0.8)
  slab_vars <- c(50, 100, 200)
  
  # Get best models
  best_models <- get_best_models_safe()
  
  # Initialize results
  sensitivity_results_step <- list()
  
  # For each dependent variable
  for (dep_var in names(best_models)) {
    cat("\n=== Starting analysis for", dep_var, "===\n")
    sensitivity_results_step[[dep_var]] <- list()
    models_info <- best_models[[dep_var]]
    
    # For each top model
    for (i in 1:min(5, nrow(models_info))) {
      model_info <- models_info[i, ]
      window_size <- model_info$Window
      method <- as.character(model_info$Method)
      model_name <- paste0("model_", i)
      
      cat("\n--- Processing", dep_var, "window", window_size, "method", method, "---\n")
      
      # Initialize results for this model
      model_results <- list()
      
      # Create parameter combinations
      param_list <- list()
      for (sigma in sigmas) {
        for (slab_var in slab_vars) {
          param_list[[length(param_list) + 1]] <- list(
            dep_var = dep_var,
            window_size = window_size,
            method = method,
            sigma = sigma,
            slab_var = slab_var
          )
        }
      }
      
      # Create a small cluster - use just 2 or 3 cores for stability
      num_cores <- min(3, detectCores() - 1)
      cat("Using", num_cores, "cores for parallel processing\n")
      cl <- makeCluster(num_cores)
      
      # Export required data and functions
      clusterExport(cl, c("expanding_windows"), envir = .GlobalEnv)
      clusterEvalQ(cl, {
        library(bsts)
      })
      
      # Run analysis in parallel
      config_results <- parLapply(cl, param_list, analyze_config)
      
      # Clean up
      stopCluster(cl)
      
      # Process results
      for (j in 1:length(param_list)) {
        params <- param_list[[j]]
        result <- config_results[[j]]
        
        config_name <- paste0("sigma_", params$sigma, "_slab_", params$slab_var)
        
        if (result$success) {
          # Store successful result
          model_results[[config_name]] <- list(
            sigma = params$sigma,
            slab_var = params$slab_var,
            mape = result$mape,
            smape = result$smape,
            waic = result$waic,
            looic = result$looic,
            has_regression = result$has_regression
          )
          
          cat("Configuration", config_name, "- MAPE:", round(result$mape, 4),
             "SMAPE:", round(result$smape, 4), "Has regression:", result$has_regression, "\n")
        } else {
          cat("Configuration", config_name, "failed:", result$error, "\n")
        }
      }
      
      # Store results for this model
      sensitivity_results_step[[dep_var]][[model_name]] <- list(
        window = window_size,
        method = method,
        results = model_results
      )
      
      # Find best configuration
      best_config <- NULL
      best_mape <- Inf
      best_config_name <- ""
      
      for (config_name in names(model_results)) {
        config <- model_results[[config_name]]
        if (!is.na(config$mape) && config$mape < best_mape) {
          best_mape <- config$mape
          best_config <- config
          best_config_name <- config_name
        }
      }
      
      if (is.null(best_config)) {
        cat("No valid configurations found.\n")
      } else {
        # Print best configuration
        cat("\nBest configuration:", best_config_name, "\n")
        cat("MAPE:", round(best_config$mape, 4), "\n")
        cat("SMAPE:", round(best_config$smape, 4), "\n")
        cat("Sigma:", best_config$sigma, "\n")
        cat("Slab variance:", best_config$slab_var, "\n")
        cat("Has regression:", best_config$has_regression, "\n\n")
        
        # Create summary table
        config_summary <- data.frame(
          Sigma = numeric(),
          SlabVar = numeric(),
          MAPE = numeric(),
          SMAPE = numeric(),
          WAIC = numeric(),
          LOOIC = numeric(),
          HasRegression = logical()
        )
        
        for (config_name in names(model_results)) {
          config <- model_results[[config_name]]
          config_summary <- rbind(config_summary, 
                                data.frame(
                                  Sigma = config$sigma,
                                  SlabVar = config$slab_var,
                                  MAPE = config$mape,
                                  SMAPE = config$smape,
                                  WAIC = config$waic,
                                  LOOIC = config$looic,
                                  HasRegression = config$has_regression
                                ))
        }
        
        # Set row names and sort
        rownames(config_summary) <- names(model_results)
        config_summary <- config_summary[order(config_summary$MAPE), ]
        print(round(config_summary, 4))
      }
      
      # Clean up after each model
      rm(model_results, config_results)
      gc()
    }
  }
  
  return(sensitivity_results_step)
}

# Run the analysis
start_time <- Sys.time()
cat("Starting sensitivity analysis at:", format(start_time), "\n")

sensitivity_results_step <- run_sensitivity_analysis_parallel()

end_time <- Sys.time()
execution_time <- end_time - start_time
cat("\nSensitivity analysis completed in:", format(execution_time), "\n")
```


```{r}
# Function to identify the top 5 models by SMAPE
find_top_models <- function(all_modelewstep, sensitivity_results_step) {
  top_models_summary <- list()
  
  # Process each dependent variable
  for(dep_var in names(sensitivity_results_step)) {
    cat("\n================================================================\n")
    cat("TOP 5 MODELS FOR", toupper(dep_var), "BASED ON SMAPE\n")
    cat("================================================================\n")
    
    # 1. Collect all model results including window performance and sensitivity analysis
    all_models <- data.frame(
      Source = character(),
      Window = integer(),
      Method = character(),
      Sigma = numeric(),
      SlabVar = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Add window performance results (from best_models)
    for (i in 1:6) { # Assuming 6 windows
      if (dep_var %in% names(all_modelewstep) && length(all_modelewstep[[dep_var]]) >= i) {
        # Get the testing data for this window and variable
        test_data <- expanding_windows[[dep_var]][[i]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Make predictions for testing period
        pred <- predict(all_modelewstep[[dep_var]][[i]], newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate SMAPE for mean
        smape_mean <- mean(2 * abs(actual_values - pred_mean) / 
                         (abs(actual_values) + abs(pred_mean))) * 100
        
        # Calculate SMAPE for median
        smape_median <- mean(2 * abs(actual_values - pred_median) / 
                           (abs(actual_values) + abs(pred_median))) * 100
        
        # Add to all_models dataframe
        all_models <- rbind(all_models, 
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "mean",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_mean,
                            stringsAsFactors = FALSE
                          ),
                          data.frame(
                            Source = "Window",
                            Window = i,
                            Method = "median",
                            Sigma = NA,
                            SlabVar = NA,
                            SMAPE = smape_median,
                            stringsAsFactors = FALSE
                          ))
      }
    }
    
    # Add sensitivity analysis results
    if (dep_var %in% names(sensitivity_results_step)) {
      for (model_name in names(sensitivity_results_step[[dep_var]])) {
        model_data <- sensitivity_results_step[[dep_var]][[model_name]]
        window <- model_data$window
        method <- model_data$method
        
        for (config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_models <- rbind(all_models,
                            data.frame(
                              Source = "Sensitivity",
                              Window = window,
                              Method = method,
                              Sigma = config$sigma,
                              SlabVar = config$slab_var,
                              SMAPE = config$smape,
                              stringsAsFactors = FALSE
                            ))
        }
      }
    }
    
    # 2. Sort by SMAPE and select top 5 models
    top_models <- all_models[order(all_models$SMAPE), ][1:min(5, nrow(all_models)), ]
    
    # 3. Create a formatted output
    formatted_models <- data.frame(
      Rank = 1:nrow(top_models),
      Description = character(nrow(top_models)),
      SMAPE = top_models$SMAPE,
      stringsAsFactors = FALSE
    )
    
    for (i in 1:nrow(top_models)) {
      model <- top_models[i, ]
      
      if (model$Source == "Window") {
        desc <- sprintf("%s Window %d", model$Method, model$Window)
      } else {
        desc <- sprintf("%s Window %d (sigma=%.1f, slab=%.0f)", 
                      model$Method, model$Window, model$Sigma, model$SlabVar)
      }
      
      formatted_models$Description[i] <- desc
    }
    
    # Display the top models
    print(formatted_models)
    
    # Store the results
    top_models_summary[[dep_var]] <- list(
      top_models = top_models,
      formatted_output = formatted_models
    )
  }
  
  return(top_models_summary)
}

# Example usage:
 top_model_results <- find_top_models(all_modelewstep, sensitivity_results_step)
```

```{r}
# Function to predict holdout period using top 5 models and create weighted ensemble
predict_and_create_ensemble <- function(all_modelewstep, top_model_results, hold_out_dataset) {
  # Clean up any existing connections
  try(stopImplicitCluster(), silent = TRUE)
  try(stopCluster(cl), silent = TRUE)
  gc()  # Force garbage collection
  
  # Load required packages
  library(parallel)
  
  # To store all results
  all_predictions <- list()
  
  # For each dependent variable
  for(dep_var in names(top_model_results)) {
    cat("\n================================================================\n")
    cat("HOLDOUT PREDICTIONS FOR", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Get top models for this variable
    top_models <- top_model_results[[dep_var]]$top_models
    
    # Get holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      cat("Holdout data not available for", dep_var, "\n")
      next
    }
    
    holdout_data <- hold_out_dataset[[dep_var]]
    
    # Prepare parameters for parallel execution
    param_list <- list()
    for(i in 1:nrow(top_models)) {
      param_list[[i]] <- list(
        dep_var = dep_var,
        model_info = top_models[i, ],
        model_index = i
      )
    }
    
    # Create a small cluster - use just 2 or 3 cores for stability
    num_cores <- min(3, detectCores() - 1)
    cat("Using", num_cores, "cores for parallel processing\n")
    cl <- makeCluster(num_cores)
    
    # Export required data and functions
    clusterExport(cl, c("all_modelewstep", "hold_out_dataset", "expanding_windows"), envir = .GlobalEnv)
    clusterEvalQ(cl, {
      library(bsts)
    })
    
    # Worker function for parallel execution
    predict_model_worker <- function(params) {
      dep_var <- params$dep_var
      model_info <- params$model_info
      model_index <- params$model_index
      
      # Create result container
      result <- list(
        model_index = model_index,
        dep_var = dep_var,
        success = FALSE
      )
      
      # Get the holdout data
      holdout_data <- hold_out_dataset[[dep_var]]
      actual_values <- holdout_data[, 1]
      pred_data <- as.data.frame(holdout_data[, -1])
      
      # Create period labels
      period_labels <- c(as.character(105:111), "Q1")#period
      
      # Create model description
      if(model_info$Source == "Window") {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, ")")
      } else {
        model_desc <- paste(model_info$Method, "(Window", model_info$Window, 
                          ", sigma=", model_info$Sigma, 
                          ", slab=", model_info$SlabVar, ")")
      }
      
      tryCatch({
        # Get or create model based on source
        if(model_info$Source == "Window") {
          # Use existing model
          window <- model_info$Window
          method <- as.character(model_info$Method)
          
          if(!(dep_var %in% names(all_modelewstep)) || length(all_modelewstep[[dep_var]]) < window) {
            return(c(result, list(error = "Model not available")))
          }
          
          model <- all_modelewstep[[dep_var]][[window]]
          
        } else {
          # Create model with sensitivity parameters
          window <- model_info$Window
          method <- as.character(model_info$Method)
          sigma <- model_info$Sigma
          slab_var <- model_info$SlabVar
          
          # Get original model
          if(!(dep_var %in% names(all_modelewstep)) || length(all_modelewstep[[dep_var]]) < window) {
            return(c(result, list(error = "Original model not available")))
          }
          
          original_model <- all_modelewstep[[dep_var]][[window]]
          
          # Get training data
          train_data <- expanding_windows[[dep_var]][[window]]$train
          y <- train_data[, 1]
          X <- as.data.frame(train_data[, -1])
          
          # Create formula
          predictors <- colnames(X)
          formula_str <- paste("y ~", paste(predictors, collapse = " + "))
          formula <- as.formula(formula_str)
          
          # Create model data
          model_data <- as.data.frame(cbind(y = y, X))
          
          # Set up state specification
          ss <- AddSeasonal(list(), y, nseasons = 4, season.duration = 1)
          ss <- AddLocalLinearTrend(ss, y, 
                                  level.sigma.prior = SdPrior(sigma = sigma),
                                  slope.sigma.prior = SdPrior(sigma = sigma))
          
          # Create model
          model <- bsts(formula, 
                       state.specification = ss,
                       niter = 10000,
                       data = model_data)
        }
        
        # Make predictions - using individual predictions for each period
        predictions <- numeric(nrow(holdout_data))
        
        for(t in 1:nrow(holdout_data)) {
          # Create data for this period
          if(model_info$Source == "Window") {
            # For window models
            this_period_data <- pred_data[t, , drop = FALSE]
            
            # Make prediction
            single_pred <- predict(model, newdata = this_period_data, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          } else {
            # For sensitivity models
            pred_row <- data.frame(y = NA)
            pred_row <- cbind(pred_row, pred_data[t, , drop = FALSE])
            
            # Make prediction
            single_pred <- predict(model, newdata = pred_row, burn = 100)
            
            # Extract prediction
            if(method == "mean") {
              predictions[t] <- mean(single_pred$distribution[, 1])
            } else {
              predictions[t] <- median(single_pred$distribution[, 1])
            }
          }
        }
        
        # Calculate errors
        abs_errors <- abs(predictions - actual_values)
        avg_abs_error <- mean(abs_errors)
        
        # Return successful result
        return(list(
          model_index = model_index,
          dep_var = dep_var,
          success = TRUE,
          model_desc = model_desc,
          predictions = predictions,
          actual_values = actual_values,
          abs_errors = abs_errors,
          avg_abs_error = avg_abs_error,
          period_labels = period_labels,
          smape = model_info$SMAPE
        ))
        
      }, error = function(e) {
        return(c(result, list(error = e$message)))
      })
    }
    
    # Run predictions in parallel
    model_results <- parLapply(cl, param_list, predict_model_worker)
    
    # Stop cluster
    stopCluster(cl)
    
    # Process and display results
    var_predictions <- list()
    
    for(result in model_results) {
      if(result$success) {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, ":", result$model_desc, "\n")
        cat("----------------------------------------------------------------\n")
        
        # Print results in a tabular format
        cat("\nPeriod\tActual\t\tPredicted\tAbsError\n")
        for(p in 1:length(result$period_labels)) {
          cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                    result$period_labels[p], 
                    result$actual_values[p], 
                    result$predictions[p], 
                    result$abs_errors[p]))
        }
        
        cat("\nAverage Absolute Error:", round(result$avg_abs_error, 2), "\n")
        
        # Store successful predictions
        var_predictions[[result$model_index]] <- result
      } else {
        cat("\n----------------------------------------------------------------\n")
        cat("MODEL", result$model_index, "failed:", result$error, "\n")
        cat("----------------------------------------------------------------\n")
      }
    }
    
    # Store predictions for this variable
    all_predictions[[dep_var]] <- var_predictions
    
    # Now create weighted ensemble
    successful_models <- model_results[sapply(model_results, function(r) r$success)]
    
    if(length(successful_models) > 0) {
      # Get the actual values and period labels from the first successful model
      actual_values <- successful_models[[1]]$actual_values
      period_labels <- successful_models[[1]]$period_labels
      
      # Create prediction matrix
      pred_matrix <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      smape_values <- numeric(length(successful_models))
      model_descriptions <- character(length(successful_models))
      
      # Fill the matrix with predictions
      for(i in 1:length(successful_models)) {
        pred_matrix[, i] <- successful_models[[i]]$predictions
        smape_values[i] <- successful_models[[i]]$smape
        model_descriptions[i] <- successful_models[[i]]$model_desc
      }
      
      # Calculate weights based on inverse SMAPE
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      
      # Display the weights
      cat("\n================================================================\n")
      cat("WEIGHTED ENSEMBLE FOR", toupper(dep_var), "\n")
      cat("================================================================\n")
      
      cat("\nModel Weights:\n")
      for(i in 1:length(weights)) {
        cat(sprintf("Model %d: %s - Weight: %.4f (SMAPE: %.4f)\n", 
                  i, model_descriptions[i], weights[i], smape_values[i]))
      }
      
      # Calculate weighted predictions
      weighted_predictions <- numeric(length(actual_values))
      for(i in 1:length(actual_values)) {
        weighted_predictions[i] <- sum(pred_matrix[i, ] * weights, na.rm = TRUE)
      }
      
      # Calculate errors
      abs_errors <- abs(weighted_predictions - actual_values)
      
      # Calculate ensemble SMAPE
      ensemble_smape <- mean(2 * abs(actual_values - weighted_predictions) / 
                           (abs(actual_values) + abs(weighted_predictions))) * 100
      
      # Display results
      cat("\n----------------------------------------------------------------\n")
      cat("WEIGHTED ENSEMBLE RESULTS\n")
      cat("----------------------------------------------------------------\n")
      cat("Ensemble SMAPE:", round(ensemble_smape, 4), "%\n\n")
      
      cat("Period\tActual\t\tPredicted\tAbsError\n")
      for(p in 1:length(period_labels)) {
        cat(sprintf("%s\t%.2f\t\t%.2f\t\t%.2f\n", 
                  period_labels[p], 
                  actual_values[p], 
                  weighted_predictions[p], 
                  abs_errors[p]))
      }
      
      cat("\nAverage Absolute Error:", round(mean(abs_errors), 2), "\n")
    } else {
      cat("\nNo successful models for", dep_var, "- cannot create ensemble\n")
    }
  }
  
  # Return all predictions
  return(all_predictions)
}

# Run the function to predict and create weighted ensemble
all_results <- predict_and_create_ensemble(all_modelewstep, top_model_results, hold_out_dataset)
```

```{r}
# This function replicates your 'print_ensemble_tables' logic but returns a named vector
# of Weighted Ensemble SMAPEs (one entry per dep_var).
get_ensemble_smapes <- function(all_results, top_model_results, hold_out_dataset) {
  ensemble_smapes <- numeric(0)  # named vector
  
  for(dep_var in names(top_model_results)) {
    # Skip if no holdout data
    if(!(dep_var %in% names(hold_out_dataset))) {
      next
    }
    holdout_data  <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    
    # Filter out NULL models
    successful_models <- all_results[[dep_var]]
    successful_models <- successful_models[!sapply(successful_models, is.null)]
    
    if(length(successful_models) > 0) {
      smape_values <- numeric(length(successful_models))
      pred_matrix  <- matrix(NA, nrow = length(actual_values), ncol = length(successful_models))
      
      for(i in seq_along(successful_models)) {
        pred_matrix[, i]  <- successful_models[[i]]$predictions
        smape_values[i]   <- successful_models[[i]]$smape
      }
      
      # Weighted predictions
      inverse_smape <- 1 / smape_values
      weights <- inverse_smape / sum(inverse_smape)
      weighted_predictions <- rowSums(t(t(pred_matrix) * weights), na.rm = TRUE)
      
      # Weighted Ensemble SMAPE
      ensemble_smape <- mean(
        2 * abs(actual_values - weighted_predictions) /
        (abs(actual_values) + abs(weighted_predictions))
      ) * 100
      
      # Store in named vector
      ensemble_smapes[dep_var] <- ensemble_smape
    }
  }
  
  return(ensemble_smapes)
}
ensemble_smapes = get_ensemble_smapes(all_results, top_model_results, hold_out_dataset)
```

## ensemble result 
```{r}
# Function to combine existing results and create ensemble SMAPE
combine_existing_results <- function(all_modelewstep, sensitivity_results, hold_out_dataset) {
  
  final_results <- data.frame(
    Variable = character(),
    Prediction_Type = character(),
    Ensemble_SMAPE = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Process each dependent variable
  for(dep_var in names(hold_out_dataset)) {
    cat("\n================================================================\n")
    cat("PROCESSING", toupper(dep_var), "\n")
    cat("================================================================\n")
    
    # Collect all SMAPE values and predictions from both methods
    all_configs <- data.frame(
      Method = character(),
      Window = integer(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Method 1: Original window models (from all_modelewstep)
    for(window in 1:5) {
      if(dep_var %in% names(all_modelewstep) && length(all_modelewstep[[dep_var]]) >= window) {
        
        # Get test data
        test_data <- expanding_windows[[dep_var]][[window]]$test
        actual_values <- test_data[, 1]
        test_predictors <- as.data.frame(test_data[, -1])
        
        # Get existing model and make predictions
        model <- all_modelewstep[[dep_var]][[window]]
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        
        # Calculate SMAPE for this configuration
        smape <- mean(2 * abs(actual_values - pred_mean) / 
                        (abs(actual_values) + abs(pred_mean))) * 100
        
        all_configs <- rbind(all_configs, 
                             data.frame(Method = "Original", Window = window, SMAPE = smape))
      }
    }
    
    # Method 2: Sensitivity analysis (from sensitivity_results)
    if(dep_var %in% names(sensitivity_results)) {
      for(model_name in names(sensitivity_results[[dep_var]])) {
        model_data <- sensitivity_results[[dep_var]][[model_name]]
        
        for(config_name in names(model_data$results)) {
          config <- model_data$results[[config_name]]
          
          all_configs <- rbind(all_configs, 
                               data.frame(Method = "Sensitivity", 
                                          Window = model_data$window, 
                                          SMAPE = config$smape))
        }
      }
    }
    
    # Select top 10 configurations
    top_configs <- all_configs[order(all_configs$SMAPE), ][1:min(10, nrow(all_configs)), ]
    
    cat("Top 10 configurations selected:\n")
    print(top_configs)
    
    # Get holdout data
    holdout_data <- hold_out_dataset[[dep_var]]
    actual_values <- holdout_data[, 1]
    pred_data <- as.data.frame(holdout_data[, -1])
    period_labels <- c(as.character(73:79), "Q4")
    
    # Generate holdout predictions for top 10 configurations
    pred_matrix_mean <- matrix(NA, nrow = length(actual_values), ncol = nrow(top_configs))
    pred_matrix_median <- matrix(NA, nrow = length(actual_values), ncol = nrow(top_configs))
    weights <- 1 / top_configs$SMAPE  # Inverse SMAPE weighting
    
    # For each top configuration, get holdout predictions
    for(i in 1:nrow(top_configs)) {
      config <- top_configs[i, ]
      
      if(config$Method == "Original") {
        # Use existing original model
        model <- all_modelewstep[[dep_var]][[config$Window]]
        pred <- predict(model, newdata = pred_data, burn = 100)
        pred_matrix_mean[, i] <- colMeans(pred$distribution)
        pred_matrix_median[, i] <- apply(pred$distribution, 2, median)
        
      } else {
        # For sensitivity models, we already have the predictions in all_results
        # Find the corresponding predictions from your existing all_results
        found_pred <- FALSE
        
        if(exists("all_results") && dep_var %in% names(all_results)) {
          for(result in all_results[[dep_var]]) {
            if(!is.null(result) && "predictions" %in% names(result)) {
              # This is a simplified approach - you might need to match more precisely
              # based on your specific data structure
              pred_matrix_mean[, i] <- result$predictions
              pred_matrix_median[, i] <- result$predictions  # Assuming same for now
              found_pred <- TRUE
              break
            }
          }
        }
        
        if(!found_pred) {
          cat("Could not find predictions for sensitivity config", i, "\n")
          pred_matrix_mean[, i] <- NA
          pred_matrix_median[, i] <- NA
        }
      }
    }
    
    # Remove failed predictions
    valid_cols <- !is.na(colSums(pred_matrix_mean))
    pred_matrix_mean <- pred_matrix_mean[, valid_cols, drop = FALSE]
    pred_matrix_median <- pred_matrix_median[, valid_cols, drop = FALSE]
    weights <- weights[valid_cols]
    
    if(ncol(pred_matrix_mean) > 0) {
      # Normalize weights
      weights <- weights / sum(weights)
      
      # Calculate weighted ensemble predictions
      ensemble_preds_mean <- rowSums(t(t(pred_matrix_mean) * weights), na.rm = TRUE)
      ensemble_preds_median <- rowSums(t(t(pred_matrix_median) * weights), na.rm = TRUE)
      
      # Calculate ensemble SMAPE
      ensemble_smape_mean <- mean(2 * abs(actual_values - ensemble_preds_mean) / 
                                    (abs(actual_values) + abs(ensemble_preds_mean))) * 100
      ensemble_smape_median <- mean(2 * abs(actual_values - ensemble_preds_median) / 
                                      (abs(actual_values) + abs(ensemble_preds_median))) * 100
      
      # Store results
      final_results <- rbind(final_results, 
                             data.frame(Variable = dep_var, Prediction_Type = "Mean", Ensemble_SMAPE = ensemble_smape_mean),
                             data.frame(Variable = dep_var, Prediction_Type = "Median", Ensemble_SMAPE = ensemble_smape_median))
      
      cat("\nResults for", dep_var, ":\n")
      cat("Mean Ensemble SMAPE:", round(ensemble_smape_mean, 4), "%\n")
      cat("Median Ensemble SMAPE:", round(ensemble_smape_median, 4), "%\n")
    }
  }
  
  return(final_results)
}

# Run the analysis using existing results only
ensemble_results <- combine_existing_results(all_modelewstep, sensitivity_results, hold_out_dataset)

# Create the final table you requested
cat("\n================================================================\n")
cat("FINAL RESULTS TABLE\n")
cat("================================================================\n")

# Aggregate across all variables
mean_smape <- mean(ensemble_results[ensemble_results$Prediction_Type == "Mean", "Ensemble_SMAPE"])
median_smape <- mean(ensemble_results[ensemble_results$Prediction_Type == "Median", "Ensemble_SMAPE"])

final_table <- data.frame(
  Prediction_Type = c("Mean", "Median"),
  SMAPE = c(mean_smape, median_smape)
)

print(final_table)

cat("\nDetailed by variable:\n")
print(ensemble_results)
```

# Influence

```{r}
if (!requireNamespace("foreach", quietly = TRUE)) {
  install.packages("foreach")
}
if (!requireNamespace("doParallel", quietly = TRUE)) {
  install.packages("doParallel")
}

# Setup parallel processing
library(parallel)
library(foreach)
library(doParallel)
library(bsts)

# Register parallel backend
cores <- detectCores() - 1  # Leave one core free for system processes
registerDoParallel(cores)
cat("Using", cores, "cores for parallel processing\n")

# Function to calculate error metrics
calculate_errors <- function(actual, predicted) {
  mse <- mean((actual - predicted)^2)
  mae <- mean(abs(actual - predicted))
  mape <- mean(abs((actual - predicted) / actual)) * 100
  smape <- mean(2 * abs(actual - predicted) / (abs(actual) + abs(predicted))) * 100
  mase <- mae / mean(abs(diff(actual)))
  owa <- (mase + mape) / 2
  return(data.frame(MSE = mse, MAE = mae, MAPE = mape, SMAPE = smape, MASE = mase, OWA = owa))
}

# Function to get window data
get_window_data <- function(dep_var, window_size) {
  window_data <- expanding_windows[[dep_var]][[window_size]]$train
  return(window_data)
}

# Function to get the best models based on testing dataset performance (changed from holdout)
get_best_models_test <- function(all_models) {
  best_predictions <- list()
  
  for (dep_var in names(all_models)) {
    results <- data.frame(
      Window = integer(),
      Method = character(),
      MAPE = numeric(),
      SMAPE = numeric(),
      stringsAsFactors = FALSE
    )
    
    # Get predictions from each model
    for (i in 1:length(all_models[[dep_var]])) {
      model <- all_models[[dep_var]][[i]]
      
      # Get the testing data for this window and variable
      test_data <- expanding_windows[[dep_var]][[i]]$test
      actual_values <- test_data[, 1]
      test_predictors <- as.data.frame(test_data[, -1])
      
      # Make predictions for testing period
      tryCatch({
        pred <- predict(model, newdata = test_predictors, burn = 100)
        pred_mean <- colMeans(pred$distribution)
        pred_median <- apply(pred$distribution, 2, median)
        
        # Calculate error metrics
        errors_mean <- calculate_errors(actual_values, pred_mean)
        errors_median <- calculate_errors(actual_values, pred_median)
        
        results <- rbind(results,
                       data.frame(Window = i, Method = "mean", 
                                MAPE = errors_mean$MAPE, SMAPE = errors_mean$SMAPE),
                       data.frame(Window = i, Method = "median", 
                                MAPE = errors_median$MAPE, SMAPE = errors_median$SMAPE))
      }, error = function(e) {
        cat("Error predicting for", dep_var, "window", i, ":", conditionMessage(e), "\n")
      })
    }
    
    # If we have results, select the best two models based on MAPE
    if (nrow(results) > 0) {
      best_two <- results[order(results$MAPE), ][1:min(2, nrow(results)), ]
      best_predictions[[dep_var]] <- best_two
    } else {
      cat("No valid predictions for", dep_var, "- skipping\n")
    }
  }
  
  # Print best models
  cat("\n=== Best Models for Testing Period ===\n")
  for(dep_var in names(best_predictions)) {
    cat("\n", dep_var, ":\n")
    print(best_predictions[[dep_var]])
  }
  
  return(best_predictions)
}

# FIXED Function to calculate influence measures with LOOIC and LPD
calculate_influence_test <- function(model, train_data, method, dep_var, window_size) {
  # Get original training data
  y <- train_data[, 1]
  X <- train_data[, -1]
  n_obs <- length(y)
  
  # Create model matrix for X
  X_matrix <- model.matrix(~ ., data = as.data.frame(X))[, -1]
  
  # Create time labels starting from 1997 Q1
  start_year <- 1997
  time_labels <- paste0(rep(start_year:(start_year + floor(n_obs/4)), each=4)[1:n_obs], 
                       " Q", rep(1:4, length.out=n_obs))
  
  # Results storage
  results <- data.frame(
    time = time_labels,
    pointwise_lpd = numeric(n_obs),
    delta_looic = numeric(n_obs),
    delta_test_forecast = numeric(n_obs)
  )
  
  # Get test data - FIXED to use passed parameters
  test_data <- expanding_windows[[dep_var]][[window_size]]$test
  test_y <- test_data[, 1]
  test_X <- as.data.frame(test_data[, -1])
  
  # Make original predictions for test period
  original_pred <- predict(model, newdata = test_X, burn = 100)
  original_point_preds <- if(method == "median") {
    apply(original_pred$distribution, 2, median)
  } else {
    colMeans(original_pred$distribution)
  }
  
  # Calculate pointwise log predictive density 
  log_lik <- model$log.likelihood
  n_samples <- length(log_lik)  # Using number of samples from log-likelihood
  
  # Ensure we have enough samples (safety check)
  if (n_samples == 0) {
    stop("No log-likelihood values available.")
  }

  log_lik_matrix <- matrix(log_lik, nrow = n_samples, ncol = n_obs, byrow = FALSE)
  
  results$pointwise_lpd <- colMeans(log_lik_matrix, na.rm = TRUE)  # Average LPD
  
  # LOOIC Calculation and test prediction changes - Parallelized version
  looic_values <- rep(NA, n_obs)
  delta_forecasts <- rep(NA, n_obs)
  
  # Parallel loop over observations
  parallel_results <- foreach(i = 1:n_obs, 
                           .packages = c("bsts"),
                           .export = c("test_X", "original_point_preds", "method")) %dopar% {
    # Create leave-one-out dataset
    loo_data <- train_data[-i, ]
    y_loo <- loo_data[, 1]
    X_loo <- loo_data[, -1]
    
    # Fit model on leave-one-out data with reduced iterations for speed
    ss <- AddSeasonal(list(), y_loo, nseasons = 4, season.duration = 1)
    ss <- AddLocalLinearTrend(ss, y_loo)
    
    model_loo <- tryCatch({
      bsts(y_loo,
           X = X_loo,
           state.specification = ss,
           niter = 10000,  # Reduced from 10000 for speed
           ping = 0)
    }, 
    error = function(e) {
      return(NULL)  # Return NULL if fitting fails
    })
    
    if (is.null(model_loo)) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    # Calculate LOOIC for leave-out model
    log_lik_loo <- model_loo$log.likelihood
    if (length(log_lik_loo) == 0) {
      return(list(looic = NA, delta_forecast = NA))
    }
    
    loo_liks <- numeric(length(log_lik_loo))
    for (j in 1:length(log_lik_loo)) {
      loo_liks[j] <- mean(log_lik_loo[-j], na.rm = TRUE)
    }
    
    looic <- -2 * mean(loo_liks, na.rm = TRUE)
    
    # Calculate change in test forecasts
    # Predict for test period using the leave-one-out model
    pred_loo <- predict(model_loo, newdata = test_X, burn = 100)
    
    point_pred_loo <- if(method == "median") {
      apply(pred_loo$distribution, 2, median)
    } else {
      colMeans(pred_loo$distribution)
    }
    
    # Calculate average change across all test observations
    delta_forecast <- mean(point_pred_loo - original_point_preds)
    
    return(list(looic = looic, delta_forecast = delta_forecast))
  }
  
  # Collect the parallel results
  for (i in 1:n_obs) {
    if (!is.null(parallel_results[[i]])) {
      looic_values[i] <- parallel_results[[i]]$looic
      delta_forecasts[i] <- parallel_results[[i]]$delta_forecast
    }
  }
  
  # Calculate original LOOIC
  looic_original <- -2 * mean(colMeans(log_lik_matrix, na.rm = TRUE))
  
  # Assign results
  results$delta_looic <- looic_values - looic_original  # Calculate delta LOOIC
  results$delta_test_forecast <- delta_forecasts
  
  return(results)
}

# FIXED Function to analyze the top models with test data
analyze_top_models_parallel <- function(best_models, all_models) {
  all_influence_results <- list()
  
  for(dep_var in names(best_models)) {
    cat("\n\nAnalyzing dependent variable:", dep_var)
    dep_influence <- list()
    models <- best_models[[dep_var]]
    
    for(i in 1:nrow(models)) {
      cat("\n\nProcessing model", i, "for", dep_var)
      model_info <- models[i, ]
      
      # Get training data for this window
      window_data <- get_window_data(dep_var, model_info$Window)
      
      # Get the original model 
      original_model <- all_models[[dep_var]][[model_info$Window]]
      
      # Calculate influence measures - FIXED to pass dep_var and window_size
      influence_results <- calculate_influence_test(
        original_model, 
        window_data,
        model_info$Method,
        dep_var,
        model_info$Window
      )
      
      # Add model identifier
      identifier <- paste("Window", model_info$Window, "-", model_info$Method)
      dep_influence[[identifier]] <- influence_results
    }
    
    all_influence_results[[dep_var]] <- dep_influence
  }
  
  return(all_influence_results)
}

# Function to generate comprehensive results tables
generate_influence_summary <- function(influence_results) {
  for(dep_var in names(influence_results)) {
    cat("\n=== Results for", dep_var, "===\n")
    
    for(model_name in names(influence_results[[dep_var]])) {
      cat("\n--- Model:", model_name, "---\n")
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # Round numeric columns to 4 decimal places
      results_df[, 2:4] <- round(results_df[, 2:4], 4)
      
      # Sort by absolute delta_test_forecast to find most influential observations
      sorted_df <- results_df[order(abs(results_df$delta_test_forecast), decreasing = TRUE), ]
      
      cat("Top 25 most influential observations:\n")
      print(head(sorted_df, 25))
      
      cat("\nSummary statistics:\n")
      
      # Calculate summary statistics separately
      lpd_summary <- summary(results_df$pointwise_lpd)
      looic_summary <- summary(results_df$delta_looic)
      forecast_summary <- summary(results_df$delta_test_forecast)
      
      # Print each metric separately to avoid data frame mixing issues
      cat("\nPointwise LPD:\n")
      print(round(lpd_summary, 4))
      cat("Std Dev:", round(sd(results_df$pointwise_lpd, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta LOOIC:\n")
      print(round(looic_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_looic, na.rm = TRUE), 4), "\n")
      
      cat("\nDelta Test Forecast:\n")
      print(round(forecast_summary, 4))
      cat("Std Dev:", round(sd(results_df$delta_test_forecast, na.rm = TRUE), 4), "\n")
    }
  }
}

# Run the analysis with parallel processing
run_influence_analysis_parallel <- function() {
  # Start timing
  start_time <- Sys.time()
  cat("Starting parallel influence analysis at:", format(start_time), "\n")
  
  # First get the best models based on test data performance
  best_models <- get_best_models_test(all_modelewstep)
  
  # Run the influence analysis in parallel
  influence_results <- analyze_top_models_parallel(best_models, all_modelewstep)
  
  # Generate summary tables
  generate_influence_summary(influence_results)
  
  # End timing
  end_time <- Sys.time()
  execution_time <- end_time - start_time
  cat("\nInfluence analysis completed in:", format(execution_time), "\n")
  
  return(influence_results)
}

# Run the analysis
influence_results_parallel_step <- run_influence_analysis_parallel()

# Clean up the parallel backend when done
stopImplicitCluster()
```


# Excel

```{r}
# Complete Excel Results Creation with Stepwise Elimination Handling
# Modified create_enhanced_prediction_tables function - HANDLES STEPWISE ELIMINATION
create_enhanced_prediction_tables_revised <- function(all_evaluations, all_models, extended_test_sets, 
                                              crps_scores, lpd_scores, file_path, 
                                              aggregate_smape_results,
                                              ensemble_results) {
  dep_vars <- names(all_evaluations)
  wb <- createWorkbook()
  
  for(dep_var in dep_vars) {
    addWorksheet(wb, dep_var)
    
    # Get ensemble SMAPEs from the new ensemble analysis
    ensemble_mean_smape <- NA
    ensemble_median_smape <- NA
    
    if(!is.null(ensemble_results) && nrow(ensemble_results) > 0) {
      # Filter for this variable
      var_results <- ensemble_results[ensemble_results$Variable == dep_var, ]
      if(nrow(var_results) > 0) {
        mean_row <- var_results[var_results$Prediction_Type == "Mean", ]
        median_row <- var_results[var_results$Prediction_Type == "Median", ]
        
        if(nrow(mean_row) > 0) ensemble_mean_smape <- round(mean_row$Ensemble_SMAPE, 2)
        if(nrow(median_row) > 0) ensemble_median_smape <- round(median_row$Ensemble_SMAPE, 2)
      }
    }
    
    # ONLY ENSEMBLE ROWS - removed Use Mean and Use Median
    variable_data <- data.frame(
      Col1 = c(
        dep_var,
        "Holds out period",
        "Prediction based on testing SMAPE",
        "Ensemble Mean (Top 10)",
        "Ensemble Median (Top 10)"     
      ),
      Col2 = c(
        "",
        "2023Q1 - 2024Q4",
        "SMAPE",
        ensemble_mean_smape,
        ensemble_median_smape          
      ),
      stringsAsFactors = FALSE
    )
    
    writeData(wb, dep_var, variable_data, startRow = 1, startCol = 1, colNames = FALSE)
    setColWidths(wb, dep_var, cols = 1:2, widths = c(25, 20))
    
    # Handle stepwise elimination in prediction sections
    start_row <- 7
    
    smape_mean   <- sapply(all_evaluations[[dep_var]], function(x) x$errors_mean$SMAPE)
    smape_median <- sapply(all_evaluations[[dep_var]], function(x) x$errors_median$SMAPE)
    
    crps_mean <- numeric(7)
    crps_sd   <- numeric(7)
    for(i in 1:7) {
      if(!is.null(crps_scores[[dep_var]])) {
        window_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == i, ]
        if(nrow(window_data) > 0) {
          crps_mean[i] <- window_data$CRPS_Mean
          crps_sd[i]   <- window_data$CRPS_SD
        } else {
          crps_mean[i] <- NA
          crps_sd[i]   <- NA
        }
      } else {
        crps_mean[i] <- NA
        crps_sd[i]   <- NA
      }
    }
    
    lpd_mean <- numeric(7)
    lpd_sd   <- numeric(7)
    for(i in 1:7) {
      if(!is.null(lpd_scores[[dep_var]])) {
        window_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == i, ]
        if(nrow(window_data) > 0) {
          lpd_mean[i] <- window_data$LPD_Mean
          lpd_sd[i]   <- window_data$LPD_SD
        } else {
          lpd_mean[i] <- NA
          lpd_sd[i]   <- NA
        }
      } else {
        lpd_mean[i] <- NA
        lpd_sd[i]   <- NA
      }
    }
    
    top_5_mean   <- order(smape_mean)[1:5]
    top_5_median <- order(smape_median)[1:5]
    
    writeData(wb, dep_var, "Predictive interval for Top 5 models based on sMAPE", 
              startRow = start_row, startCol = 1)
    
    writeData(wb, dep_var, "Use Mean", startRow = start_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = start_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = start_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = start_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = start_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = start_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = start_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = start_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = start_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = start_row + 2, startCol = 9)
    
    current_row <- start_row + 3
    for(window in top_5_mean) {
      tryCatch({
        # CHECK IF EXTENDED TEST SETS EXISTS FOR THIS WINDOW
        if(!is.null(extended_test_sets[[dep_var]]) && 
           length(extended_test_sets[[dep_var]]) >= window &&
           !is.null(extended_test_sets[[dep_var]][[window]])) {
          
          pred_data <- extended_test_sets[[dep_var]][[window]]
          last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
          
          # GET MODEL AND CHECK WHAT VARIABLES IT ACTUALLY USES
          model <- all_models[[dep_var]][[window]]
          if(!is.null(model) && !is.null(model$coefficients)) {
            model_vars <- colnames(model$coefficients)
            available_vars <- colnames(last_row_predictors)
            
            # FIND VARIABLES THAT SURVIVED STEPWISE SELECTION
            selected_vars <- intersect(model_vars, available_vars)
            eliminated_vars <- setdiff(model_vars, available_vars)
            
            if(length(eliminated_vars) > 0) {
              cat("Variables eliminated by stepwise for", dep_var, "window", window, ":", paste(eliminated_vars, collapse=", "), "\n")
            }
            
            if(length(selected_vars) > 0) {
              cat("Variables selected for", dep_var, "window", window, ":", paste(selected_vars, collapse=", "), "\n")
              
              # USE ONLY THE SELECTED VARIABLES FOR PREDICTION
              last_row_predictors <- last_row_predictors[, selected_vars, drop = FALSE]
              
              # MAKE PREDICTION WITH SELECTED VARIABLES ONLY
              pred <- predict.bsts(model, 
                                 newdata = as.data.frame(last_row_predictors),
                                 burn = 100)
              dist <- pred$distribution[,1]
              
              writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
              writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
              writeData(wb, dep_var, mean(dist),            startRow = current_row, startCol = 3)
              writeData(wb, dep_var, median(dist),          startRow = current_row, startCol = 4)
              writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
              
              writeData(wb, dep_var, smape_mean[window],  startRow = current_row, startCol = 6)
              writeData(wb, dep_var, crps_mean[window],   startRow = current_row, startCol = 7)
              writeData(wb, dep_var, lpd_mean[window],    startRow = current_row, startCol = 8)
              writeData(wb, dep_var, lpd_sd[window],      startRow = current_row, startCol = 9)
              
            } else {
              # NO VARIABLES SURVIVED STEPWISE SELECTION
              cat("No variables survived stepwise selection for", dep_var, "window", window, "\n")
              writeData(wb, dep_var, paste("Window", window, "- No variables selected"), startRow = current_row, startCol = 1)
              writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 2)
              writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 3)
              writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 4)
              writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 5)
              writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
              writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
              writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
              writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
            }
          } else {
            # NO MODEL AVAILABLE
            writeData(wb, dep_var, paste("Window", window, "- No model"), startRow = current_row, startCol = 1)
            writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 2)
            writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 3)
            writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 4)
            writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 5)
            writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
            writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
            writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
            writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
          }
        } else {
          # NO EXTENDED TEST DATA
          writeData(wb, dep_var, paste("Window", window, "- No test data"), startRow = current_row, startCol = 1)
          writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 2)
          writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 3)
          writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 4)
          writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 5)
          writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
          writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
          writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
          writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
        }
        
      }, error = function(e) {
        cat("Error in window", window, "for", dep_var, ":", e$message, "\n")
        writeData(wb, dep_var, paste("Window", window, "- Error:", substr(e$message, 1, 20)), startRow = current_row, startCol = 1)
        writeData(wb, dep_var, "ERROR", startRow = current_row, startCol = 2)
        writeData(wb, dep_var, "ERROR", startRow = current_row, startCol = 3)
        writeData(wb, dep_var, "ERROR", startRow = current_row, startCol = 4)
        writeData(wb, dep_var, "ERROR", startRow = current_row, startCol = 5)
        writeData(wb, dep_var, smape_mean[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      })
      
      current_row <- current_row + 1
    }
    
    writeData(wb, dep_var, "Use Median", startRow = current_row + 1, startCol = 1)
    
    writeData(wb, dep_var, "Window",  startRow = current_row + 2, startCol = 1)
    writeData(wb, dep_var, "2.5%",    startRow = current_row + 2, startCol = 2)
    writeData(wb, dep_var, "Mean",    startRow = current_row + 2, startCol = 3)
    writeData(wb, dep_var, "Median",  startRow = current_row + 2, startCol = 4)
    writeData(wb, dep_var, "97.5%",   startRow = current_row + 2, startCol = 5)
    writeData(wb, dep_var, "SMAPE",   startRow = current_row + 2, startCol = 6)
    writeData(wb, dep_var, "CRPS",    startRow = current_row + 2, startCol = 7)
    writeData(wb, dep_var, "LPD Mean",startRow = current_row + 2, startCol = 8)
    writeData(wb, dep_var, "LPD SD",  startRow = current_row + 2, startCol = 9)
    
    current_row <- current_row + 3
    for(window in top_5_median) {
      tryCatch({
        # SAME STEPWISE HANDLING FOR MEDIAN SECTION
        if(!is.null(extended_test_sets[[dep_var]]) && 
           length(extended_test_sets[[dep_var]]) >= window &&
           !is.null(extended_test_sets[[dep_var]][[window]])) {
          
          pred_data <- extended_test_sets[[dep_var]][[window]]
          last_row_predictors <- pred_data[nrow(pred_data), -1, drop = FALSE]
          
          model <- all_models[[dep_var]][[window]]
          if(!is.null(model) && !is.null(model$coefficients)) {
            model_vars <- colnames(model$coefficients)
            available_vars <- colnames(last_row_predictors)
            
            selected_vars <- intersect(model_vars, available_vars)
            eliminated_vars <- setdiff(model_vars, available_vars)
            
            if(length(selected_vars) > 0) {
              last_row_predictors <- last_row_predictors[, selected_vars, drop = FALSE]
              
              pred <- predict.bsts(model, 
                                 newdata = as.data.frame(last_row_predictors),
                                 burn = 100)
              dist <- pred$distribution[,1]
              
              writeData(wb, dep_var, format_window_range(window), startRow = current_row, startCol = 1)
              writeData(wb, dep_var, quantile(dist, 0.025), startRow = current_row, startCol = 2)
              writeData(wb, dep_var, mean(dist),            startRow = current_row, startCol = 3)
              writeData(wb, dep_var, median(dist),          startRow = current_row, startCol = 4)
              writeData(wb, dep_var, quantile(dist, 0.975), startRow = current_row, startCol = 5)
              
              writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
              writeData(wb, dep_var, crps_mean[window],    startRow = current_row, startCol = 7)
              writeData(wb, dep_var, lpd_mean[window],     startRow = current_row, startCol = 8)
              writeData(wb, dep_var, lpd_sd[window],       startRow = current_row, startCol = 9)
              
            } else {
              writeData(wb, dep_var, paste("Window", window, "- No variables selected"), startRow = current_row, startCol = 1)
              writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 2)
              writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 3)
              writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 4)
              writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 5)
              writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
              writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
              writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
              writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
            }
          } else {
            writeData(wb, dep_var, paste("Window", window, "- No model"), startRow = current_row, startCol = 1)
            writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 2)
            writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 3)
            writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 4)
            writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 5)
            writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
            writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
            writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
            writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
          }
        } else {
          writeData(wb, dep_var, paste("Window", window, "- No test data"), startRow = current_row, startCol = 1)
          writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 2)
          writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 3)
          writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 4)
          writeData(wb, dep_var, "N/A", startRow = current_row, startCol = 5)
          writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
          writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
          writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
          writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
        }
        
      }, error = function(e) {
        cat("Error in median window", window, "for", dep_var, ":", e$message, "\n")
        writeData(wb, dep_var, paste("Window", window, "- Error:", substr(e$message, 1, 20)), startRow = current_row, startCol = 1)
        writeData(wb, dep_var, "ERROR", startRow = current_row, startCol = 2)
        writeData(wb, dep_var, "ERROR", startRow = current_row, startCol = 3)
        writeData(wb, dep_var, "ERROR", startRow = current_row, startCol = 4)
        writeData(wb, dep_var, "ERROR", startRow = current_row, startCol = 5)
        writeData(wb, dep_var, smape_median[window], startRow = current_row, startCol = 6)
        writeData(wb, dep_var, crps_mean[window], startRow = current_row, startCol = 7)
        writeData(wb, dep_var, lpd_mean[window], startRow = current_row, startCol = 8)
        writeData(wb, dep_var, lpd_sd[window], startRow = current_row, startCol = 9)
      })
      
      current_row <- current_row + 1
    }
    
    setColWidths(wb, dep_var, cols = 1, width = 50)
    setColWidths(wb, dep_var, cols = 2:9, width = 15)
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## 3) add_sensitivity_analysis - Simplified version with WAIC, LOOIC, MAPE removed
###############################################################################
add_sensitivity_analysis <- function(sensitivity_results, file_path) {
  if(file.exists(file_path)) {
    wb <- loadWorkbook(file_path)
    
    for(dep_var in names(sensitivity_results)) {
      if(dep_var %in% names(wb)) {
        sheet_data <- readWorkbook(wb, sheet = dep_var)
        last_content_row <- max(which(!is.na(sheet_data[,1]) & sheet_data[,1] != ""), na.rm = TRUE)
        last_row <- last_content_row + 6
        
        results_data <- data.frame(
          Window = integer(),
          Method = character(),
          Sigma = numeric(),
          Slab_Var = numeric(),
          SMAPE = numeric(),
          stringsAsFactors = FALSE
        )
        
        for (model_name in names(sensitivity_results[[dep_var]])) {
          model_info <- sensitivity_results[[dep_var]][[model_name]]
          window <- model_info$window
          method <- model_info$method
          
          for (result_name in names(model_info$results)) {
            result <- model_info$results[[result_name]]
            results_data <- rbind(results_data, data.frame(
              Window = window,
              Method = method,
              Sigma = if(!is.null(result$sigma)) result$sigma else NA,
              Slab_Var = if(!is.null(result$slab_var)) result$slab_var else NA,
              SMAPE = if(!is.null(result$smape)) result$smape else NA,
              stringsAsFactors = FALSE
            ))
          }
        }
        
        writeData(wb, dep_var, "Sensitivity Analysis Results", startRow = last_row, startCol = 1)
        writeData(wb, dep_var, results_data, startRow = last_row + 1, startCol = 1, colNames = TRUE)
        setColWidths(wb, dep_var, cols = 1:5, widths = c(10,10,10,10,10))
      }
    }
    
    saveWorkbook(wb, file_path, overwrite = TRUE)
  }
  return(invisible(NULL))
}

###############################################################################
## 4) add_influence_analysis_to_workbook - Complete influence analysis
###############################################################################
add_influence_analysis_to_workbook <- function(influence_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  for (dep_var in names(influence_results)) {
    if (!dep_var %in% names(wb)) {
      addWorksheet(wb, dep_var)
    }
    
    sheet_data <- tryCatch({
      readWorkbook(wb, sheet = dep_var)
    }, error = function(e) {
      data.frame()
    })
    
    # Calculate exact start row by finding the last sensitivity analysis row
    non_empty_rows <- which(!is.na(sheet_data[,1]) & sheet_data[,1] != "")
    
    if (length(non_empty_rows) > 0) {
      last_content_row <- max(non_empty_rows, na.rm = TRUE)
      sensitivity_rows <- which(sheet_data[,1] == "Sensitivity Analysis Results")
      
      if (length(sensitivity_rows) > 0) {
        sensitivity_row <- max(sensitivity_rows)
        for (i in (sensitivity_row + 1):nrow(sheet_data)) {
          if (is.na(sheet_data[i,1]) || sheet_data[i,1] == "") {
            last_sensitivity_row <- i - 1
            break
          }
          if (i == nrow(sheet_data)) {
            last_sensitivity_row <- i
          }
        }
        current_row <- last_sensitivity_row + 9
      } else {
        current_row <- last_content_row + 9
      }
    } else {
      current_row <- 1
    }
    
    processed_models <- c()
    
    for (model_name in names(influence_results[[dep_var]])) {
      if (model_name %in% processed_models) next
      processed_models <- c(processed_models, model_name)
      
      results_df <- influence_results[[dep_var]][[model_name]]
      
      # CRITICAL FIX: Map delta_test_forecast to delta_holdout_forecast if needed
      if ("delta_test_forecast" %in% names(results_df) && 
          !"delta_holdout_forecast" %in% names(results_df)) {
        results_df$delta_holdout_forecast <- results_df$delta_test_forecast
      }
      
      # Process delta_looic if present
      if ("delta_looic" %in% names(results_df)) {
        results_df$delta_looic <- as.numeric(as.character(results_df$delta_looic))
      } else {
        results_df$delta_looic <- rep(NA, nrow(results_df))
      }
      
      # Round numeric columns to 4 decimal places
      numeric_cols <- c("pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      for (col in numeric_cols) {
        if (col %in% names(results_df) && is.numeric(results_df[[col]])) {
          results_df[[col]] <- round(results_df[[col]], 4)
        }
      }
      
      # Sort by absolute delta_holdout_forecast
      if (sum(!is.na(results_df$delta_holdout_forecast)) > 0) {
        sorted_df <- results_df[order(abs(results_df$delta_holdout_forecast), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df <- results_df
      }
      top_25_forecast <- head(sorted_df, 25)
      
      # Write the title
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta Holdout Forecast - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      # Get the correct column names
      if (all(c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      } else if (all(c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast") %in% colnames(top_25_forecast))) {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_test_forecast")
        colnames(top_25_forecast)[colnames(top_25_forecast) == "delta_test_forecast"] <- "delta_holdout_forecast"
      } else {
        write_cols <- c("time", "pointwise_lpd", "delta_looic", "delta_holdout_forecast")
      }
      
      # Make sure we only write columns that actually exist
      write_cols <- intersect(write_cols, colnames(top_25_forecast))
      
      # Write the data
      writeData(wb, dep_var, top_25_forecast[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_forecast)), 
                cols = col_idx)
      }
      
      # Proceed to next section - 5 rows after this table
      current_row <- current_row + nrow(top_25_forecast) + 5
      
      # Sort by absolute delta_looic
      if (sum(!is.na(results_df$delta_looic)) > 0) {
        sorted_df_looic <- results_df[order(abs(results_df$delta_looic), decreasing = TRUE, na.last = TRUE), ]
      } else {
        sorted_df_looic <- results_df
      }
      top_25_looic <- head(sorted_df_looic, 25)
      
      # Write looic section
      writeData(wb, dep_var, paste0("Top 25 Periods by Absolute Delta LOOIC - Model: ", model_name),
                startRow = current_row, startCol = 1)
      
      writeData(wb, dep_var, top_25_looic[, write_cols],
                startRow = current_row + 2, startCol = 1, colNames = TRUE)
      
      # Apply styles
      addStyle(wb, dep_var, createStyle(textDecoration = "bold"), rows = current_row, cols = 1, gridExpand = TRUE)
      
      # Apply number format to numeric columns
      for (col_idx in 2:length(write_cols)) {
        addStyle(wb, dep_var, 
                createStyle(numFmt = "0.0000"), 
                rows = (current_row + 3):(current_row + 3 + nrow(top_25_looic)), 
                cols = col_idx)
      }
      
      # Proceed to next model section - 5 rows after this table
      current_row <- current_row + nrow(top_25_looic) + 5
    }
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  cat("Influence analysis results added to workbook:", file_path, "\n")
}

###############################################################################
## 5) add_diagnostics_to_workbook - Complete diagnostics function
###############################################################################
add_diagnostics_to_workbook <- function(all_models, expanding_windows, all_diagnostics, all_looic, 
                                        bayesian_R2_results, crps_scores, lpd_scores, 
                                        ljung_box_results, file_path) {
  wb <- loadWorkbook(file_path)
  
  suppressWarnings({
    if (!requireNamespace("coda", quietly = TRUE)) {
      install.packages("coda")
    }
    library(coda)
  })
  
  for(dep_var in names(all_models)) {
    sheet_data <- readWorkbook(wb, sheet = dep_var)
    current_row <- nrow(sheet_data) + 22
    
    for(window in 1:length(all_models[[dep_var]])) {
      if (is.null(all_models[[dep_var]][[window]])) next
      
      train_size <- if (!is.null(expanding_windows[[dep_var]]) && 
                        !is.null(expanding_windows[[dep_var]][[window]]) && 
                        !is.null(expanding_windows[[dep_var]][[window]]$train)) {
        nrow(expanding_windows[[dep_var]][[window]]$train)
      } else {
        NA
      }
      
      test_size <- if (!is.null(expanding_windows[[dep_var]]) && 
                       !is.null(expanding_windows[[dep_var]][[window]]) && 
                       !is.null(expanding_windows[[dep_var]][[window]]$test)) {
        nrow(expanding_windows[[dep_var]][[window]]$test)
      } else {
        NA
      }
      
      n_predictors <- if (!is.null(expanding_windows[[dep_var]]) && 
                          !is.null(expanding_windows[[dep_var]][[window]]) && 
                          !is.null(expanding_windows[[dep_var]][[window]]$train)) {
        ncol(expanding_windows[[dep_var]][[window]]$train) - 1
      } else {
        NA
      }
      
      dic_value <- if (!is.null(all_diagnostics) && 
                       !is.null(all_diagnostics[[dep_var]]) && 
                       !is.null(all_diagnostics[[dep_var]]$DIC) &&
                       length(all_diagnostics[[dep_var]]$DIC) >= window) {
        all_diagnostics[[dep_var]]$DIC[window]
      } else {
        NA
      }
      
      waic_value <- if (!is.null(all_diagnostics) && 
                        !is.null(all_diagnostics[[dep_var]]) && 
                        !is.null(all_diagnostics[[dep_var]]$WAIC) &&
                        length(all_diagnostics[[dep_var]]$WAIC) >= window) {
        all_diagnostics[[dep_var]]$WAIC[window]
      } else {
        NA
      }
      
      looic_value <- if (!is.null(all_looic) && 
                         !is.null(all_looic[[dep_var]]) && 
                         length(all_looic[[dep_var]]) >= window) {
        all_looic[[dep_var]][window]
      } else {
        NA
      }
      
      r2_value <- if (!is.null(bayesian_R2_results) && 
                      !is.null(bayesian_R2_results[[dep_var]]) && 
                      !is.null(bayesian_R2_results[[dep_var]]$Mean_R2) &&
                      length(bayesian_R2_results[[dep_var]]$Mean_R2) >= window) {
        bayesian_R2_results[[dep_var]]$Mean_R2[window]
      } else {
        NA
      }
      
      crps_value <- if (!is.null(crps_scores) && 
                        !is.null(crps_scores[[dep_var]]) && 
                        any(crps_scores[[dep_var]]$Window == window)) {
        crps_data <- crps_scores[[dep_var]][crps_scores[[dep_var]]$Window == window, ]
        if (nrow(crps_data) > 0 && !is.na(crps_data$CRPS_Mean) && !is.na(crps_data$CRPS_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", crps_data$CRPS_Mean, crps_data$CRPS_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lpd_value <- if (!is.null(lpd_scores) && 
                       !is.null(lpd_scores[[dep_var]]) && 
                       any(lpd_scores[[dep_var]]$Window == window)) {
        lpd_data <- lpd_scores[[dep_var]][lpd_scores[[dep_var]]$Window == window, ]
        if (nrow(lpd_data) > 0 && !is.na(lpd_data$LPD_Mean) && !is.na(lpd_data$LPD_SD)) {
          sprintf("Mean: %.4f, SD: %.4f", lpd_data$LPD_Mean, lpd_data$LPD_SD)
        } else {
          "N/A"
        }
      } else {
        "N/A"
      }
      
      lb_stat <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_Statistic) &&
                     length(ljung_box_results[[dep_var]]$LB_Statistic) >= window) {
        ljung_box_results[[dep_var]]$LB_Statistic[window]
      } else {
        NA
      }
      
      lb_pval <- if (!is.null(ljung_box_results) && 
                     !is.null(ljung_box_results[[dep_var]]) && 
                     !is.null(ljung_box_results[[dep_var]]$LB_pvalue) &&
                     length(ljung_box_results[[dep_var]]$LB_pvalue) >= window) {
        ljung_box_results[[dep_var]]$LB_pvalue[window]
      } else {
        NA
      }
      
      metrics <- data.frame(
        Metric = c(
          "Training Sample Size",
          "Test Sample Size",
          "Number of Predictors",
          "DIC/number of training period",
          "WAIC/number of training period",
          "LOOIC",
          "Bayesian R²",
          "Number of iterations used in BSTS",
          "CRPS",
          "LPD",
          "LB_Statistic",
          "LB_pvalue"
        ),
        Value = c(
          train_size,
          test_size,
          n_predictors,
          dic_value,
          waic_value,
          looic_value,
          r2_value,
          10000,
          crps_value,
          lpd_value,
          lb_stat,
          lb_pval
        )
      )
      
      writeData(wb, dep_var, paste("Window", window), startRow = current_row)
      current_row <- current_row + 2
      writeData(wb, dep_var, metrics, startRow = current_row, startCol = 1, colNames = FALSE)
      current_row <- current_row + nrow(metrics) + 2
      
      tryCatch({
        coef_stats <- data.frame(
          Parameter     = character(),
          Mean          = numeric(),
          Median        = numeric(),
          CI_2.5        = numeric(),
          CI_97.5       = numeric(),
          ESS           = numeric(),
          Post_Prob     = numeric(),
          Geweke_Pval   = numeric(),
          HW_Pval       = numeric(),
          RL_Autocorr   = numeric(),
          RL_Iterations = numeric(),
          RL_Burnin     = numeric(),
          RL_Dependence = numeric(),
          stringsAsFactors = FALSE
        )
        
        model <- all_models[[dep_var]][[window]]
        if (!is.null(model) && !is.null(model$coefficients)) {
          coef_matrix <- as.matrix(model$coefficients)
          
          for(j in 1:ncol(coef_matrix)) {
            param_name <- colnames(coef_matrix)[j]
            chain <- as.vector(coef_matrix[,j])
            if (length(chain) < 2) next
            
            ci <- tryCatch({
              quantile(chain, probs = c(0.025, 0.975))
            }, error = function(e) c(NA, NA))
            
            mean_val   <- tryCatch(mean(chain), error = function(e) NA)
            median_val <- tryCatch(median(chain), error = function(e) NA)
            ess_val    <- tryCatch(as.numeric(effectiveSize(mcmc(chain))), error = function(e) NA)
            pip        <- tryCatch(mean(chain != 0), error = function(e) NA)
            
            geweke_pval <- tryCatch({
              geweke <- geweke.diag(mcmc(chain))
              2 * pnorm(-abs(geweke$z))
            }, error = function(e) NA)
            
            hw_pval <- tryCatch({
              hw <- heidel.diag(mcmc(chain))
              if (is.null(hw)) NA else hw[1, "pvalue"]
            }, error = function(e) NA)
            
            rl_stats <- tryCatch({
              if (length(unique(chain)) >= 10) {
                binary_chain <- as.mcmc(as.numeric(chain > median(chain)))
                rl <- raftery.diag(binary_chain)
                c(cor(chain[-1], chain[-length(chain)]),
                  rl$resmatrix[1,"N"],
                  rl$resmatrix[1,"M"],
                  rl$resmatrix[1,"I"])
              } else {
                rep(NA, 4)
              }
            }, error = function(e) rep(NA, 4))
            
            coef_stats <- rbind(coef_stats, data.frame(
              Parameter     = param_name,
              Mean          = round(mean_val, 4),
              Median        = round(median_val, 4),
              CI_2.5        = round(ci[1], 4),
              CI_97.5       = round(ci[2], 4),
              ESS           = round(ess_val, 0),
              Post_Prob     = round(pip, 4),
              Geweke_Pval   = round(geweke_pval, 4),
              HW_Pval       = round(hw_pval, 4),
              RL_Autocorr   = round(rl_stats[1], 4),
              RL_Iterations = round(rl_stats[2], 0),
              RL_Burnin     = round(rl_stats[3], 0),
              RL_Dependence = round(rl_stats[4], 2),
              stringsAsFactors=FALSE
            ))
          }
        }
        
        if (nrow(coef_stats) > 0) {
          writeData(wb, dep_var, coef_stats, startRow = current_row)
          current_row <- current_row + nrow(coef_stats) + 3
        } else {
          writeData(wb, dep_var, "No coefficient data available", startRow = current_row)
          current_row <- current_row + 4
        }
        
      }, error = function(e) {
        writeData(wb, dep_var, paste("Error processing coefficients:", e$message), startRow = current_row)
        current_row <- current_row + 4
      })
    }
    
    setColWidths(wb, dep_var, cols = 1:13, widths = "auto")
  }
  
  saveWorkbook(wb, file_path, overwrite = TRUE)
  return(wb)
}

###############################################################################
## 6) format_window_range - Helper function for window descriptions
###############################################################################
format_window_range <- function(window) {
  train_ranges <- c(
    "1-70", 
    "1-75",
    "1-80",
    "1-85",
    "1-90"
  )
  
  test_ranges <- c(
    "71-105",
    "76-105",
    "81-105",
    "86-105",
    "91-105"
  )
  
  paste(train_ranges[window], "as the training and the", test_ranges[window], "as the testing")
}

###############################################################################
## 7) create_excel_results_revised - MAIN FUNCTION - Creates EW_Step_2015Q1.xlsx
###############################################################################
create_excel_results_revised <- function(
  all_evaluations, 
  all_models, 
  extended_test_sets, 
  crps_scores, 
  lpd_scores, 
  sensitivity_results, 
  influence_results, 
  expanding_windows, 
  all_diagnostics, 
  all_looic, 
  bayesian_R2_results, 
  ljung_box_results,
  aggregate_smape_results,
  ensemble_results
) {
  file_path <- "EW_Step_2023Q1.xlsx"  # CORRECT FILE NAME FOR STEPWISE
  
  # Handle influence results if they exist
  if (!is.null(influence_results)) {
    for (dep_var in names(influence_results)) {
      for (model_name in names(influence_results[[dep_var]])) {
        results_df <- influence_results[[dep_var]][[model_name]]
        if ("delta_test_forecast" %in% names(results_df) && 
            !"delta_holdout_forecast" %in% names(results_df)) {
          influence_results[[dep_var]][[model_name]]$delta_holdout_forecast <- 
            results_df$delta_test_forecast
        }
      }
    }
  }
  
  cat("Creating prediction tables...\n")
  wb <- create_enhanced_prediction_tables_revised(
    all_evaluations = all_evaluations, 
    all_models = all_models, 
    extended_test_sets = extended_test_sets, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    file_path = file_path,
    aggregate_smape_results = aggregate_smape_results,
    ensemble_results = ensemble_results
  )
  
  cat("Adding sensitivity analysis...\n")
  add_sensitivity_analysis(sensitivity_results, file_path)
  
  # Add influence analysis with proper spacing (only if data exists)
  if (!is.null(influence_results)) {
    cat("Adding influence analysis...\n")
    add_influence_analysis_to_workbook(influence_results, file_path)
  }
  
  # Add diagnostics
  cat("Adding diagnostics...\n")
  add_diagnostics_to_workbook(
    all_models = all_models, 
    expanding_windows = expanding_windows, 
    all_diagnostics = all_diagnostics, 
    all_looic = all_looic,
    bayesian_R2_results = bayesian_R2_results, 
    crps_scores = crps_scores, 
    lpd_scores = lpd_scores, 
    ljung_box_results = ljung_box_results,
    file_path = file_path
  )
  
  return(paste("Results successfully written to", file_path))
}

###############################################################################
## 8) USAGE - Call with YOUR EXACT variable names for stepwise
###############################################################################
result <- create_excel_results_revised(
  all_evaluations = all_evaluations_ewstep, 
  all_models = all_modelewstep, 
  extended_test_sets = extended_test_sets,  # FIXED: should be extended_test_sets, not extended_test_sets_step
  crps_scores = crps_scores_ew, 
  lpd_scores = lpd_scores_ew_step, 
  sensitivity_results = sensitivity_results_step, 
  influence_results = influence_results_parallel_step, 
  expanding_windows = expanding_windows, 
  all_diagnostics = all_diagnostics_ew_step, 
  all_looic = all_looic_ew_step, 
  bayesian_R2_results = bayesian_R2_results_ew_step,
  ljung_box_results = ljung_box_results_ew_step,
  aggregate_smape_results = aggregate_smape_results,
  ensemble_results = ensemble_results
)

print(result)
```